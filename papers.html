<!DOCTYPE html>
<html lang="en-us">

  <head>
  <!-- Global Site Tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-107339008-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments)};
    gtag('js', new Date());
    gtag('config', 'UA-107339008-1');
  </script>

  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="keywords" content="machine learning, source code, big code, naturalness, software engineering, programming languages">

  <title>
    
      Search all Publications on Machine Learning for Source Code &middot; Machine Learning for Big Code and Naturalness
    
  </title>

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="shortcut icon" href="/public/favicon.svg">
  <link rel="search" href="/public/opensearchdescription.xml" 
      type="application/opensearchdescription+xml" 
      title="ML4Code" />

  <script src="https://code.jquery.com/jquery-3.2.1.min.js"
  integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
  crossorigin="anonymous"></script>
  
  <link rel="stylesheet" type="text/css" href="//cdn.datatables.net/1.10.16/css/jquery.dataTables.min.css">
  <script type="text/javascript" charset="utf8" src="//cdn.datatables.net/1.10.16/js/jquery.dataTables.min.js"></script>
</head>


  <body class="theme-base-0d layout-reverse">

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Machine Learning for Big Code and Naturalness
        </a>
      </h1>
      <p class="lead">Research on machine learning for source code.</p>      
    </div>

  <nav class="sidebar-nav">
   <div class="sidebar-item"><p style="font-size: 12px">Search related work <input type='text' id='searchTarget' size="16"/> <button onClick="search();">Go</button></p></div>
   <a class="sidebar-nav-item active" href="/papers.html">List of Papers</a>
   <a class="sidebar-nav-item" href="/tags.html">Papers by Tag</a>
   <a class="sidebar-nav-item" href="/tsne-viz.html">2D Map of Papers</a>

   <a class="sidebar-nav-item" href="/base-taxonomy/">Core Taxonomy</a>


  <a class="sidebar-nav-item" href="/resources.html">Resources, Courses &#38; Events</a>
  <a class="sidebar-nav-item" href="/contributing.html">Contributing</a>
  <a class="sidebar-nav-item" href="/contributors.html">Contributors</a>
  </nav>

  <div class="sidebar-item">
    <p style="font-size: 12px">Contact <a href="https://miltos.allamanis.com">Miltos Allamanis</a> about this survey or website.
    <span style="font-size: 9px">
      Made with <a href="https://jekyllrb.com">Jekyll</a> and <a href="https://github.com/poole/hyde">Hyde</a>.
    </span></p>
  </div>
</div></div>

<script>
$("#searchTarget").keydown(function (e) {	
  if (e.keyCode == 13) {
    search();
  }
});

function search() {
  try {
    ga('send', 'event', 'search', 'search', $("#searchTarget").val());
  } finally {
    window.location = "/papers.html#" + $("#searchTarget").val();
  }
}
</script>


    <div class="content container">
      Search across all paper titles, abstracts, authors by using the search field.
Please consider <a href="/contributing.html">contributing</a> by updating
the information of existing papers or adding new work.

<table id="allPapers">
<thead><th>Year</th><th>Title</th><th>Authors</th><th>Venue</th><th>Abstract</th></thead><tbody>



<tr>
	<td>2020</td>
	<td><a href="/publications/tabassum2020code/">Code and Named Entity Recognition in StackOverflow</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Code and Named Entity Recognition in StackOverflow' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Code and Named Entity Recognition in StackOverflow' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Code%20and%20Named%20Entity%20Recognition%20in%20StackOverflow' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Jeniya Tabassum, Mounica Maddela, Wei Xu, Alan Ritter</td>
	<td>ACL</td>
	<td><p>There is an increasing interest in studying natural language and computer code together, as large corpora of programming texts become readily available on the Internet. For example, StackOverflow currently has over 15 million programming related questions written by 8.5 million users. Meanwhile, there is still a lack of fundamental NLP techniques for identifying code tokens or software-related named entities that appear within natural language sentences. In this paper, we introduce a new named entity recognition (NER) corpus for the computer programming domain, consisting of 15,372 sentences annotated with 20 fine-grained entity types. We also present the SoftNER model that combines contextual information with domain specific knowledge using an attention network. The code token recognizer combined with an entity segmentation model we proposed, consistently improves the performance of the named entity tagger. Our proposed SoftNER tagger outperforms the BiLSTM-CRF model with an absolute increase of +9.73 F-1 score on StackOverflow data.</p>
</td>
	<td>dataset information extraction </td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/svyatkovskiy2020intellicode/">IntelliCode Compose: Code Generation Using Transformer</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=IntelliCode Compose: Code Generation Using Transformer' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=IntelliCode Compose: Code Generation Using Transformer' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=IntelliCode%20Compose:%20Code%20Generation%20Using%20Transformer' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, Neel Sundaresan</td>
	<td></td>
	<td><p>In software development through integrated development environments (IDEs), code completion is one of the most widely used features. Nevertheless, majority of integrated development environments only support completion of methods and APIs, or arguments.</p>

<p>In this paper, we introduce IntelliCode Compose − a general-purpose multilingual code completion tool which is capable of predicting sequences of code tokens of arbitrary types, generating up to entire lines of syntactically correct code. It leverages state-of-the-art generative transformer model trained on 1.2 billion lines of source code in Python, C#, JavaScript and TypeScript programming languages. IntelliCode Compose is deployed as a cloud-based web service. It makes use of client-side tree-based caching, efficient parallel implementation of the beam search decoder, and compute graph optimizations to meet edit-time completion suggestion requirements in the Visual Studio Code IDE and Azure Notebook.
Our best model yields an average edit similarity of 86.7% and a perplexity of 1.82 for Python programming language.</p>
</td>
	<td>autocompletion </td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/svyatkovskiy2020fast/">Fast and Memory-Efficient Neural Code Completion</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Fast and Memory-Efficient Neural Code Completion' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Fast and Memory-Efficient Neural Code Completion' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Fast%20and%20Memory-Efficient%20Neural%20Code%20Completion' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Alexey Svyatkovskiy, Sebastian Lee, Anna Hadjitofi, Maik Riechert, Juliana Franco, Miltiadis Allamanis</td>
	<td></td>
	<td><p>Code completion is one of the most widely used features of modern integrated development environments (IDEs). Deep learning has recently made significant progress in the statistical prediction of source code. However, state-of-the-art neural network models consume prohibitively large amounts of memory, causing computational burden to the development environment, especially when deployed in lightweight client devices.</p>

<p>In this work, we reframe neural code completion from a generation task to a task of learning to rank the valid completion suggestions computed from static analyses. By doing so, we are able to design and test a variety of deep neural network model configurations. One of our best models consumes 6 MB of RAM, computes a single suggestion in 8 ms, and achieves 90% recall in its top five suggestions. Our models outperform standard language modeling code completion techniques in terms of predictive performance, computational speed, and memory efficiency. Furthermore, they learn about code semantics from the natural language aspects of the code (e.g. identifier names) and can generalize better to previously unseen code.</p>
</td>
	<td>autocompletion </td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/ciurumelea2020suggesting/">Suggesting Comment Completions for Python using Neural Language Models</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Suggesting Comment Completions for Python using Neural Language Models' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Suggesting Comment Completions for Python using Neural Language Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Suggesting%20Comment%20Completions%20for%20Python%20using%20Neural%20Language%20Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>A. Ciurumelea, S. Proksch, H.C. Gall</td>
	<td>SANER</td>
	<td><p>Source-code comments are an important communication medium between developers to better understand and maintain software. Current research focuses on auto-generating comments by summarizing the code. However, good comments contain additional details, like important design decisions or required trade-offs, and only developers can decide on the proper comment content. Automated summarization techniques cannot include information that does not exist in the code, therefore fully-automated approaches while helpful, will be of limited use. In our work, we propose to empower developers through a semi-automated system instead. We investigate the feasibility of using neural language models trained on a large corpus of Python documentation strings to generate completion suggestions and obtain promising results. By focusing on confident predictions, we can obtain a top-3 accuracy of over 70%, although this comes at the cost of lower suggestion frequency. Our models can be improved by leveraging context information like the signature and the full body of the method. Additionally, we are able to return good accuracy completions even for new projects, suggesting the generalizability of our approach.</p>
</td>
	<td>bimodal autocomplete documentation </td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/shrivastava2020on-the-fly/">On-the-Fly Adaptation of Source Code Models using Meta-Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=On-the-Fly Adaptation of Source Code Models using Meta-Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=On-the-Fly Adaptation of Source Code Models using Meta-Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=On-the-Fly%20Adaptation%20of%20Source%20Code%20Models%20using%20Meta-Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>D. Shrivastava, H. Larochelle, D. Tarlow</td>
	<td></td>
	<td><p>The ability to adapt to unseen, local contexts is an important challenge that successful models of source code must overcome. One of the most popular approaches for the adaptation of such models is dynamic evaluation. With dynamic evaluation, when running a model on an unseen file, the model is updated immediately after having observed each token in that file. In this work, we propose instead to frame the problem of context adaptation as a meta-learning problem. We aim to train a base source code model that is best able to learn from information in a file to deliver improved predictions of missing tokens. Unlike dynamic evaluation, this formulation allows us to select more targeted information (support tokens) for adaptation, that is both before and after a target hole in a file. We consider an evaluation setting that we call line-level maintenance, designed to reflect the downstream task of code auto-completion in an IDE. Leveraging recent developments in meta-learning such as first-order MAML and Reptile, we demonstrate improved performance in experiments on a large scale Java GitHub corpus, compared to other adaptation baselines including dynamic evaluation. Moreover, our analysis shows that, compared to a non-adaptive baseline, our approach improves performance on identifiers and literals by 44% and 15%, respectively.</p>
</td>
	<td>language model autocomplete </td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/briem2020offside/">OffSide: Learning to Identify Mistakes in Boundary Conditions</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=OffSide: Learning to Identify Mistakes in Boundary Conditions' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=OffSide: Learning to Identify Mistakes in Boundary Conditions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=OffSide:%20Learning%20to%20Identify%20Mistakes%20in%20Boundary%20Conditions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>J. A. Briem, J. Smit, H. Sellik, P. Rapoport, G. Gousios, M. Aniche</td>
	<td>2nd Workshop on Testing for Deep Learning and Deep Learning for Testing</td>
	<td><p>Mistakes in boundary conditions are the cause of many bugs in software.
These mistakes happen when, e.g., developers make use of <code class="language-plaintext highlighter-rouge">&lt;</code> or <code class="language-plaintext highlighter-rouge">&gt;</code> in cases
where they should have used <code class="language-plaintext highlighter-rouge">&lt;=</code> or <code class="language-plaintext highlighter-rouge">&gt;=</code>. Mistakes in boundary conditions
are often hard to find and manually detecting them might be very time-consuming
for developers. While researchers have been proposing techniques to cope with
mistakes in the boundaries for a long time, the automated detection of such bugs still
remains a challenge. We conjecture that, for a tool to be able to precisely identify mistakes
in boundary conditions, it should be able to capture the overall context of the source code
under analysis. In this work, we propose a deep learning model that learn mistakes in boundary
conditions and, later, is able to identifythem in unseen code snippets. We train and test a
model on over 1.5 million code snippets, with and without mistakes in different boundary conditions.
Our model shows an accuracy from 55% up to 87%. The model is also able to detect 24 out of 41
real-world bugs;however, with a high false positive rate. The existing state-of-the-practice linter
tools are not able to detect any of the bugs. We hope this paper can pave the road towards deep
learning models that will be able to support developers in detecting mistakes in boundary conditions.</p>
</td>
	<td>defect </td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/compton2020embedding/">Embedding Java Classes with code2vec: Improvements from Variable Obfuscation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Embedding Java Classes with code2vec: Improvements from Variable Obfuscation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Embedding Java Classes with code2vec: Improvements from Variable Obfuscation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Embedding%20Java%20Classes%20with%20code2vec:%20Improvements%20from%20Variable%20Obfuscation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>R. Compton, E. Frank, P. Patros, A. Koay</td>
	<td>MSR</td>
	<td><p>Automatic source code analysis in key areas of software engineering, such as code security, can benefit from Machine Learning (ML). However, many standard ML approaches require a numeric representation of data and cannot be applied directly to source code. Thus, to enable ML, we need to embed source code into numeric feature vectors while maintaining the semantics of the code as much as possible. code2vec is a recently released embedding approach that uses the proxy task of method name prediction to map Java methods to feature vectors. However, experimentation with code2vec shows that it learns to rely on variable names for prediction, causing it to be easily fooled by typos or adversarial attacks. Moreover, it is only able to embed individual Java methods and cannot embed an entire collection of methods such as those present in a typical Java class, making it difficult to perform predictions at the class level (e.g., for the identification of malicious Java classes). Both shortcomings are addressed in the research presented in this paper. We investigate the effect of obfuscating variable names during the training of a code2vec model to force it to rely on the structure of the code rather than specific names and consider a simple approach to creating class-level embeddings by aggregating sets of method embeddings. Our results, obtained on a challenging new collection of source-code classification problems, indicate that obfuscating variable names produces an embedding model that is both impervious to variable naming and more accurately reflects code semantics. The datasets, models, and code are shared for further ML research on source code.</p>
</td>
	<td>naming adversarial </td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/wang2020cocogum/">CoCoGUM: Contextual Code Summarization with Multi-Relational GNN on UMLs</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=CoCoGUM: Contextual Code Summarization with Multi-Relational GNN on UMLs' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=CoCoGUM: Contextual Code Summarization with Multi-Relational GNN on UMLs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=CoCoGUM:%20Contextual%20Code%20Summarization%20with%20Multi-Relational%20GNN%20on%20UMLs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Yanlin Wang, Lun Du, Ensheng Shi, Yuxuan Hu, Shi Han, Dongmei Zhang</td>
	<td></td>
	<td><p>Code summaries are short natural language (NL) descriptions of code snippets that help developers better understand and maintain source code. Due to the pivotal role of code summaries in software development and maintenance, there is a surge of works on automatic code summarization to reduce the heavy burdens of developers. However, contemporary approaches only leverage the information within the boundary of the method being summarized (i.e., local context), and ignore that using broader context could assist with code summarization. In this paper, we explore two global context information, namely intra-class and inter-class context information, and propose the model CoCoGUM: Contextual Code Summarization with Multi-Relational Graph Neural Networks on UMLs. CoCoGUM first incorporates class names as the intra-class context, which is further fed to a Transformer-based sentence embedding model to extract the class lexical embeddings. Then, relevant Unified Modeling Language (UML) class diagrams are extracted as inter-class context and we use a Multi-Relational Graph Neural Network (MR-GNN) to encode the class relational embeddings. Class lexical embeddings and class relational embeddings, together with the outputs from code token encoder and AST encoder, are passed to the decoder armed with a two-level attention mechanism to generate high-quality context-aware code summaries. We conduct extensive experiments to evaluate our approach and compare it with other automatic code summarization models. The experimental results show that CoCoGUM outperforms state-of-the-art methods.</p>
</td>
	<td>summarization </td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/wang2020trans/">TranS^3: A Transformer-based Framework for Unifying Code Summarization and Code Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=TranS^3: A Transformer-based Framework for Unifying Code Summarization and Code Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=TranS^3: A Transformer-based Framework for Unifying Code Summarization and Code Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=TranS%5E3:%20A%20Transformer-based%20Framework%20for%20Unifying%20Code%20Summarization%20and%20Code%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>W. Wang, Y. Zhang, Z. Zeng, G. Xu</td>
	<td></td>
	<td><p>Code summarization and code search have been widely adopted in sofwaredevelopmentandmaintenance. However, fewstudieshave explored the efcacy of unifying them. In this paper, we propose TranS^3 , a transformer-based framework to integrate code summarization with code search. Specifcally, for code summarization,TranS^3 enables an actor-critic network, where in the actor network, we encode the collected code snippets via transformer- and tree-transformer-based encoder and decode the given code snippet to generate its comment. Meanwhile, we iteratively tune the actor network via the feedback from the critic network for enhancing the quality of the generated comments. Furthermore, we import the generated comments to code search for enhancing its accuracy. To evaluatetheefectivenessof TranS^3 , we conduct a set of experimental studies and case studies where the experimental results suggest that TranS^3 can signifcantly outperform multiple state-of-the-art approaches in both code summarization and code search and the study results further strengthen the efcacy of TranS^3 from the developers’ points of view.</p>
</td>
	<td>retrieval documentation </td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/dinella2020hoppity/">Hoppity: Learning Bug Detection and Repair</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Hoppity: Learning Bug Detection and Repair' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Hoppity: Learning Bug Detection and Repair' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Hoppity:%20Learning%20Bug%20Detection%20and%20Repair' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>E. Dinella, H. Dai, Z. Li, M. Naik, L. Song, K. Wang</td>
	<td>ICLR</td>
	<td><p>We present a learning-based approach to detect and fix a broad range of bugs in Javascript programs. We frame the problem in terms of learning a sequence of graph transformations: given a buggy program modeled by a graph structure, our model makes a sequence of predictions including the position of bug nodes and corresponding graph edits to produce a fix. Unlike previous works that use deep neural networks, our approach targets bugs that are more complex and semantic in nature (i.e.~bugs that require adding or deleting statements to fix). We have realized our approach in a tool called HOPPITY. By training on 338,877 Javascript code change commits on Github, HOPPITY correctly detects and fixes bugs in 9,612 out of 42,365 programs in an end-to-end fashion. Given the bug location and type of the fix, HOPPITY also outperforms the baseline approach by a wide margin.</p>
</td>
	<td>edit repair </td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/feng2020codebert/">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=CodeBERT: A Pre-Trained Model for Programming and Natural Languages' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=CodeBERT: A Pre-Trained Model for Programming and Natural Languages' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=CodeBERT:%20A%20Pre-Trained%20Model%20for%20Programming%20and%20Natural%20Languages' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin, T. Liu, D. Jiang, M. Zhou</td>
	<td></td>
	<td><p>We present CodeBERT, a bimodal pre-trained model for programming language (PL) and nat-ural language (NL). CodeBERT learns general-purpose representations that support downstream NL-PL applications such as natural language codesearch, code documentation generation, etc. We develop CodeBERT with Transformer-based neural architecture, and train it with a hybrid objective function that incorporates the pre-training task of replaced token detection, which is to detect plausible alternatives sampled from generators. This enables us to utilize both bimodal data of NL-PL pairs and unimodal data, where the former provides input tokens for model training while the latter helps to learn better generators. We evaluate CodeBERT on two NL-PL applications by fine-tuning model parameters. Results show that CodeBERT achieves state-of-the-art performance on both natural language code search and code documentation generation tasks. Furthermore, to investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and evaluate in a zero-shot setting where parameters of pre-trained models are fixed. Results show that CodeBERT performs better than previous pre-trained models on NL-PL probing.</p>
</td>
	<td>pretraining </td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/wei2020lambdanet/">LambdaNet: Probabilistic Type Inference using Graph Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=LambdaNet: Probabilistic Type Inference using Graph Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=LambdaNet: Probabilistic Type Inference using Graph Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=LambdaNet:%20Probabilistic%20Type%20Inference%20using%20Graph%20Neural%20Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>J. Wei, M. Goyal, G. Durrett, I. Dillig</td>
	<td>ICLR</td>
	<td><p>As gradual typing becomes increasingly popular in languages like Python and TypeScript, there is a growing need to infer type annotations automatically. While type annotations help with tasks like code completion and static error catching, these annotations cannot be fully inferred by compilers and are tedious to annotate by hand. This paper proposes a probabilistic type inference scheme for TypeScript based on a graph neural network. Our approach first uses lightweight source code analysis to generate a program abstraction called a type dependency graph, which links type variables with logical constraints as well as name and usage information. Given this program abstraction, we then use a graph neural network to propagate information between related type variables and eventually make type predictions. Our neural architecture can predict both standard types, like number or string, as well as user-defined types that have not been encountered during training. Our experimental results show that our approach outperforms prior work in this space by 14% (absolute) on library types, while having the ability to make type predictions that are out of scope for existing techniques.</p>
</td>
	<td>GNN types </td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/bielik2020adversarial/">Adversarial Robustness for Code</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Adversarial Robustness for Code' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Adversarial Robustness for Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Adversarial%20Robustness%20for%20Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>P. Bielik, M. Vechev</td>
	<td></td>
	<td><p>We propose a novel technique which addresses the challenge of learning accurate and robust models of code in a principled way. Our method consists of three key components: (i) learning to abstain from making a prediction if uncertain, (ii) adversarial training, and (iii) representation refinement which learns the program parts relevant for the prediction and abstracts the rest. These components are used to iteratively train multiple models, each of which learns a suitable program representation necessary to make robust predictions on a different subset of the dataset. We instantiated our approach to the task of type inference for dynamically typed languages and demonstrate its effectiveness by learning a model that achieves 88% accuracy and 84% robustness. Further, our evaluation shows that using the combination of all three components is key to obtaining accurate and robust models.</p>
</td>
	<td>adversarial types </td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/ramakrishnan2020semantic/">Semantic Robustness of Models of Source Code</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Semantic Robustness of Models of Source Code' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Semantic Robustness of Models of Source Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Semantic%20Robustness%20of%20Models%20of%20Source%20Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>G. Ramakrishnan, J. Henkel, Z. Wang, A. Albarghouthi, S. Jha, T. Reps</td>
	<td></td>
	<td><p>Deep neural networks are vulnerable to adversarial examples - small input perturbations that result in incorrect predictions. We study this problem in the context of models of source code, where we want the network to be robust to source-code modifications that preserve code functionality. We define a natural notion of robustness, k-transformation robustness, in which an adversary performs up to k semantics-preserving transformations to an input program. We show how to train robust models using an adversarial training objective inspired by that of Madry et al. (2018) for continuous domains.</p>

<p>We implement an extensible framework for adversarial training over source code, and conduct a thorough evaluation on a number of datasets and two different architectures. Our results show (1) the increase in robustness following adversarial training, (2) the ability of training on weak adversaries to provide robustness to attacks by stronger adversaries, and (3) the shift in attribution focus of adversarially trained models towards semantic vs. syntactic features.</p>
</td>
	<td>adversarial naming </td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/aye2020sequence/">Sequence Model Design for Code Completion in the Modern IDE</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Sequence Model Design for Code Completion in the Modern IDE' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Sequence Model Design for Code Completion in the Modern IDE' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Sequence%20Model%20Design%20for%20Code%20Completion%20in%20the%20Modern%20IDE' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Gareth Ari Aye, Gail E. Kaiser</td>
	<td>Optional</td>
	<td><p>Code completion plays a prominent role in modern integrated development environments (IDEs). Machine learning has become ubiquitous in analogous natural language writing and search software, surfacing more relevant autocompletions and search suggestions in fewer keystrokes. Prior research has reported training high-accuracy, deep neural networks for modeling source code, but little attention has been given to the practical constraints imposed by interactive developer tools. In particular, neural language models for source code modeling like the one described in Maybe Deep Neural Networks are the Best Choice for Modeling Source Code are framed around code completion, but only report accuracy of next-token prediction. However, in order for a language model (LM) to work well within real-world code completion systems, it must also always make suggestions that produce valid code that typechecks to support code completion’s role in correctness-checking; return instantaneous results to help programmers code more efficiently in fewer keystrokes; and be small enough to fit comfortably on disk and in memory on developer workstations, since virtually all modern IDEs run locally and support offline usage. To meet these additional requirements, we propose a novel design for predicting top-k next tokens that combines static analysis’ ability to enumerate all valid keywords and in-scope identifiers with the ability of a language model to place a probability distribution over them. Our model mixes character-level input representation with token output to represent out-of-vocabulary (OOV) tokens meaningfully and minimize prediction latency. OOV tokens can be predicted through detection of local repetition common in software. This design achieves state-of-art accuracy in source code modeling and fits the constraints imposed by real-world code completion implementations in modern IDEs.</p>
</td>
	<td>autocomplete </td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/hellendoorn2020global/">Global Relational Models of Source Code</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Global Relational Models of Source Code' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Global Relational Models of Source Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Global%20Relational%20Models%20of%20Source%20Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>V.J. Hellendoorn, C. Sutton, R. Singh, P. Maniatis, D. Bieber</td>
	<td>ICLR</td>
	<td><p>Models of code can learn distributed representations of a program’s syntax and semantics to predict many non-trivial properties of a program. Recent state-of-the-art models leverage highly structured representations of programs, such as trees, graphs and paths therein (e.g. data-flow relations), which are precise and abundantly available for code. This provides a strong inductive bias towards semantically meaningful relations, yielding more generalizable representations than classical sequence-based models. Unfortunately, these models primarily rely on graph-based message passing to represent relations in code, which makes them de facto local due to the high cost of message-passing steps, quite in contrast to modern, global sequence-based models, such as the Transformer. In this work, we bridge this divide between global and structured models by introducing two new hybrid model families that are both global and incorporate structural bias: Graph Sandwiches, which wrap traditional (gated) graph message-passing layers in sequential message-passing layers; and Graph Relational Embedding Attention Transformers (GREAT for short), which bias traditional Transformers with relational information from graph edge types. By studying a popular, non-trivial program repair task, variable-misuse identification, we explore the relative merits of traditional and hybrid model families for code representation. Starting with a  graph-based model that already improves upon the prior state-of-the-art for this task by 20%, we show that our proposed hybrid models improve an additional 10-15%, while training both faster and using fewer parameters.</p>
</td>
	<td>variable misuse defect GNN </td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/hoang2020cc2vec/">CC2Vec: Distributed Representations of Code Changes</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=CC2Vec: Distributed Representations of Code Changes' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=CC2Vec: Distributed Representations of Code Changes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=CC2Vec:%20Distributed%20Representations%20of%20Code%20Changes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>T. Hoang, H. J. Kang, J. Lawall, D. Lo</td>
	<td>ICSE</td>
	<td><p>Existing work on software patches often use features specific to a single task. These works often rely on manually identified features, and human effort is required to identify these features for each task. In this work, we propose CC2Vec, a neural network model that learns a representation of code changes guided by their accompanying log messages, which represent the semantic intent of the code changes. CC2Vec models the hierarchical structure of a code change with the help of the attention mechanism and uses multiple comparison functions to identify the differences between the removed and added code.</p>

<p>To evaluate if CC2Vec can produce a distributed representation of code changes that is general and useful for multiple tasks on software patches, we use the vectors produced by CC2Vec for three tasks: log message generation, bug fixing patch identification, and just-in-time defect prediction. In all tasks, the models using CC2Vec outperform the state-of-the-art techniques.</p>
</td>
	<td>edit </td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/xu2020incorporating/">Incorporating External Knowledge through Pre-training for Natural Language to Code Generation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Incorporating External Knowledge through Pre-training for Natural Language to Code Generation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Incorporating External Knowledge through Pre-training for Natural Language to Code Generation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Incorporating%20External%20Knowledge%20through%20Pre-training%20for%20Natural%20Language%20to%20Code%20Generation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Frank F. Xu, Zhengbao Jiang, Pengcheng Yin, Bogdan Vasilescu, Graham Neubig</td>
	<td>ACL</td>
	<td><p>Open-domain code generation aims to generate code in a general-purpose programming language (such as Python) from natural language (NL) intents. Motivated by the intuition that developers usually retrieve resources on the web when writing code, we explore the effectiveness of incorporating two varieties of external knowledge into NL-to-code generation: automatically mined NL-code pairs from the online programming QA forum StackOverflow and programming language API documentation. Our evaluations show that combining the two sources with data augmentation and retrieval-based data re-sampling improves the current state-of-the-art by up to 2.2% absolute BLEU score on the code generation testbed CoNaLa. The code and resources are available at [Open-domain code generation aims to generate code in a general-purpose programming language (such as Python) from natural language (NL) intents. Motivated by the intuition that developers usually retrieve resources on the web when writing code, we explore the effectiveness of incorporating two varieties of external knowledge into NL-to-code generation: automatically mined NL-code pairs from the online programming QA forum StackOverflow and programming language API documentation. Our evaluations show that combining the two sources with data augmentation and retrieval-based data re-sampling improves the current state-of-the-art by up to 2.2% absolute BLEU score on the code generation testbed CoNaLa. The code and resources are available at <a href="https://github.com/neulab/external-knowledge-codegen">https://github.com/neulab/external-knowledge-codegen</a>.</p>
</td>
	<td>bimodal generation </td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/ashwath2020predicting/">Predicting Vulnerability in Large Codebases With Deep Code Representation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Predicting Vulnerability in Large Codebases With Deep Code Representation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Predicting Vulnerability in Large Codebases With Deep Code Representation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Predicting%20Vulnerability%20in%20Large%20Codebases%20With%20Deep%20Code%20Representation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Anshul Tanwar, Krishna Sundaresan, Parmesh Ashwath, Prasanna Ganesan, Sathish Kumar Chandrasekaran, Sriram Ravi</td>
	<td></td>
	<td><p>Currently, while software engineers write code for various modules, quite often, various types of errors - coding, logic, semantic, and others (most of which are not caught by compilation and other tools) get introduced. Some of these bugs might be found in the later stage of testing, and many times it is reported by customers on production code. Companies have to spend many resources, both money and time in finding and fixing the bugs which would have been avoided if coding was done right. Also, concealed flaws in software can lead to security vulnerabilities that potentially allow attackers to compromise systems and applications. Interestingly, same or similar issues/bugs, which were fixed in the past (although in different modules), tend to get introduced in production code again.
We developed a novel AI-based system which uses the deep representation of Abstract Syntax Tree (AST) created from the source code and also the active feedback loop to identify and alert the potential bugs that could be caused at the time of development itself i.e. as the developer is writing new code (logic and/or function). This tool integrated with IDE as a plugin would work in the background, point out existing similar functions/code-segments and any associated bugs in those functions. The tool would enable the developer to incorporate suggestions right at the time of development, rather than waiting for UT/QA/customer to raise a defect.
We assessed our tool on both open-source code and also on Cisco codebase for C and C++ programing language. Our results confirm that deep representation of source code and the active feedback loop is an assuring approach for predicting security and other vulnerabilities present in the code.</p>
</td>
	<td>AST program analysis static analysis </td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/arakelyan2020towards/">Towards Learning Representations of Binary Executable Files for Security Tasks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards Learning Representations of Binary Executable Files for Security Tasks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards Learning Representations of Binary Executable Files for Security Tasks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Towards%20Learning%20Representations%20of%20Binary%20Executable%20Files%20for%20Security%20Tasks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>S. Arakelyan, C. Hauser, E. Kline, A. Galstyan</td>
	<td>AAAI</td>
	<td><p>Tackling binary analysis problems has traditionally implied manually defining rules and heuristics. As an alternative, we are suggesting using machine learning models for learning distributed representations of binaries that can be applicable for a number of downstream tasks. We construct a computational graph from the binary executable and use it with a graph convolutional neural network to learn a high dimensional representation of the program. We show the versatility of this approach by using our representations to solve two semantically different binary analysis tasks – algorithm classification and vulnerability discovery. We compare the proposed approach to our own strong baseline as well as published results and demonstrate improvement on the state of the art methods for both tasks.</p>
</td>
	<td>GNN representation </td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/panthaplackel2020copy/">Copy that! Editing Sequences by Copying Spans</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Copy that! Editing Sequences by Copying Spans' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Copy that! Editing Sequences by Copying Spans' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Copy%20that!%20Editing%20Sequences%20by%20Copying%20Spans' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Sheena Panthaplackel, Miltiadis Allamanis, Marc Brockschmidt</td>
	<td></td>
	<td><p>Neural sequence-to-sequence models are finding increasing use in editing of documents, for example in correcting a text document or repairing source code. In this paper, we argue that common seq2seq models (with a facility to copy single tokens) are not a natural fit for such tasks, as they have to explicitly copy each unchanged token. We present an extension of seq2seq models capable of copying entire spans of the input to the output in one step, greatly reducing the number of decisions required during inference. This extension means that there are now many ways of generating the same output, which we handle by deriving a new objective for training and a variation of beam search for inference that explicitly handle this problem.</p>

<p>In our experiments on a range of editing tasks of natural language and source code, we show that our new model consistently outperforms simpler baselines.</p>
</td>
	<td>edit </td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/panthaplackel2020associating/">Associating Natural Language Comment and Source Code Entities</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Associating Natural Language Comment and Source Code Entities' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Associating Natural Language Comment and Source Code Entities' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Associating%20Natural%20Language%20Comment%20and%20Source%20Code%20Entities' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>S. Panthaplackel, M. Gligoric, R. J. Mooney, J. J. Li</td>
	<td>AAAI</td>
	<td><p>Comments are an integral part of software development; they are natural language descriptions associated with source code elements. Understanding explicit associations can be useful in improving code comprehensibility and maintaining the consistency between code and comments. As an initial step towards this larger goal, we address the task of associating entities in Javadoc comments with elements in Java source code. We propose an approach for automatically extracting supervised data using revision histories of open source projects and present a manually annotated evaluation dataset for this task. We develop a binary classifier and a sequence labeling model by crafting a rich feature set which encompasses various aspects of code, comments, and the relationships between them. Experiments show that our systems outperform several baselines learning from the proposed supervision.</p>
</td>
	<td>dataset bimodal </td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/yasunaga2020graph/">Graph-based, Self-Supervised Program Repair from Diagnostic Feedback</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Graph-based, Self-Supervised Program Repair from Diagnostic Feedback' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Graph-based, Self-Supervised Program Repair from Diagnostic Feedback' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Graph-based,%20Self-Supervised%20Program%20Repair%20from%20Diagnostic%20Feedback' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Michihiro Yasunaga, Percy Liang</td>
	<td></td>
	<td><p>We consider the problem of learning to repair programs from diagnostic feedback (e.g., compiler error messages). Program repair is challenging for two reasons: First, it requires reasoning and tracking symbols across source code and diagnostic feedback. Second, labeled datasets available for program repair are relatively small. In this work, we propose novel solutions to these two challenges. First, we introduce a program-feedback graph, which connects symbols relevant to program repair in source code and diagnostic feedback, and then apply a graph neural network on top to model the reasoning process. Second, we present a self-supervised learning paradigm for program repair that leverages unlabeled programs available online to create a large amount of extra program repair examples, which we use to pre-train our models. We evaluate our proposed approach on two applications: correcting introductory programming assignments (DeepFix dataset) and correcting the outputs of program synthesis (SPoC dataset). Our final system, DrRepair, significantly outperforms prior work, achieving 66.1% full repair rate on DeepFix (+20.8% over the prior best), and 48.0% synthesis success rate on SPoC (+3.3% over the prior best).</p>
</td>
	<td>repair edit GNN </td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/pandi2020opttyper/">OptTyper: Probabilistic Type Inference by Optimising Logical and Natural Constraints</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=OptTyper: Probabilistic Type Inference by Optimising Logical and Natural Constraints' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=OptTyper: Probabilistic Type Inference by Optimising Logical and Natural Constraints' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=OptTyper:%20Probabilistic%20Type%20Inference%20by%20Optimising%20Logical%20and%20Natural%20Constraints' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>I. V. Pandi, E.T. Barr, A.D. Gordon, C. Sutton</td>
	<td></td>
	<td><p>We present a new approach to the type inference problem for dynamic languages. Our goal is to combine logical constraints, that is, deterministic information from a type system, with natural constraints, uncertain information about types from sources like identifier names. To this end, we introduce a framework for probabilistic type inference that combines logic and learning: logical constraints on the types are extracted from the program, and deep learning is applied to predict types from surface-level code properties that are statistically associated, such as variable names. The main insight of our method is to constrain the predictions from the learning procedure to respect the logical constraints, which we achieve by relaxing the logical inference problem of type prediction into a continuous optimisation problem. To evaluate the idea, we built a tool called OptTyper to predict a TypeScript declaration file for a JavaScript library. OptTyper combines a continuous interpretation of logical constraints derived by a simple program transformation and static analysis of the JavaScript code, with natural constraints obtained from a deep learning model, which learns naming conventions for types from a large codebase. We evaluate OptTyper on a data set of 5,800 open-source JavaScript projects that have type annotations in the well-known DefinitelyTyped repository. We find that combining logical and natural constraints yields a large improvement in performance over either kind of information individually, and produces 50% fewer incorrect type predictions than previous approaches.</p>
</td>
	<td>types bimodal </td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/kanade2020pretrained/">Pre-trained Contextual Embedding of Source Code</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Pre-trained Contextual Embedding of Source Code' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Pre-trained Contextual Embedding of Source Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Pre-trained%20Contextual%20Embedding%20of%20Source%20Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>A. Kanade, P. Maniatis, G. Balakrishnan, K. Shi</td>
	<td></td>
	<td><p>The source code of a program not only serves as a formal description of an executable task, but it also serves to communicate developer intent in a human-readable form. To facilitate this, developers use meaningful identifier names and natural-language documentation. This makes it possible to successfully apply sequence-modeling approaches, shown to be effective in natural-language processing, to source code. A major advancement in natural-language understanding has been the use of pre-trained token embeddings; BERT and other works have further shown that pre-trained contextual embeddings can be extremely powerful and can be fine-tuned effectively for a variety of downstream supervised tasks. Inspired by these developments, we present the first attempt to replicate this success on source code. We curate a massive corpus of Python programs from GitHub to pre-train a BERT model, which we call Code Understanding BERT (CuBERT). We also pre-train Word2Vec embeddings on the same dataset. We create a benchmark of five classification tasks and compare fine-tuned CuBERT against sequence models trained with and without the Word2Vec embeddings. Our results show that CuBERT outperforms the baseline methods by a margin of 2.9-22%. We also show its superiority when fine-tuned with smaller datasets, and over fewer epochs. We further evaluate CuBERT’s effectiveness on a joint classification, localization and repair task involving prediction of two pointers.</p>
</td>
	<td>pretraining </td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/karampatsis2020big/">Big Code != Big Vocabulary: Open-Vocabulary Models for Source Code</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Big Code != Big Vocabulary: Open-Vocabulary Models for Source Code' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Big Code != Big Vocabulary: Open-Vocabulary Models for Source Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Big%20Code%20!=%20Big%20Vocabulary:%20Open-Vocabulary%20Models%20for%20Source%20Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>RM. Karampatsis, H. Babii, R. Robbes, C. Sutton, A. Janes</td>
	<td>ICSE</td>
	<td><p>Statistical language modeling techniques have successfully been applied to large source code corpora, yielding a variety of new software development tools, such as tools for code suggestion, improving readability, and API migration. A major issue with these techniques is that code introduces new vocabulary at a far higher rate than natural language, as new identifier names proliferate. Both large vocabularies and out-of-vocabulary issues severely affect Neural Language Models (NLMs) of source code, degrading their performance and rendering them unable to scale. In this paper, we address this issue by: 1) studying how various modelling choices impact the resulting vocabulary on a large-scale corpus of 13,362 projects; 2) presenting an open vocabulary source code NLM that can scale to such a corpus, 100 times larger than in previous work; and 3) showing that such models outperform the state of the art on three distinct code corpora (Java, C, Python). To our knowledge, these are the largest NLMs for code that have been reported.</p>
</td>
	<td>language model </td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/nguyen2020suggesting/">Suggesting Natural Method Names to Check Name Consistencies</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Suggesting Natural Method Names to Check Name Consistencies' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Suggesting Natural Method Names to Check Name Consistencies' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Suggesting%20Natural%20Method%20Names%20to%20Check%20Name%20Consistencies' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>S. Nguyen, H. Phan, T. Le, T. N. Nguyen</td>
	<td>ICSE</td>
	<td><p>Misleading names of the methods in a project or the APIs in a software library confuse developers about program functionality
and API usages, leading to API misuses and defects. In this paper,we introduce MNire, a machine learning approach to check the
consistency between the name of a given method and its implementation. MNire first generates a candidate name and compares the
current name against it. If the two names are sufficiently similar, we consider the method as consistent. To generate the method name,
we draw our ideas and intuition from an empirical study on the nature of method names in a large dataset. Our key finding is that
high proportions of the tokens of method names can be found in the three contexts of a given method including its body,
the interface (the method’s parameter types and return type), and the enclosing class’ name. Even when such tokens are not there,
MNire uses the contexts to predict the tokens due to the high likelihoods of their co-occurrences. Our unique idea is to treat
the name generation as an abstract summarization on the tokens collected from the names of the program entities in the three
above contexts.</p>

<p>We conducted several experiments to evaluate MNire in method name consistency checking and in method name
recommending on large datasets with +14M methods. In detecting inconsistency method names, MNire improves the state-of-the-art
approach by 10.4% and 11% relatively in recall and precision, respectively. In method name recommendation, MNire improves relatively
over the state-of-the-art technique, code2vec, in both recall (18.2% higher) and precision (11.1% higher). To assess MNire’s usefulness,
we used it to detect inconsistent methods and suggest new names in several active, GitHub projects. We made 50 pull requests (PRs) and received
42 responses. Among them, five PRs were merged into the main branch, and 13 were approved for later merging. In total, in 31/42 cases,
the developer teams agree that our suggested names are more meaningful than the current names, showing MNire’s usefulness.</p>
</td>
	<td>naming </td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/kim2020code/">Code Prediction by Feeding Trees to Transformers</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Code Prediction by Feeding Trees to Transformers' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Code Prediction by Feeding Trees to Transformers' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Code%20Prediction%20by%20Feeding%20Trees%20to%20Transformers' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>S. Kim, J. Zhao, Y. Tian, S. Chandra</td>
	<td></td>
	<td><p>In this paper, we describe how to leverage Transformer, a recent neural architecture for learning from sequential data (such as text), for code completion. As in the realm of natural language processing, Transformers surpass the prediction accuracy achievable by RNNs; we provide an experimental confirmation of this over a Python dataset.</p>

<p>Furthermore, we show that the way to obtain even better accuracy from Transformers is to expose the syntactic structure of code, which is easily recovered by parsing, to the neural network. This works significantly better than presenting the code as a linear token sequence, which is how Transformers were originally intended to be used.</p>

<p>To accomplish this, we propose a novel enhancement to the self-attention mechanism of the Transformer. We enable the mechanism to learn weights—that is, how much to focus on each preceding token in the input—not only on the basis of a token’s value, but also on the basis of the spatial relationships, as in their positions in the abstract syntax tree, between each pair of tokens.</p>

<p>We provide comprehensive experimental evaluation of our proposal, along with alternative design choices, on a standard Python dataset, as well as on a Python corpus internal to Facebook.</p>
</td>
	<td>autocompletion </td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/allamanis2020typilus/">Typilus: Neural Type Hints</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Typilus: Neural Type Hints' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Typilus: Neural Type Hints' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Typilus:%20Neural%20Type%20Hints' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Miltiadis Allamanis, Earl T. Barr, Soline Ducousso, Zheng Gao</td>
	<td>PLDI</td>
	<td><p>Type inference over partial contexts in dynamically typed languages is challenging. In this work, we present a graph neural network model that predicts types by probabilistically reasoning over a program’s structure, names, and patterns. The network uses deep similarity learning to learn a TypeSpace – a continuous relaxation of the discrete space of types – and how to embed the type properties of a symbol (i.e. identifier) into it. Importantly, our model can employ one-shot learning to predict an open vocabulary of types, including rare and user-defined ones. We realise our approach in Typilus for Python that combines the TypeSpace with an optional type checker. We show that Typilus accurately predicts types. Typilus confidently predicts types for 70% of all annotatable symbols; when it predicts a type, that type optionally type checks 95% of the time. Typilus can also find incorrect type annotations; two important and popular open source libraries, fairseq and allennlp, accepted our pull requests that fixed the annotation errors Typilus discovered.</p>
</td>
	<td>types GNN </td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/kurbatova2020recommendation/">Recommendation of Move Method Refactoring Using Path-Based Representation of Code</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Recommendation of Move Method Refactoring Using Path-Based Representation of Code' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Recommendation of Move Method Refactoring Using Path-Based Representation of Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Recommendation%20of%20Move%20Method%20Refactoring%20Using%20Path-Based%20Representation%20of%20Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Z. Kurbatova, I. Veselov, Y. Golubev, T. Bryksin</td>
	<td></td>
	<td><p>Software refactoring plays an important role in increasing code quality. One of the most popular refactoring types is the Move Method refactoring. It is usually applied when a method depends more on members of other classes than on its own original class. Several approaches have been proposed to recommend Move Method refactoring automatically. Most of them are based on heuristics and have certain limitations (e.g., they depend on the selection of metrics and manually-defined thresholds). In this paper, we propose an approach to recommend Move Method refactoring based on a path-based representation of code called code2vec that is able to capture the syntactic structure and semantic information of a code fragment. We use this code representation to train a machine learning classifier suggesting to move methods to more appropriate classes. We evaluate the approach on two publicly available datasets: a manually compiled dataset of well-known open-source projects and a synthetic dataset with automatically injected code smell instances. The results show that our approach is capable of recommending accurate refactoring opportunities and outperforms JDeodorant and JMove, which are state of the art tools in this field.</p>
</td>
	<td>refactoring </td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/lachaux2020unsupervised/">Unsupervised Translation of Programming Languages</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Translation of Programming Languages' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Unsupervised Translation of Programming Languages' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Unsupervised%20Translation%20of%20Programming%20Languages' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Marie-Anne Lachaux, Baptiste Roziere, Lowik Chanussot, Guillaume Lample</td>
	<td></td>
	<td><p>A transcompiler, also known as source-to-source translator, is a system that converts source code from a high-level programming language (such as C++ or Python) to another. Transcompilers are primarily used for interoperability, and to port codebases written in an obsolete or deprecated language (e.g. COBOL, Python 2) to a modern one. They typically rely on handcrafted rewrite rules, applied to the source code abstract syntax tree. Unfortunately, the resulting translations often lack readability, fail to respect the target language conventions, and require manual modifications in order to work properly. The overall translation process is timeconsuming and requires expertise in both the source and target languages, making code-translation projects expensive. Although neural models significantly outperform their rule-based counterparts in the context of natural language translation, their applications to transcompilation have been limited due to the scarcity of parallel data in this domain. In this paper, we propose to leverage recent approaches in unsupervised machine translation to train a fully unsupervised neural transcompiler. We train our model on source code from open source GitHub projects, and show that it can translate functions between C++, Java, and Python with high accuracy. Our method relies exclusively on monolingual source code, requires no expertise in the source or target languages, and can easily be generalized to other programming languages. We also build and release a test set composed of 852 parallel functions, along with unit tests to check the correctness of translations. We show that our model outperforms rule-based commercial baselines by a significant margin.</p>
</td>
	<td>migration </td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/leclair2020improved/">Improved Code Summarization via a Graph Neural Network</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Improved Code Summarization via a Graph Neural Network' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Improved Code Summarization via a Graph Neural Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Improved%20Code%20Summarization%20via%20a%20Graph%20Neural%20Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>A. LeClair, S. Haque, L. Wu, C. McMillan</td>
	<td></td>
	<td><p>Automatic source code summarization is the task of generating natural language descriptions for source code. Automatic code summarization is a rapidly expanding research area, especially as the community has taken greater advantage of advances in neural network and AI technologies. In general, source code summarization techniques use the source code as input and outputs a natural language description. Yet a strong consensus is developing that using structural information as input leads to improved performance. The first approaches to use structural information flattened the AST into a sequence. Recently, more complex approaches based on random AST paths or graph neural networks have improved on the models using flattened ASTs. However, the literature still does not describe the using a graph neural network together with source code sequence as separate inputs to a model. Therefore, in this paper, we present an approach that uses a graph-based neural architecture that better matches the default structure of the AST to generate these summaries. We evaluate our technique using a data set of 2.1 million Java method-comment pairs and show improvement over four baseline techniques, two from the software engineering literature, and two from machine learning literature.</p>
</td>
	<td>summarization </td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/zhang2020generating/">Generating Adversarial Examples for Holding Robustness of Source Code Processing Models</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Generating Adversarial Examples for Holding Robustness of Source Code Processing Models' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Generating Adversarial Examples for Holding Robustness of Source Code Processing Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Generating%20Adversarial%20Examples%20for%20Holding%20Robustness%20of%20Source%20Code%20Processing%20Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>H. Zhang, Z. Li, G. Li, L. Ma, Y. Liu, Z. Jin</td>
	<td>AAAI</td>
	<td><p>Automated  processing,  analysis,  and  generation  of  source code are among the key activities
in software and system life-cycle. To this end, while deep learning (DL) exhibits a certain level
of capability in handling these tasks, the current state-of-the-art DL models still suffer from
non-robust issues and can be easily fooled by adversarial attacks.</p>

<p>Different  from  adversarial 
attacks  for  image,  audio,  andnatural  languages,  the  structured  nature  of  programming
languages  brings  new  challenges.  In  this  paper,  we  propose a Metropolis-Hastings
sampling-based identifier renaming technique, named Metropolis-Hastings Modifier (MHM),
which  generates  adversarial  examples  for  DL  models  specialized for source code processing.
Our in-depth evaluation on a functionality classification benchmark demonstrates the
effectiveness  of  MHM  in  generating  adversarial  examples of source code. The higher robustness
and performance enhanced through our  adversarial training with MHM further confirms the usefulness
of DL models-based method for future fully automated source code processing.</p>
</td>
	<td>adversarial </td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/mukherjee2020searching/">Searching a Database of Source Codes Using Contextualized Code Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Searching a Database of Source Codes Using Contextualized Code Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Searching a Database of Source Codes Using Contextualized Code Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Searching%20a%20Database%20of%20Source%20Codes%20Using%20Contextualized%20Code%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>R. Mukherjee, S. Chaudhuri, C. Jermaine</td>
	<td></td>
	<td><p>We assume a database containing a large set of program source codes and consider the problem of contextualized code search over that database. A programmer has written some part of a program, but has left part of the program (such as a method or a function body) incomplete. The goal is to use the context surrounding the missing code to automatically ‘figure out’ which of the codes in the database would be useful to the programmer in order to help complete the missing code, in the sense that the programmer could either re-purpose the retrieved code and use the re-purposed code to fill the missing spot in the program. Or, the user could use the retrieved code as a model for implementing the missing code. The search is ‘contextualized’ in the sense that the search engine should use clues in the partially-completed code to figure out which database code is most useful. The user should not be required to formulate an explicit query.</p>

<p>We cast contextualized code search as a learning problem, where the goal is to learn a distribution function computing the likelihood that each database code completes the program, and propose a neural model for predicting which database code is likely to be most useful. Because it will be prohibitively expensive to apply a neural model to each code in a database of millions or billions of codes at search time, one of our key technical concerns is ensuring a speedy search. We address this by learning a ‘reverse encoder’ that can be used to reduce the problem of evaluating each database code to computing a convolution of two normal distributions, making it possible to search a large database of codes in a reasonable time.</p>
</td>
	<td>search representation </td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/li2020dlfix/">DLFix: Context-based Code Transformation Learning for Automated Program Repair</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=DLFix: Context-based Code Transformation Learning for Automated Program Repair' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=DLFix: Context-based Code Transformation Learning for Automated Program Repair' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=DLFix:%20Context-based%20Code%20Transformation%20Learning%20for%20Automated%20Program%20Repair' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Y. Li, S. Wang, T. N. Nguyen</td>
	<td>ICSE</td>
	<td><p>Automated Program Repair (APR) is very useful in helping developers in the process of software development and maintenance. Despite recent advances in deep learning (DL), the DL-based APR approaches still have limitations in learning bug-fixing code changes and the context of the surrounding source code of the bug-fixing code changes. These limitations lead to incorrect fixing locations or fixes. In this paper, we introduce DLFix, a two-tier DL model that treats APR as code transformation learning from the prior bug fixes and the surrounding code contexts of the fixes. The first layer is a tree-based RNN model that learns the contexts of bug fixes and its result is used as an additional weighting input for the second layer designed to learn the bug-fixing code transformations.</p>

<p>We conducted several experiments to evaluate DLFix in two benchmarks: Defect4J and Bugs.jar, and a newly built bug datasets with a total of +20K real-world bugs in eight projects. We compared DLFix against a total of 13 state-of-the-art pattern-based APR tools. Our results show that DLFix can auto-fix more bugs than 11 of them, and is comparable and complementary to the top two pattern-based APR tools in which there are 7 and 11 unique bugs that they cannot detect, respectively, but we can. Importantly, DLFix is fully automated and data-driven, and does not require hard-coding of bug-fixing patterns as in those tools. We compared DLFix against 4 state-of-the-art deep learning based APR models. DLFix is able to fix 2.5 times more bugs than the best performing~baseline.</p>
</td>
	<td>edit repair AST </td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/louis2020where/">Where should I comment my code? A dataset and model for predicting locations that need comments</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Where should I comment my code? A dataset and model for predicting locations that need comments' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Where should I comment my code? A dataset and model for predicting locations that need comments' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Where%20should%20I%20comment%20my%20code?%20A%20dataset%20and%20model%20for%20predicting%20locations%20that%20need%20comments' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>A. Louis, S.K. Dash, E.T. Barr, M.D. Ernst, and C. Sutton</td>
	<td>International Conference on Software Engineering (ICSE; NIER track)</td>
	<td><p>Programmers should write code comments, but not on every line
of code. We have created a machine learning model that suggests
locations where a programmer should write a code comment. We
trained it on existing commented code to learn locations that are
chosen by developers. Once trained, the model can predict locations
in new code. Our models achieved precision of 74% and recall of
13% in identifying comment-worthy locations. This first success
opens the door to future work, both in the new where-to-comment
problem and in guiding comment generation.</p>
</td>
	<td>bimodal documentation </td>
</tr>



<tr>
	<td>2019</td>
	<td><a href="/publications/li2019neural/">Neural Code Search Evaluation Dataset</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Neural Code Search Evaluation Dataset' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Neural Code Search Evaluation Dataset' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Neural%20Code%20Search%20Evaluation%20Dataset' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>H. Li, S. Kim, S. Chandra</td>
	<td></td>
	<td><p>There has been an increase of interest in code search using natural language. Assessing the performance of such code search models can be difficult without a readily available evaluation suite. In this paper, we present an evaluation dataset consisting of natural language query and code snippet pairs, with the hope that future work in this area can use this dataset as a common benchmark. We also provide the results of two code search models ([1] and [6]) from recent work.</p>
</td>
	<td>dataset search </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/agashe2019julce/">JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=JuICe:%20A%20Large%20Scale%20Distantly%20Supervised%20Dataset%20for%20Open%20Domain%20Context-based%20Code%20Generation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>R. Agashe, S. Iyer, L. Zettlemoyer</td>
	<td></td>
	<td><p>Interactive programming with interleaved code snippet cells and natural language markdown is recently gaining popularity in the form of Jupyter notebooks, which accelerate prototyping and collaboration. To study code generation conditioned on a long context history, we present JuICe, a corpus of 1.5 million examples with a curated test set of 3.7K instances based on online programming assignments. Compared with existing contextual code generation datasets, JuICe provides refined human-curated data, open-domain code, and an order of magnitude more training data. Using JuICe, we train models for two tasks: (1) generation of the API call sequence in a code cell, and (2) full code cell generation, both conditioned on the NL-Code history up to a particular code cell. Experiments using current baseline code generation models show that both context and distant supervision aid in generation, and that the dataset is challenging for current systems.</p>
</td>
	<td>dataset bimodal </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/malik2019nl2type/">NL2Type: Inferring JavaScript Function Types from Natural Language Information</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=NL2Type: Inferring JavaScript Function Types from Natural Language Information' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=NL2Type: Inferring JavaScript Function Types from Natural Language Information' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=NL2Type:%20Inferring%20JavaScript%20Function%20Types%20from%20Natural%20Language%20Information' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>R.S. Malik, J. Patra, M. Pradel</td>
	<td>ICSE</td>
	<td><p>JavaScript is dynamically typed and hence lacks thetype safety  of  statically  typed  languages,
leading  to  suboptimal IDE support, difficult to understand APIs, and unexpected run-time behavior.
Several gradual type systems have been proposed, e.g., Flow and TypeScript, but they rely on developers
to annotatecode with types. This paper presents NL2Type, a learning-based approach for predicting likely
type signatures of JavaScript functions. The key idea is to exploit natural language information in
source code, such as comments, function names, and parameternames,  a  rich  source  of  knowledge
that  is  typically  ignored by  type  inference  algorithms.  We  formulate  the  problem  of predicting
types as a classification problem and train a recurrent, LSTM-based neural model that, after learning
from an annotatedcode  base,  predicts  function  types  for  unannotated  code.  We evaluate   the 
approach   with   a   corpus   of   162,673   JavaScript files  from  real-world  projects. 
NL2Type  predicts  types  with  aprecision of 84.1% and a recall of 78.9% when considering only
the  top-most  suggestion,  and  with  a  precision  of  95.5%  and  arecall  of  89.6%  when
considering  the  top-5  suggestions.  The
approach  outperforms  both  JSNice,  a  state-of-the-art  approach that  analyzes  implementations 
of  functions  instead  of  natural language  information,  and  DeepTyper,  a  recent  type  prediction
approach that is also based on deep learning. Beyond predicting types,  NL2Type  serves  as  a
consistency  checker  for  existing type  annotations.  We  show  that  it  discovers  39  inconsistencies
that  deserve  developer  attention  (from  a  manual  analysis  of  50 warnings), most of which 
are due to incorrect type annotations.</p>
</td>
	<td>bimodal types </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/liu2019neural/">Neural query expansion for code search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Neural query expansion for code search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Neural query expansion for code search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Neural%20query%20expansion%20for%20code%20search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>J. Liu, S. Kim, V. Murali, S. Chaudhuri, S. Chandra</td>
	<td>MAPL</td>
	<td><p>Searching repositories of existing source code for code snippets is a key task in software engineering. Over the years, many approaches to this problem have been proposed. One recent tool called NCS, takes in a natural language query and outputs relevant code snippets, often being able to correctly answer Stack Overflow questions. But what happens when the developer doesn’t provide a query with a clear intent? What if shorter queries are used to demonstrate a more vague intent?</p>

<p>We find that the performance of NCS regresses with shorter queries. Furthermore, data from developers’ code search history logs shows that shorter queries have a less successful code search session: there are more query reformulations and more time is spent browsing the results. These observations lead us to believe that using NCS alone with short queries may not be productive enough.</p>

<p>In this paper, we explore an additional way of using neural networks in code search: the automatic expansion of queries. We present NQE, a neural model that takes in a set of keywords and predicts a set of keywords to expand the query to NCS. NQE learns to predict keywords that co-occur with the query keywords in the underlying corpus, which helps expand the query in a productive way. Our results show that with query expansion, NQE + NCS is able to perform better than using NCS alone.</p>
</td>
	<td>search </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/liu2019learning/">Learning to Sport and Refactor Inconsistent Method Names</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning to Sport and Refactor Inconsistent Method Names' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning to Sport and Refactor Inconsistent Method Names' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20to%20Sport%20and%20Refactor%20Inconsistent%20Method%20Names' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>K. Liu, D. Kim, T. F. Bissyand́e, T. Kim, K. Kim, A. Koyuncu, S. Kim, Y. Le Traon</td>
	<td>ICSE</td>
	<td><p>To ensure code readability and facilitate software maintenance, program methods must be named properly. In particular, method names must be consistent with the corresponding method implementations. Debugging method names remains an important topic in the literature, where various approaches analyze commonalities among method names in a large dataset to detect inconsistent method names and suggest better ones. We note that the state-of-the-art does not analyze the implemented code itself to assess consistency. We thus propose a novel automated approach to debugging method names based on the analysis of consistency between method names and method code. The approach leverages deep feature representation techniques adapted to the nature of each artifact. Experimental results on over 2.1 million Java methods show that we can achieve up to 15 percentage points improvement over the state-of-the-art, establishing a record performance of 67.9% F1-measure in identifying inconsistent method names. We further demonstrate that our approach yields up to 25% accuracy in suggesting full names, while the state-of-the-art lags far behind at 1.1% accuracy. Finally, we report on our success in fixing 66 inconsistent method names in a live study on projects in the wild.</p>
</td>
	<td>naming </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/liu2019generating/">Generating commit messages from diffs using pointer-generator network</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Generating commit messages from diffs using pointer-generator network' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Generating commit messages from diffs using pointer-generator network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Generating%20commit%20messages%20from%20diffs%20using%20pointer-generator%20network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Q. Liu, Z. Liu, H. Zhu, H. Fan, B. Du, Y. Qian</td>
	<td>MSR</td>
	<td><p>The commit messages in source code repositories are valuable but not easy to be generated manually in time for tracking issues, reporting bugs, and understanding codes. Recently published works indicated that the deep neural machine translation approaches have drawn considerable attentions on automatic generation of commit messages. However, they could not deal with out-of-vocabulary (OOV) words, which are essential context-specific identifiers such as class names and method names in code diffs. In this paper, we propose PtrGNCMsg, a novel approach which is based on an improved sequence-to-sequence model with the pointer-generator network to translate code diffs into commit messages. By searching the smallest identifier set with the highest probability, PtrGNCMsg outperforms recent approaches based on neural machine translation, and first enables the prediction of OOV words. The experimental results based on the corpus of diffs and manual commit messages from the top 2,000 Java projects in GitHub show that PtrGNCMsg outperforms the state-of-the-art approach with improved BLEU by 1.02, ROUGE-1 by 4.00 and ROUGE-L by 3.78, respectively.</p>
</td>
	<td>edit </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/markovtsev2019style/">STYLE-ANALYZER: fixing code style inconsistencies with interpretable unsupervised algorithms</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=STYLE-ANALYZER: fixing code style inconsistencies with interpretable unsupervised algorithms' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=STYLE-ANALYZER: fixing code style inconsistencies with interpretable unsupervised algorithms' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=STYLE-ANALYZER:%20fixing%20code%20style%20inconsistencies%20with%20interpretable%20unsupervised%20algorithms' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>V. Markovtsev, W. Long, H. Mougard, K. Slavnov, E. Bulychev</td>
	<td>MSR</td>
	<td><p>Source code reviews are manual, time-consuming, and expensive. Human involvement should be focused on analyzing the most relevant aspects of the program, such as logic and maintainability, rather than amending style, syntax, or formatting defects. Some tools with linting capabilities can format code automatically and report various stylistic violations for supported programming languages. They are based on rules written by domain experts, hence, their configuration is often tedious, and it is impractical for the given set of rules to cover all possible corner cases. Some machine learning-based solutions exist, but they remain uninterpretable black boxes. This paper introduces STYLE-ANALYZER, a new open source tool to automatically fix code formatting violations using the decision tree forest model which adapts to each codebase and is fully unsupervised. STYLE-ANALYZER is built on top of our novel assisted code review framework, Lookout. It accurately mines the formatting style of each analyzed Git repository and expresses the found format patterns with compact human-readable rules. STYLE-ANALYZER can then suggest style inconsistency fixes in the form of code review comments. We evaluate the output quality and practical relevance of STYLE-ANALYZER by demonstrating that it can reproduce the original style with high precision, measured on 19 popular JavaScript projects, and by showing that it yields promising results in fixing real style mistakes. STYLE-ANALYZER includes a web application to visualize how the rules are triggered. We release STYLE-ANALYZER as a reusable and extendable open source software package on GitHub for the benefit of the community.</p>
</td>
	<td>style </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/liu2019deepfuzz/">DeepFuzz: Automatic Generation of Syntax Valid C Programs for Fuzz Testing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=DeepFuzz: Automatic Generation of Syntax Valid C Programs for Fuzz Testing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=DeepFuzz: Automatic Generation of Syntax Valid C Programs for Fuzz Testing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=DeepFuzz:%20Automatic%20Generation%20of%20Syntax%20Valid%20C%20Programs%20for%20Fuzz%20Testing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>X. Liu, X. Li, R. Prajapati, D. Wu</td>
	<td>AAAI</td>
	<td><p>Compilers  are among  the  most  fundamental  programming
tools for building software. However, production compilers
remain  buggy.  Fuzz  testing  is  often  leveraged  with  newly-generated,
or  mutated  inputs  in  order  to  find  new  bugs  or security vulnerabilities.
In this paper, we propose a grammar-based fuzzing tool called DeepFuzz. Based on a generative
Sequence-to-Sequence model, DeepFuzz automatically and continuously generates well-formed
C programs. We use this set of new C programs to fuzz off-the-shelf C compilers, e.g. GCC and Clang/LLVM.
We present a detailed case study to analyze  the  success  rate  and  coverage  improvement  of  the
generated C programs for fuzz testing. We analyze the performance of DeepFuzz with three types of sampling
methods  as  well  as  three  types  of  generation  strategies.  Consequently, DeepFuzz 
improved the testing efficacy in regards to the line, function, and branch coverage. In our preliminary
study, we found and reported 8 bugs of GCC, all of which are actively being addressed by developers.</p>
</td>
	<td>fuzzing generation </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/mesbah2019deepdelta/">DeepDelta: Learning to Repair Compilation Errors</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=DeepDelta: Learning to Repair Compilation Errors' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=DeepDelta: Learning to Repair Compilation Errors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=DeepDelta:%20Learning%20to%20Repair%20Compilation%20Errors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>A. Mesbah, A. Rice, E. Johnstin, N. Glorioso</td>
	<td></td>
	<td><p>Programmers spend a substantial amount of time manually repairing
code that does not compile. We observe that the repairs for
any particular error class typically follow a pattern and are highly
mechanical. We propose a novel approach that automatically learns
these patterns with a deep neural network and suggests program
repairs for the most costly classes of build-time compilation failures.
We describe how we collect all build errors and the human-authored,
in-progress code changes that cause those failing builds to transition
to successful builds at Google. We generate an AST diff from the
textual code changes and transform it into a domain-specific
language called Delta that encodes the change that must be made
to make the code compile. We then feed the compiler diagnostic
information (as source) and the Delta changes that resolved the
diagnostic (as target) into a Neural Machine Translation network for
training. For the two most prevalent and costly classes of Java compilation errors,
namely missing symbols and mismatched methodsignatures, our system called DeepDelta,
generates the correct repair changes for 19,314 out of 38,788 (50%) of unseen compilation
errors. The correct changes are in the top three suggested axes 86% of the time on average.</p>
</td>
	<td>repair edit compilation </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/lin2019impact/">On the Impact of Refactoring Operations on Code Naturalness</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=On the Impact of Refactoring Operations on Code Naturalness' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=On the Impact of Refactoring Operations on Code Naturalness' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=On%20the%20Impact%20of%20Refactoring%20Operations%20on%20Code%20Naturalness' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>B. Lin, C. Nagy, G. Bavota, M. Lanza</td>
	<td>SANER</td>
	<td><p>Recent studies have demonstrated that software is natural, that is, its source code is highly repetitive and predictable like human languages. Also, previous studies suggested the existence of a relationship between code quality and its naturalness, presenting empirical evidence showing that buggy code is “less natural” than non-buggy code. We conjecture that this qualitynaturalness relationship could be exploited to support refactoring activities (e.g., to locate source code areas in need of refactoring). We perform a first step in this direction by analyzing whether refactoring can improve the naturalness of code. We use state-of-the-art tools to mine a large dataset of refactoring operations performed in open source systems. Then, we investigate the impact of different types of refactoring operations on the naturalness of the impacted code. We found that (i) code refactoring does not necessarily increase the naturalness of the refactored code; and (ii) the impact on the code naturalness strongly depends on the type of refactoring operations.</p>
</td>
	<td>language model refactoring </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/zhao2019neural/">Neural Networks for Modeling Source Code Edits</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Neural Networks for Modeling Source Code Edits' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Neural Networks for Modeling Source Code Edits' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Neural%20Networks%20for%20Modeling%20Source%20Code%20Edits' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>R. Zhao, D. Bieber, K. Swersky, D. Tarlow</td>
	<td></td>
	<td><p>Programming languages are emerging as a challenging and interesting domain for machine learning. A core task, which has received significant attention in recent years, is building generative models of source code. However, to our knowledge, previous generative models have always been framed in terms of generating static snapshots of code. In this work, we instead treat source code as a dynamic object and tackle the problem of modeling the edits that software developers make to source code files. This requires extracting intent from previous edits and leveraging it to generate subsequent edits. We develop several neural networks and use synthetic data to test their ability to learn challenging edit patterns that require strong generalization. We then collect and train our models on a large-scale dataset of Google source code, consisting of millions of fine-grained edits from thousands of Python developers. From the modeling perspective, our main conclusion is that a new composition of attentional and pointer network components provides the best overall performance and scalability. From the application perspective, our results provide preliminary evidence of the feasibility of developing tools that learn to predict future edits.</p>
</td>
	<td>edit </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/li2019using/">Using GGNN to recommend log statement level</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Using GGNN to recommend log statement level' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Using GGNN to recommend log statement level' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Using%20GGNN%20to%20recommend%20log%20statement%20level' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>M. Li, J. Pei, J. He, K. Song, F. Che, Y. Huang, C. Wang</td>
	<td></td>
	<td><p>In software engineering, log statement is an important part because programmers can’t access to users’ program and they can only rely on log message to find the root of bugs. The mechanism of “log level” allows developers and users to specify the appropriate amount of logs to print during the execution of the software. And 26\% of the log statement modification is to modify the level. We tried to use ML method to predict the suitable level of log statement. The specific model is GGNN(gated graph neural network) and we have drawn lessons from Microsoft’s research. In this work, we apply Graph Neural Networks to predict the usage of log statement level of some open source java projects from github. Given the good performance of GGNN in this task, we are confident that GGNN is an excellent choice for processing source code. We envision this model can play an important role in applying AI/ML technique for Software Development Life Cycle more broadly.</p>
</td>
	<td>GNN logging </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/lu2019program/">Program Classification Using Gated Graph Attention Neural Network for Online Programming Service</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Program Classification Using Gated Graph Attention Neural Network for Online Programming Service' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Program Classification Using Gated Graph Attention Neural Network for Online Programming Service' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Program%20Classification%20Using%20Gated%20Graph%20Attention%20Neural%20Network%20for%20Online%20Programming%20Service' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>M. Lu, D. Tan, N. Xiong, Z. Chen, H. Li</td>
	<td></td>
	<td><p>The online programing services, such as Github, TopCoder, and EduCoder, have promoted a lot of social interactions among the service users. However, the existing social interactions is rather limited and inefficient due to the rapid increasing of source-code repositories, which is difficult to explore manually. The emergence of source-code mining provides a promising way to analyze those source codes, so that those source codes can be relatively easy to understand and share among those service users. Among all the source-code mining attempts,program classification lays a foundation for various tasks related to source-code understanding, because it is impossible for a machine to understand a computer program if it cannot classify the program correctly. Although numerous machine learning models, such as the Natural Language Processing (NLP) based models and the Abstract Syntax Tree (AST) based models, have been proposed to classify computer programs based on their corresponding source codes, the existing works cannot fully characterize the source codes from the perspective of both the syntax and semantic information. To address this problem, we proposed a Graph Neural Network (GNN) based model, which integrates data flow and function call information to the AST,and applies an improved GNN model to the integrated graph, so as to achieve the state-of-art program classification accuracy. The experiment results have shown that the proposed work can classify programs with accuracy over 97%.</p>
</td>
	<td>GNN representation </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/ahmed2019learning/">Learning Lenient Parsing & Typing via Indirect Supervision</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning Lenient Parsing & Typing via Indirect Supervision' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning Lenient Parsing & Typing via Indirect Supervision' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20Lenient%20Parsing%20&%20Typing%20via%20Indirect%20Supervision' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>T. Ahmed, V. Hellendoorn, P. Devanbu</td>
	<td></td>
	<td><p>Both professional coders and teachers frequently deal with imperfect (fragmentary, incomplete, ill-formed) code. Such fragments are common in StackOverflow; students also frequently produce ill-formed code, for which instructors, TAs (or students themselves) must find repairs. In either case, the developer experience could be greatly improved if such code could somehow be parsed &amp; typed; this makes them more amenable to use within IDEs and allows early detection and repair of potential errors. We introduce a lenient parser, which can parse &amp; type fragments, even ones with simple errors. Training a machine learner to leniently parse &amp; type imperfect code requires a large training set of pairs of imperfect code and its repair (and/or type information); such training sets are limited by human effort and curation. In this paper, we present a novel indirectly supervised approach to train a lenient parser, without access to such human-curated training data. We leverage the huge corpus of mostly correct code available on Github, and the massive, efficient learning capacity of Transformer-based NN architectures. Using GitHub data, we first create a large dataset of fragments of code and corresponding tree fragments and type annotations; we then randomly corrupt the input fragments (while requiring correct output) by seeding errors that mimic corruptions found in StackOverflow and student data. Using this data, we train high-capacity transformer models to overcome both fragmentation and corruption. With this novel approach, we can achieve reasonable performance on parsing &amp; typing StackOverflow fragments; we also demonstrate that our approach achieves best-in-class performance on a large dataset of student errors.</p>
</td>
	<td>types </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/li2019improving/">Improving Bug Detection via Context-Based Code Representation Learning and Attention-Based Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Improving Bug Detection via Context-Based Code Representation Learning and Attention-Based Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Improving Bug Detection via Context-Based Code Representation Learning and Attention-Based Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Improving%20Bug%20Detection%20via%20Context-Based%20Code%20Representation%20Learning%20and%20Attention-Based%20Neural%20Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Y. Li, S. Wang, T. N. Nguyen, S. V. Nguyen</td>
	<td>OOPSLA 2019</td>
	<td><p>Bug detection has been shown to be an effective way to help developers in detecting bugs early, thus, saving much effort and time in software development process. Recently, deep learning-based bug detection approaches have gained successes over the traditional machine learning-based approaches, the rule-based program analysis approaches, and mining-based approaches. However, they are still limited in detecting bugs that involve multiple methods and suffer high rate of false positives. In this paper, we propose a combination approach with the use of contexts and attention neural network to overcome those limitations. We propose to use as the global context the Program Dependence Graph (PDG) and Data Flow Graph (DFG) to connect the method under investigation with the other relevant methods that might contribute to the buggy code. The global context is complemented by the local context extracted from the path on the AST built from the method’s body. The use of PDG and DFG enables our model to reduce the false positive rate, while to complement for the potential reduction in recall, we make use of the attention neural network mechanism to put more weights on the buggy paths in the source code. That is, the paths that are similar to the buggy paths will be ranked higher, thus, improving the recall of our model. We have conducted several experiments to evaluate our approach on a very large dataset with +4.973M methods in 92 different project versions. The results show that our tool can have a relative improvement up to 160% on F-score when comparing with the state-of-the-art bug detection approaches. Our tool can detect 48 true bugs in the list of top 100 reported bugs, which is 24 more true bugs when comparing with the baseline approaches. We also reported that our representation is better suitable for bug detection and relatively improves over the other representations up to 206% in accuracy.</p>
</td>
	<td>representation defect </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/zhang2019novel/">A Novel Neural Source Code Representation based on Abstract Syntax Tree</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Novel Neural Source Code Representation based on Abstract Syntax Tree' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Novel Neural Source Code Representation based on Abstract Syntax Tree' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=A%20Novel%20Neural%20Source%20Code%20Representation%20based%20on%20Abstract%20Syntax%20Tree' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>J. Zhang, X. Wang, H. Zhang, H Sun, K. Wang, X. Liu</td>
	<td>ICSE</td>
	<td><p>Exploiting machine learning techniques for analyzing programs has attracted much attention. One key problem is how to represent code fragments well for follow-up analysis. Traditional information retrieval based methods often treat programs as natural language texts, which could miss important semantic information of source code. Recently, state-of-the-art studies demonstrate that abstract syntax tree (AST) based neural models can better represent source code. However, the sizes of ASTs are usually large and the existing models are prone to the long-term dependency problem. In this paper, we propose a novel AST-based Neural Network (ASTNN) for source code representation. Unlike existing models that work on entire ASTs, ASTNN splits each large AST into a sequence of small statement trees, and encodes the statement trees to vectors by capturing the lexical and syntactical knowledge of statements. Based on the sequence of statement vectors, a bidirectional RNN model is used to leverage the naturalness of statements and finally produce the vector representation of a code fragment. We have applied our neural network based source code representation method to two common program comprehension tasks: source code classification and code clone detection. Experimental results on the two tasks indicate that our model is superior to state-of-the-art approaches.</p>
</td>
	<td>representation AST </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/leclair2019recommendations/">Recommendations for Datasets for Source Code Summarization</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Recommendations for Datasets for Source Code Summarization' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Recommendations for Datasets for Source Code Summarization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Recommendations%20for%20Datasets%20for%20Source%20Code%20Summarization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>A. LeClair, C. McMillan</td>
	<td>NAACL 2019</td>
	<td><p>Source Code Summarization is the task of writing short, natural language descriptions of source code. The main use for these descriptions is in software documentation e.g. the one-sentence Java method descriptions in JavaDocs. Code summarization is rapidly becoming a popular research problem, but progress is restrained due to a lack of suitable datasets. In addition, a lack of community standards for creating datasets leads to confusing and unreproducible research results – we observe swings in performance of more than 33% due only to changes in dataset design. In this paper, we make recommendations for these standards from experimental results. We release a dataset based on prior work of over 2.1m pairs of Java methods and one sentence method descriptions from over 28k Java projects. We describe the dataset and point out key differences from natural language data, to guide and support future researchers.</p>
</td>
	<td>summarization dataset </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/leclair2019neural/">A Neural Model for Generating Natural Language Summaries of Program Subroutines</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Neural Model for Generating Natural Language Summaries of Program Subroutines' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Neural Model for Generating Natural Language Summaries of Program Subroutines' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=A%20Neural%20Model%20for%20Generating%20Natural%20Language%20Summaries%20of%20Program%20Subroutines' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>A. LeClair, S. Jiang, C. McMillan</td>
	<td>ICSE</td>
	<td><p>Source code summarization – creating natural language descriptions of source code behavior – is a rapidly-growing research topic with applications to automatic documentation generation, program comprehension, and software maintenance. Traditional techniques relied on heuristics and templates built manually by human experts. Recently, data-driven approaches based on neural machine translation have largely overtaken template-based systems. But nearly all of these techniques rely almost entirely on programs having good internal documentation; without clear identifier names, the models fail to create good summaries. In this paper, we present a neural model that combines words from code with code structure from an AST. Unlike previous approaches, our model processes each data source as a separate input, which allows the model to learn code structure independent of the text in code. This process helps our approach provide coherent summaries in many cases even when zero internal documentation is provided. We evaluate our technique with a dataset we created from 2.1m Java methods. We find improvement over two baseline techniques from SE literature and one from NLP literature.</p>
</td>
	<td>summarization documentation </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/lacomis2019neural/">A Neural Approach to Decompiled Identifier Renaming</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Neural Approach to Decompiled Identifier Renaming' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Neural Approach to Decompiled Identifier Renaming' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=A%20Neural%20Approach%20to%20Decompiled%20Identifier%20Renaming' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>J. Lacomis, P. Yin, E.J. Schwartz, M. Allamanis, C. Le Goues, G. Neubig, B. Vasilescu</td>
	<td>ASE</td>
	<td><p>The decompiler is one of the most common tools for examining binaries without corresponding source code. It transforms binaries into high-level code, reversing the compilation process. However, compilation loses information contained within the original source code (e.g. structure, type information, and variable names). Semantically meaningful variable names are known to increase code understandability, but they generally cannot be recovered by decompilers. We propose the Decompiled Identifier Renaming Engine (DIRE), a novel probabilistic technique for variable name recovery that uses both lexical and structural information. We also present a technique for generating corpora suitable for training and evaluating models of decompiled code renaming, which we use to create a corpus of 164,632 unique x86-64 binaries generated from C projects mined from GitHub. Our results show that on this corpus DIRE can predict variable names identical to the names in the original source code up to 74.3% of the time.</p>
</td>
	<td>deobfuscation naming compilation </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/zhang2019learning/">Learning Uniform Semantic Features for Natural Language and Programming Language Globally, Locally and Sequentially</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning Uniform Semantic Features for Natural Language and Programming Language Globally, Locally and Sequentially' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning Uniform Semantic Features for Natural Language and Programming Language Globally, Locally and Sequentially' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20Uniform%20Semantic%20Features%20for%20Natural%20Language%20and%20Programming%20Language%20Globally,%20Locally%20and%20Sequentially' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Y. Zhang, W. Zheng, M. Li</td>
	<td>AAAI</td>
	<td><p>Semantic feature learning for natural language and programming language is a preliminary step in addressing many software mining tasks. Many existing methods leverage
information in lexicon and syntax to learn features for textual data.
However, such information is inadequate to represent the entire semantics in either text sentence or code snippet. This
motivates us to propose a new approach to learn semantic
features for both languages, through extracting three levels of
information, namely global, local and sequential information,
from textual data. For tasks involving both modalities, we
project the data of both types into a uniform feature space so
that the complementary knowledge in between can be utilized
in their representation. In this paper, we build a novel and
general-purpose feature learning framework called UniEmbed, to uniformly learn comprehensive semantic representation for both natural language and programming language.
Experimental results on three real-world software mining
tasks show that UniEmbed outperforms state-of-the-art models in feature learning and prove the capacity and effectiveness of our model.</p>
</td>
	<td>representation bimodal </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/allamanis2019adverse/">The Adverse Effects of Code Duplication in Machine Learning Models of Code</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=The Adverse Effects of Code Duplication in Machine Learning Models of Code' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=The Adverse Effects of Code Duplication in Machine Learning Models of Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=The%20Adverse%20Effects%20of%20Code%20Duplication%20in%20Machine%20Learning%20Models%20of%20Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>M. Allamanis</td>
	<td></td>
	<td><p>The field of big code relies on mining large corpora of code to perform some learning task. A significant threat to this approach has been recently identified by Lopes et al. (2017) who found a large amount of code duplication on GitHub. However, the impact of code duplication has not been noticed by researchers devising machine learning models for source code. In this article, we study the effect of code duplication to machine learning models showing that reported metrics are sometimes inflated by up to 100% when testing on duplicated code corpora compared to the performance on de-duplicated corpora which more accurately represent how machine learning models of code are used by software engineers. We present an “errata” for widely used datasets, list best practices for collecting code corpora and evaluating machine learning models on them, and release tools to help the community avoid this problem in future research.</p>
</td>
	<td>dataset </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/kulal2019spoc/">SPoC: Search-based Pseudocode to Code</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=SPoC: Search-based Pseudocode to Code' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=SPoC: Search-based Pseudocode to Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=SPoC:%20Search-based%20Pseudocode%20to%20Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>S. Kulal, P. Pasupat, K. Chandra, M. Lee, O. Padon, A. Aiken, P. Liang</td>
	<td></td>
	<td><p>We consider the task of mapping pseudocode to long programs that are functionally correct. Given test cases as a mechanism to validate programs, we search over the space of possible translations of the pseudocode to find a program that passes the validation. However, without proper credit assignment to localize the sources of program failures, it is difficult to guide search toward more promising programs. We propose to perform credit assignment based on signals from compilation errors, which constitute 88.7% of program failures. Concretely, we treat the translation of each pseudocode line as a discrete portion of the program, and whenever a synthesized program fails to compile, an error localization method tries to identify the portion of the program responsible for the failure. We then focus search over alternative translations of the pseudocode for those portions. For evaluation, we collected the SPoC dataset (Search-based Pseudocode to Code) containing 18,356 programs with human-authored pseudocode and test cases. Under a budget of 100 program compilations, performing search improves the synthesis success rate over using the top-one translation of the pseudocode from 25.6% to 44.7%.</p>
</td>
	<td>bimodal synthesis </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/kovalenko2019pathminer/">PathMiner : A Library for Mining of Path-Based Representations of Code</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=PathMiner : A Library for Mining of Path-Based Representations of Code' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=PathMiner : A Library for Mining of Path-Based Representations of Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=PathMiner%20:%20A%20Library%20for%20Mining%20of%20Path-Based%20Representations%20of%20Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>V. Kovalenko, E. Bogomolov, A. Bacchelli</td>
	<td>MSR</td>
	<td><p>One recent, significant advance in modeling source code for machine learning algorithms has been the introduction of path-based representation – an approach consisting in representing a snippet of code as a collection of paths from its syntax tree. Such representation efficiently captures the structure of code, which, in turn, carries its semantics and other information.
Building the path-based representation involves parsing the code and extracting the paths from its syntax tree; these steps build up to a substantial technical job. With no common reusable toolkit existing for this task, the burden of mining diverts the focus of researchers from the essential work and hinders newcomers in the field of machine learning on code.</p>

<p>In this paper, we present PathMiner – an open-source library for mining path-based representations of code. PathMiner is fast, flexible, well-tested, and easily extensible to support input code in any common programming language. Preprint [https://doi.org/10.5281/zenodo.2595271]; released tool [https://doi.org/10.5281/zenodo.2595257].</p>
</td>
	<td>representation AST </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/yonai2019mercem/">Mercem: Method Name Recommendation Based on Call Graph Embedding</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Mercem: Method Name Recommendation Based on Call Graph Embedding' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Mercem: Method Name Recommendation Based on Call Graph Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Mercem:%20Method%20Name%20Recommendation%20Based%20on%20Call%20Graph%20Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>H. Yonai, Y. Hayase, H. Kitagawa</td>
	<td></td>
	<td><p>Comprehensibility of source code is strongly affected by identifier names, therefore software developers need to give good (e.g. meaningful but short) names to identifiers. On the other hand, giving a good name is sometimes a difficult and time-consuming task even for experienced developers. To support naming identifiers, several techniques for recommending identifier name candidates have been proposed. These techniques, however, still have challenges on the goodness of suggested candidates and limitations on applicable situations. This paper proposes a new approach to recommending method names by applying graph embedding techniques to the method call graph. The evaluation experiment confirms that the proposed technique can suggest more appropriate method name candidates in difficult situations than the state of the art approach.</p>
</td>
	<td>naming representation refactoring </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/katz2019towards/">Towards Neural Decompilation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards Neural Decompilation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards Neural Decompilation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Towards%20Neural%20Decompilation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>O. Katz, Y. Olshaker, Y. Goldberg, E. Yahav</td>
	<td></td>
	<td><p>We address the problem of automatic decompilation, converting a program in low-level representation back to a higher-level human-readable programming language. The problem of decompilation is extremely important for security researchers. Finding vulnerabilities and understanding how malware operates is much easier when done over source code.</p>

<p>The importance of decompilation has motivated the construction of hand-crafted rule-based decompilers. Such decompilers have been designed by experts to detect specific control-flow structures and idioms in low-level code and lift them to source level. The cost of supporting additional languages or new language features in these models is very high.</p>

<p>We present a novel approach to decompilation based on neural machine translation. The main idea is to automatically learn a decompiler from a given compiler. Given a compiler from a source language S to a target language T , our approach automatically trains a decompiler that can translate (decompile) T back to S . We used our framework to decompile both LLVM IR and x86 assembly to C code with high success rates. Using our LLVM and x86 instantiations, we were able to successfully decompile over 97% and 88% of our benchmarks respectively.</p>
</td>
	<td>decompilation </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/nguyen2019graph/">Graph-based Mining of In-the-Wild, Fine-grained, Semantic Code Change Patterns</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Graph-based Mining of In-the-Wild, Fine-grained, Semantic Code Change Patterns' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Graph-based Mining of In-the-Wild, Fine-grained, Semantic Code Change Patterns' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Graph-based%20Mining%20of%20In-the-Wild,%20Fine-grained,%20Semantic%20Code%20Change%20Patterns' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>H. Nguyen, T. Nguyen, D. Dig, S. Nguyen, H. Tran, M. Hilton</td>
	<td>ICSE</td>
	<td><p>Existing approaches for detecting repetitive code changes relying on syntactic similarity cannot effectively detect semantic change patterns. In this work, we introduce a novel graph-based mining approach, CPatMiner, which is capable of detecting semantic code change patterns from a large number of open-source repositories by capturing dependencies between fine-grained change elements. We evaluated CPatMiner by mining change patterns in a diverse corpus of 5,000+ open-source projects from GitHub with 170,000+ developers. We use three complementary methods. First, we sent the mined patterns to the authors and received 108 responses. 70% of respondents recognized those patterns as their meaningful frequent changes. 79% of respondents even named the patterns, and 44% wanted IDEs to automate such repetitive changes. The mined patterns belong to various activities: adaptive (9%), perfective (20%), corrective (35%) and preventive (36%). Second, we compared CPatMiner with the state-of-the-art, AST-based technique, and reported that CPatMiner detects 2.1x more meaningful patterns. Third, we used CPatMiner to search for patterns in a corpus of 88 GitHub projects with longer histories consisting of 164M SLOCs. It constructed 322K fine-grained change graphs containing 3M nodes, and detected 17K change patterns which provide unique insights on the practice of change patterns among individuals and teams. We found that a large percentage (75%) of the patterns from individual developers are commonly shared with others, and this holds true for teams. Moreover, we found that the patterns spread widely over time. Thus, we call for a community-based change pattern database to provide important resources in novel applications.</p>
</td>
	<td>edit pattern mining </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/yin2019learning/">Learning to Represent Edits</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning to Represent Edits' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning to Represent Edits' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20to%20Represent%20Edits' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>P. Yin, G. Neubig, M. Allamanis, M. Brockschmidt, A. L. Gaunt</td>
	<td>ICLR</td>
	<td><p>We introduce the problem of learning distributed representations of edits. By combining a
“neural editor” with an “edit encoder”, our models learn to represent the salient
information of an edit and can be used to apply edits to new inputs.
We experiment on natural language and source code edit data. Our evaluation yields
promising results that suggest that our neural network models learn to capture
the structure and semantics of edits. We hope that this interesting task and
data source will inspire other researchers to work further on this problem.</p>
</td>
	<td>edit </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/alon2018code2seq/">code2seq: Generating Sequences from Structured Representations of Code</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=code2seq: Generating Sequences from Structured Representations of Code' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=code2seq: Generating Sequences from Structured Representations of Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=code2seq:%20Generating%20Sequences%20from%20Structured%20Representations%20of%20Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>U. Alon, O. Levy, E. Yahav</td>
	<td>ICLR</td>
	<td><p>The ability to generate natural language sequences from source code snippets has a variety of applications such as code summarization, documentation, and retrieval. Sequence-to-sequence (seq2seq) models, adopted from neural machine translation (NMT), have achieved state-of-the-art performance on these tasks by treating source code as a sequence of tokens. We present code2seq: an alternative approach that leverages the syntactic structure of programming languages to better encode source code. Our model represents a code snippet as the set of compositional paths in its abstract syntax tree (AST) and uses attention to select the relevant paths while decoding.</p>

<p>We demonstrate the effectiveness of our approach for two tasks, two programming languages, and four datasets of up to 16M examples. Our model significantly outperforms previous models that were specifically designed for programming languages, as well as general state-of-the-art NMT models. An interactive online demo of our model is available at http://code2seq.org.</p>
</td>
	<td>naming summarization representation </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/karampatsis2019deep/">Maybe Deep Neural Networks are the Best Choice for Modeling Source Code</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Maybe Deep Neural Networks are the Best Choice for Modeling Source Code' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Maybe Deep Neural Networks are the Best Choice for Modeling Source Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Maybe%20Deep%20Neural%20Networks%20are%20the%20Best%20Choice%20for%20Modeling%20Source%20Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>R.M. Karampatsis, C. Sutton</td>
	<td></td>
	<td><p>Statistical language modeling techniques have successfully been applied to source code, yielding a variety of new software development tools, such as tools for code suggestion and improving readability. A major issue with these techniques is that code introduces new vocabulary at a far higher rate than natural language, as new identifier names proliferate. But traditional language models limit the vocabulary to a fixed set of common words. For code, this strong assumption has been shown to have a significant negative effect on predictive performance. But the open vocabulary version of the neural network language models for code have not been introduced in the literature. We present a new open-vocabulary neural language model for code that is not limited to a fixed vocabulary of identifier names. We employ a segmentation into subword units, subsequences of tokens chosen based on a compression criterion, following previous work in machine translation. Our network achieves best in class performance, outperforming even the state-of-the-art methods of Hellendoorn and Devanbu that are designed specifically to model code. Furthermore, we present a simple method for dynamically adapting the model to a new test project, resulting in increased performance. We showcase our methodology on code corpora in three different languages of over a billion tokens each, hundreds of times larger than in previous work. To our knowledge, this is the largest neural language model for code that has been reported.</p>
</td>
	<td>language model </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/alon2019code2vec/">code2vec: Learning Distributed Representations of Code</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=code2vec: Learning Distributed Representations of Code' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=code2vec: Learning Distributed Representations of Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=code2vec:%20Learning%20Distributed%20Representations%20of%20Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>U. Alon, O. Levy, E. Yahav</td>
	<td>POPL</td>
	<td><p>We present a neural model for representing snippets of code as continuous distributed vectors (“code embeddings”).
 The main idea is to represent a code snippet as a single fixed-length
code vector, which can be used to
predict semantic properties of the snippet. To this end, code is first decomposed to a collection of paths in its
abstract syntax tree. Then, the network learns the atomic representation of each path while
simultaneously
learning how to aggregate a set of them.</p>

<p>We demonstrate the effectiveness of our approach by using it to predict a method’s name from the vector
representation of its body. We evaluate our approach by training a model on a dataset of 12M methods. We
show that code vectors trained on this dataset can predict method names from files that were unobserved
during training. Furthermore, we show that our model learns useful method name vectors that capture
semantic similarities, combinations, and analogies.</p>

<p>A comparison of our approach to previous techniques over the same dataset shows an improvement of
more than 75%, making it the first to successfully predict method names based on a large, cross-project
corpus. Our trained model, visualizations and vector similarities are available as an interactive online demo at
http://code2vec.org. The code, data and trained models are available at
https://github.com/tech-srl/code2vec.</p>
</td>
	<td>naming summarization representation </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/kacmajor2019automatic/">Automatic Acquisition of Annotated Training Corpora for Test-Code Generation </a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Automatic Acquisition of Annotated Training Corpora for Test-Code Generation ' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Automatic Acquisition of Annotated Training Corpora for Test-Code Generation ' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Automatic%20Acquisition%20of%20Annotated%20Training%20Corpora%20for%20Test-Code%20Generation%20' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>M. Kacmajor, J. D. Kelleher</td>
	<td>Information</td>
	<td><p>Open software repositories make large amounts of source code publicly available. Potentially, this source code could be used as training data to develop new, machine learning-based programming tools. For many applications, however, raw code scraped from online repositories does not constitute an adequate training dataset. Building on the recent and rapid improvements in machine translation (MT), one possibly very interesting application is code generation from natural language descriptions. One of the bottlenecks in developing these MT-inspired systems is the acquisition of parallel text-code corpora required for training code-generative models. This paper addresses the problem of automatically synthetizing parallel text-code corpora in the software testing domain. Our approach is based on the observation that self-documentation through descriptive method names is widely adopted in test automation, in particular for unit testing. Therefore, we propose synthesizing parallel corpora comprised of parsed test function names serving as code descriptions, aligned with the corresponding function bodies. We present the results of applying one of the state-of-the-art MT methods on such a generated dataset. Our experiments show that a neural MT model trained on our dataset can generate syntactically correct and semantically relevant short Java functions from quasi-natural language descriptions of functionality.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/yefet2019adversarial/">Adversarial Examples for Models of Code</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Adversarial Examples for Models of Code' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Adversarial Examples for Models of Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Adversarial%20Examples%20for%20Models%20of%20Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>N. Yefet, U. Alon, E. Yahav</td>
	<td></td>
	<td><p>Neural models of code have shown impressive performance for tasks such as predicting method names and identifying certain kinds of bugs. In this paper, we show that these models are vulnerable to adversarial examples, and introduce a novel approach for attacking trained models of code with adversarial examples. The main idea is to force a given trained model to make an incorrect prediction as specified by the adversary by introducing small perturbations that do not change the program’s semantics. To find such perturbations, we present a new technique for Discrete Adversarial Manipulation of Programs (DAMP). DAMP works by deriving the desired prediction with respect to the model’s inputs while holding the model weights constant and following the gradients to slightly modify the code.</p>

<p>To defend a model against such attacks, we propose placing a defensive model (Anti-DAMP) in front of it. Anti-DAMP detects unlikely mutations and masks them before feeding the input to the downstream model.</p>

<p>We show that our DAMP attack is effective across three neural architectures: code2vec, GGNN, and GNN-FiLM, in both Java and C#. We show that DAMP has up to 89% success rate in changing a prediction to the adversary’s choice (“targeted attack”), and a success rate of up to 94% in changing a given prediction to any incorrect prediction (“non-targeted attack”). By using Anti-DAMP, the success rate of the attack drops drastically for both targeted and non-targeted attacks, with a minor penalty of 2% relative degradation in accuracy while not performing under attack.</p>
</td>
	<td>adversarial </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/yao2019coacor/">CoaCor: Code Annotation for Code Retrieval with Reinforcement Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=CoaCor: Code Annotation for Code Retrieval with Reinforcement Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=CoaCor: Code Annotation for Code Retrieval with Reinforcement Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=CoaCor:%20Code%20Annotation%20for%20Code%20Retrieval%20with%20Reinforcement%20Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Z Yao, JR Peddamail, H. Sun</td>
	<td></td>
	<td><p>To accelerate software development, much research has been performed
to help people understand and reuse the huge amount of available code
resources. Two important tasks have been widely studied: code retrieval,
which aims to retrieve code snippets relevant to a given natural language
query from a code base, and code annotation, where the goal is to annotate a 
code snippet with anatural language description. Despite their advancement in recent
years, the two tasks are mostly explored separately. In this work, we
investigate a novel perspective of Code annotation for Code retrieval 
(hence called “CoaCor”), where a code annotation model is trained
to generate a natural language annotation that can represent the
semantic meaning of a given code snippet and can be leveraged by
a code retrieval model to better distinguish relevant code snippets
from others. To this end, we propose an effective framework based
on reinforcement learning, which explicitly encourages the code
annotation model to generate annotations that can be used for the
retrieval task. Through extensive experiments, we show that code
annotations generated by our framework are much more detailed
and more useful for code retrieval, and they can further improve
the performance of existing code retrieval models significantly.</p>
</td>
	<td>search </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/alon2019structural/">Structural Language Models for Any-Code Generation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Structural Language Models for Any-Code Generation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Structural Language Models for Any-Code Generation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Structural%20Language%20Models%20for%20Any-Code%20Generation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>U. Alon, R. Sadaka, O. Levy, E. Yahav</td>
	<td></td>
	<td><p>We address the problem of Any-Code Generation (AnyGen) - generating code without any restriction on the vocabulary or structure. The state-of-the-art in this problem is the sequence-to-sequence (seq2seq) approach, which treats code as a sequence and does not leverage any structural information. We introduce a new approach to AnyGen that leverages the strict syntax of programming languages to model a code snippet as a tree - structural language modeling (SLM). SLM estimates the probability of the program’s abstract syntax tree (AST) by decomposing it into a product of conditional probabilities over its nodes. We present a neural model that computes these conditional probabilities by considering all AST paths leading to a target node. Unlike previous structural techniques that have severely restricted the kinds of expressions that can be generated, our approach can generate arbitrary expressions in any programming language. Our model significantly outperforms both seq2seq and a variety of existing structured approaches in generating Java and C# code. We make our code, datasets, and models available online.</p>
</td>
	<td>generation </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/jayasundara2019treecaps/">TreeCaps: Tree-Structured Capsule Networks for Program Source Code Processing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=TreeCaps: Tree-Structured Capsule Networks for Program Source Code Processing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=TreeCaps: Tree-Structured Capsule Networks for Program Source Code Processing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=TreeCaps:%20Tree-Structured%20Capsule%20Networks%20for%20Program%20Source%20Code%20Processing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>V. Jayasundara, N. Bui, L. Jiang, D. Lo</td>
	<td></td>
	<td><p>Program comprehension is a fundamental task in software development and maintenance processes. Software developers often need to understand a large amount of existing code before they can develop new features or fix bugs in existing programs. Being able to process programming language code automatically and provide summaries of code functionality accurately can significantly help developers to reduce time spent in code navigation and understanding, and thus increase productivity. Different from natural language articles, source code in programming languages often follows rigid syntactical structures and there can exist dependencies among code elements that are located far away from each other through complex control flows and data flows. Existing studies on tree-based convolutional neural networks (TBCNN) and gated graph neural networks (GGNN) are not able to capture essential semantic dependencies among code elements accurately. In this paper, we propose novel tree-based capsule networks (TreeCaps) and relevant techniques for processing program code in an automated way that encodes code syntactical structures and captures code dependencies more accurately. Based on evaluation on programs written in different programming languages, we show that our TreeCaps-based approach can outperform other approaches in classifying the functionalities of many programs.</p>
</td>
	<td>representation </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/iyer2019learning/">Learning Programmatic Idioms for Scalable Semantic Parsing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning Programmatic Idioms for Scalable Semantic Parsing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning Programmatic Idioms for Scalable Semantic Parsing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20Programmatic%20Idioms%20for%20Scalable%20Semantic%20Parsing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>S. Iyer, A. Cheung, L. Zettlemoyer</td>
	<td></td>
	<td><p>Programmers typically organize executable source code using high-level coding patterns or idiomatic structures such as nested loops, exception handlers and recursive blocks, rather than as individual code tokens. In contrast, state of the art semantic parsers still map natural language instructions to source code by building the code syntax tree one node at a time. In this paper, we introduce an iterative method to extract code idioms from large source code corpora by repeatedly collapsing most-frequent depth-2 subtrees of their syntax trees, and we train semantic parsers to apply these idioms during decoding. We apply this idiom-based code generation to a recent context-dependent semantic parsing task, and improve the state of the art by 2.2% BLEU score while reducing training time by more than 50%. This improved speed enables us to scale up the model by training on an extended training set that is 5x times larger, to further move up the state of the art by an additional 2.3% BLEU and 0.9% exact match.</p>
</td>
	<td>idiom generation AST </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/hussain2019deep/">Deep Transfer Learning for Source Code Modeling</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Transfer Learning for Source Code Modeling' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Transfer Learning for Source Code Modeling' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Transfer%20Learning%20for%20Source%20Code%20Modeling' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Y. Hussain, Z. Huang, Y. Zhou, S. Wang</td>
	<td></td>
	<td><p>In recent years, deep learning models have shown great potential in source code modeling and analysis. Generally, deep learning-based approaches are problem-specific and data-hungry. A challenging issue of these approaches is that they require training from starch for a different related problem. In this work, we propose a transfer learning-based approach that significantly improves the performance of deep learning-based source code models. In contrast to traditional learning paradigms, transfer learning can transfer the knowledge learned in solving one problem into another related problem. First, we present two recurrent neural network-based models RNN and GRU for the purpose of transfer learning in the domain of source code modeling. Next, via transfer learning, these pre-trained (RNN and GRU) models are used as feature extractors. Then, these extracted features are combined into attention learner for different downstream tasks. The attention learner leverages from the learned knowledge of pre-trained models and fine-tunes them for a specific downstream task. We evaluate the performance of the proposed approach with extensive experiments with the source code suggestion task. The results indicate that the proposed approach outperforms the state-of-the-art models in terms of accuracy, precision, recall, and F-measure without training the models from scratch.</p>
</td>
	<td>pretraining </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/husain2019codesearchnet/">CodeSearchNet Challenge: Evaluating the State of Semantic Code Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=CodeSearchNet Challenge: Evaluating the State of Semantic Code Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=CodeSearchNet Challenge: Evaluating the State of Semantic Code Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=CodeSearchNet%20Challenge:%20Evaluating%20the%20State%20of%20Semantic%20Code%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>H. Husain, H. Wu, T. Gazit, M. Allamanis, M. Brockschmidt</td>
	<td></td>
	<td><p>Semantic code search is the task of retrieving relevant code given a natural language query. While related to other information retrieval tasks, it requires bridging the gap between the language used in code (often abbreviated and highly technical) and natural language more suitable to describe vague concepts and ideas.</p>

<p>To enable evaluation of progress on code search, we are releasing the CodeSearchNet Corpus and are presenting the CodeSearchNet Challenge, which consists of 99 natural language queries with about 4k expert relevance annotations of likely results from CodeSearchNet Corpus. The corpus contains about 6 million functions from open-source code spanning six programming languages (Go, Java, JavaScript, PHP, Python, and Ruby). The CodeSearchNet Corpus also contains automatically generated query-like natural language for 2 million functions, obtained from mechanically scraping and preprocessing associated function documentation. In this article, we describe the methodology used to obtain the corpus and expert labels, as well as a number of simple baseline solutions for the task.</p>

<p>We hope that CodeSearchNet Challenge encourages researchers and practitioners to study this interesting task further and will host a competition and leaderboard to track the progress on the challenge. We are also keen on extending CodeSearchNet Challenge to more queries and programming languages in the future.</p>
</td>
	<td>dataset retrieval </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/pradel2019typewriter/">TypeWriter: Neural Type Prediction with Search-based Validation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=TypeWriter: Neural Type Prediction with Search-based Validation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=TypeWriter: Neural Type Prediction with Search-based Validation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=TypeWriter:%20Neural%20Type%20Prediction%20with%20Search-based%20Validation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>M. Pradel, G. Gousios, J. Liu, S. Chandra</td>
	<td></td>
	<td><p>Maintaining large code bases written in dynamically typed languages, such as JavaScript or Python, can be challenging: simple data compatibility errors proliferate, IDE support is lacking and APIs are harder to comprehend. Recent work attempts to address those issues through either static analysis or probabilistic type inference. Unfortunately, static type inference for dynamic languages is inherently limited, while probabilistic approaches suffer from imprecision. This paper presents TypeWriter, the first combination of probabilistic prediction with search-based refinement of predicted types. TypeWriter’s predictor learns to infer the return and argument types for functions from partially annotated code bases by combining the natural language properties of code with programming language-level information. To validate predicted types, TypeWriter invokes a gradual type checker with different combinations of the predicted types, while navigating the space of possible type combinations in a feedback-directed manner. We implement the TypeWriter approach for Python and evaluate it on two code corpora: a multi-million line code base at Facebook and a collection of 500 popular open-source projects. We show that TypeWriter’s type predictor achieves a precision of 64% (91%) and a recall of 52% (68%) in the top-1 (top-5) predictions, and demonstrate that usage contexts are a helpful addition to neural type predictors. By combining predictions with search-based validation, TypeWriter can fully annotate between 42% to 64% of the files in a randomly selected corpus, while ensuring type correctness. A comparison with a static type inference tool shows that TypeWriter adds many more non-trivial types. Overall, TypeWriter provides developers with an effective way to help with the transition to fully type-annotated code.</p>
</td>
	<td>types bimodal </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/xu2019method/">Method name suggestion with hierarchical attention networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Method name suggestion with hierarchical attention networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Method name suggestion with hierarchical attention networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Method%20name%20suggestion%20with%20hierarchical%20attention%20networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>S. Xu, S. Zhang, W. Wang, X. Cao, C. Guo, J. Xu</td>
	<td>PEPM</td>
	<td><p>Method Rename has been a widely used refactoring operation that improves program comprehension and maintenance. Descriptive method names that summarize functionalities of source code can facilitate program comprehension. Much research has been done to suggest method names through source code summarization. However, unlike natural language, a code snippet consists of basic blocks organized by complicated structures. In this work, we observe a hierarchical structure — tokens form basic blocks and basic blocks form a code snippet. Based on this observation, we exploit a hierarchical attention network to learn the representation of methods. Specifically, we apply two-level attention mechanism to learn the importance of each token in a basic block and that of a basic block in a method respectively. We evaluated our approach on 10 open source repositories and compared it against three state-of-the-art approaches. The results on these open-source data show the superiority of our hierarchical attention networks in terms of effectiveness.</p>
</td>
	<td>naming </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/xu2019commit/">Commit Message Generation for Source Code Changes</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Commit Message Generation for Source Code Changes' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Commit Message Generation for Source Code Changes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Commit%20Message%20Generation%20for%20Source%20Code%20Changes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>S. Xu, Y. Yao, F. Xu, T. Gu, H. Tong, J. Lu</td>
	<td>IJCAI</td>
	<td><p>Commit  messages,  which  summarize  the  source
code changes in natural language, are essential for
program comprehension and software evolution understanding.  Unfortunately, due to the lack of direct
motivation,  commit  messages  are  sometimes neglected  by  developers,  making  it  necessary  to
automatically  generate  such  messages.    State-of-the-art  adopts  learning  based  approaches  such  as
neural machine translation models for the commitmessage generation problem.  However, they tend
to  ignore  the  code  structure  information  and  suffer from the out-of-vocabulary issue.
In this paper, we  propose  CODISUM to  address  the  above  two limitations. In particular,
we first extract both code structure and code semantics from the source code changes, and then
jointly model these two sources of  information  so  as  to  better  learn  the  representations
 of  the  code  changes.   Moreover,  we  augment  the  model  with  copying  mechanism  to  further
mitigate  the  out-of-vocabulary  issue.   Experimental  evaluations  on  real  data  demonstrate  that
the  proposed  approach  significantly  outperforms the state-of-the-art in terms of accurately generating the commit messages.</p>
</td>
	<td>edit summarization </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/he2019learning/">Learning to Fuzz from Symbolic Execution with Application to Smart Contracts</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning to Fuzz from Symbolic Execution with Application to Smart Contracts' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning to Fuzz from Symbolic Execution with Application to Smart Contracts' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20to%20Fuzz%20from%20Symbolic%20Execution%20with%20Application%20to%20Smart%20Contracts' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>J. He, M. Balunovic, N. Ambroladze, P. Tsanakov, M. Vechev</td>
	<td>CCS</td>
	<td><p>Fuzzing and symbolic execution are two complementary techniques for discovering software vulnerabilities. Fuzzing is fast and scalable, but can be ineffective when it fails to randomly select the right inputs. Symbolic execution is thorough but slow and often does not scale to deep program paths with complex path conditions. In this work, we propose to learn an effective and fast fuzzer from symbolic execution, by phrasing the learning task in the framework of imitation learning. During learning, a symbolic execution expert generates a large number of quality inputs improving coverage on thousands of programs. Then, a fuzzing policy, represented with a suitable architecture of neural networks, is trained on the generated dataset. The learned policy can then be used to fuzz new programs. We instantiate our approach to the problem of fuzzing smart contracts, a domain where contracts often implement similar functionality (facilitating learning) and security is of utmost importance. We present an end-to-end system, ILF (for Imitation Learning based Fuzzer), and an extensive evaluation over &gt;18K contracts. Our results show that ILF is effective: (i) it is fast, generating 148 transactions per second, (ii) it outperforms existing fuzzers (e.g., achieving 33% more coverage), and (iii) it detects more vulnerabilities than existing fuzzing and symbolic execution tools for Ethereum.</p>
</td>
	<td>fuzzing GNN </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/hajipour2019samplefix/">SampleFix: Learning to Correct Programs by Sampling Diverse Fixes</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=SampleFix: Learning to Correct Programs by Sampling Diverse Fixes' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=SampleFix: Learning to Correct Programs by Sampling Diverse Fixes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=SampleFix:%20Learning%20to%20Correct%20Programs%20by%20Sampling%20Diverse%20Fixes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>H. Hajipour, A. Bhattacharya, M. Fritz</td>
	<td></td>
	<td><p>Automatic program correction is an active topic of research, which holds the potential of dramatically improving productivity of programmers during the software development process and correctness of software in general. Recent advances in machine learning, deep learning and NLP have rekindled the hope to eventually fully automate the process of repairing programs. A key challenges is ambiguity, as multiple codes – or fixes – can implement the same functionality. In addition, dataset by nature fail to capture the variance introduced by such ambiguities. Therefore, we propose a deep generative model to automatically correct programming errors by learning a distribution of potential fixes. Our model is formulated as a deep conditional variational autoencoder that samples diverse fixes for the given erroneous programs. In order to account for ambiguity and inherent lack of representative datasets, we propose a novel regularizer to encourage the model to generate diverse fixes. Our evaluations on common programming errors show for the first time the generation of diverse fixes and strong improvements over the state-of-the-art approaches by fixing up to 61% of the mistakes.</p>
</td>
	<td>repair generation </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/rahman2019natural/">Natural Software Revisited</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Natural Software Revisited' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Natural Software Revisited' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Natural%20Software%20Revisited' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>M. Rahman, D. Palani, P. Rigby</td>
	<td>ICSE</td>
	<td><p>Recent works have concluded that software is more repetitive and predictable, i.e. more natural, than English texts. These works included “simple/artificial” syntax rules in their language models. When we remove SyntaxTokens we find that code is still repetitive and predictable but only at levels slightly above English. Furthermore, previous works have compared individual Java programs to general English corpora, such as Gutenberg, which contains a historically large range of styles and subjects (e.g. Saint Augustine to Oscar Wilde). We perform an additional comparison of technical StackOverflow English discussions with source code and find that this restricted English is similarly repetitive to code. Although we find that code is less repetitive than previously thought, we suspect that API code element usage will be repetitive across software projects. For example a file is opened and closed in the same manner irrespective of domain. When we restrict our n-grams to those contained in the Java API we find that the entropy is significantly lower than the English corpora. Previous works have focused on sequential sequences of tokens. When we extract program graphs of size 2, 3, and 4 nodes we see that the abstract graph representation is much more concise and repetitive than the sequential representations of the same code. This suggests that future work should focus on statistical graph models that go beyond linear sequences of tokens. Our anonymous replication package makes our scripts and data available to future researchers and reviewers.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/bavishi2019autopandas/">AutoPandas: neural-backed generators for program synthesis</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=AutoPandas: neural-backed generators for program synthesis' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=AutoPandas: neural-backed generators for program synthesis' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=AutoPandas:%20neural-backed%20generators%20for%20program%20synthesis' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>R. Bavishi, C. Lemieux, R. Fox, K. Sen, I. Stoica</td>
	<td>OOPSLA</td>
	<td><p>Developers nowadays have to contend with a growing number of APIs. While in the long-term they are very useful to developers, many modern APIs have an incredibly steep learning curve, due to their hundreds of functions handling many arguments, obscure documentation, and frequently changing semantics. For APIs that perform data transformations, novices can often provide an I/O example demonstrating the desired transformation, but may be stuck on how to translate it to the API. A programming-by-example synthesis engine that takes such I/O examples and directly produces programs in the target API could help such novices. Such an engine presents unique challenges due to the breadth of real-world APIs, and the often-complex constraints over function arguments. We present a generator-based synthesis approach to contend with these problems. This approach uses a program candidate generator, which encodes basic constraints on the space of programs. We introduce neural-backed operators which can be seamlessly integrated into the program generator. To improve the efficiency of the search, we simply use these operators at non-deterministic decision points, instead of relying on domain-specific heuristics. We implement this technique for the Python pandas library in AutoPandas. AutoPandas supports 119 pandas dataframe transformation functions. We evaluate AutoPandas on 26 real-world benchmarks and find it solves 17 of them.</p>
</td>
	<td>synthesis GNN API </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/habib2019neural/">Neural Bug Finding: A Study of Opportunities and Challenges</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Neural Bug Finding: A Study of Opportunities and Challenges' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Neural Bug Finding: A Study of Opportunities and Challenges' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Neural%20Bug%20Finding:%20A%20Study%20of%20Opportunities%20and%20Challenges' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>A. Habib, M. Pradel</td>
	<td></td>
	<td><p>Static analysis is one of the most widely adopted techniques to find software bugs before code is put in production. Designing and implementing effective and efficient static analyses is difficult and requires high expertise, which results in only a few experts able to write such analyses. This paper explores the opportunities and challenges of an alternative way of creating static bug detectors: neural bug finding. The basic idea is to formulate bug detection as a classification problem, and to address this problem with neural networks trained on examples of buggy and non-buggy code. We systematically study the effectiveness of this approach based on code examples labeled by a state-of-the-art, static bug detector. Our results show that neural bug finding is surprisingly effective for some bug patterns, sometimes reaching a precision and recall of over 80%, but also that it struggles to understand some program properties obvious to a traditional analysis. A qualitative analysis of the results provides insights into why neural bug finders sometimes work and sometimes do not work. We also identify pitfalls in selecting the code examples used to train and validate neural bug finders, and propose an algorithm for selecting effective training data.</p>
</td>
	<td>program analysis </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/gupta2019neural/">Neural Attribution for Semantic Bug-Localization in Student Programs</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Neural Attribution for Semantic Bug-Localization in Student Programs' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Neural Attribution for Semantic Bug-Localization in Student Programs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Neural%20Attribution%20for%20Semantic%20Bug-Localization%20in%20Student%20Programs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>R. Gupta, A. Kanade, S. Shevade</td>
	<td>NeurIPS</td>
	<td><p>Providing feedback is an integral part of teaching. Most open online courses on programming make use of automated grading systems to support programming assignments and give real-time feedback. These systems usually rely on test results to quantify the programs’ functional correctness. They return failing tests to the students as feedback. However, students may find it difficult to debug their programs if they receive no hints about where the bug is and how to fix it. In this work, we present NeuralBugLocator, a deep learning based technique, that can localize the bugs in a faulty program with respect to a failing test, without even running the program. At the heart of our technique is a novel tree convolutional neural network which is trained to predict whether a program passes or fails a given test. To localize the bugs, we analyze the trained network using a state-of-the-art neural prediction attribution technique and see which lines of the programs make it predict the test outcomes. Our experiments show that NeuralBugLocator is generally more accurate than two state-of-the-art program-spectrum based and one syntactic difference based bug-localization baselines.</p>
</td>
	<td>defect representation </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/goens2019case/">A case study on machine learning for synthesizing benchmarks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A case study on machine learning for synthesizing benchmarks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A case study on machine learning for synthesizing benchmarks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=A%20case%20study%20on%20machine%20learning%20for%20synthesizing%20benchmarks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>A. Goens, A. Brauckmann, S. Ertel, C. Cummins, H. Leather, J. Castrillon</td>
	<td>MAPL</td>
	<td><p>Good benchmarks are hard to find because they require a substantial effort to keep them representative for the constantly changing challenges of a particular field. Synthetic benchmarks are a common approach to deal with this, and methods from machine learning are natural candidates for synthetic benchmark generation. In this paper we investigate the usefulness of machine learning in the prominent CLgen benchmark generator. We re-evaluate CLgen by comparing the benchmarks generated by the model with the raw data used to train it. This re-evaluation indicates that, for the use case considered, machine learning did not yield additional benefit over a simpler method using the raw data. We investigate the reasons for this and provide further insights into the challenges the problem could pose for potential future generators.</p>
</td>
	<td>generation </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/gao2019neural/">A Neural Model for Method Name Generation from Functional Description</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Neural Model for Method Name Generation from Functional Description' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Neural Model for Method Name Generation from Functional Description' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=A%20Neural%20Model%20for%20Method%20Name%20Generation%20from%20Functional%20Description' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>S. Gao, C. Chen, Z. Xing, Y. Ma, W. Song, S.W. Lin</td>
	<td>SANER</td>
	<td><p>The names of software artifacts, e.g., method names, are important for software understanding and maintenance, as good names can help developers easily understand others’ code. However, the existing naming guidelines are difficult for developers, especially novices, to come up with meaningful, concise and compact names for the variables, methods, classes and files. With the popularity of open source, an enormous amount of project source code can be accessed, and the exhaustiveness and instability of manually naming methods could now be relieved by automatically learning a naming model from a large code repository. Nevertheless, building a comprehensive naming system is still challenging, due to the gap between natural language functional descriptions and method names. Specifically, there are three challenges: how to model the relationship between the functional descriptions and formal method names, how to handle the explosion of vocabulary when dealing with large repositories, and how to leverage the knowledge learned from large repositories to a specific project. To answer these questions, we propose a neural network to directly generate readable method names from natural language description. The proposed method is built upon the encoder-decoder framework with the attention and copying mechanisms. Our experiments show that our method can generate meaningful and accurate method names and achieve significant improvement over the state-of-the-art baseline models. We also address the cold-start problem using a training trick to utilize big data in GitHub for specific projects.</p>
</td>
	<td>naming summarization </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/fu2019coda/">Coda: An End-to-End Neural Program Decompiler</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Coda: An End-to-End Neural Program Decompiler' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Coda: An End-to-End Neural Program Decompiler' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Coda:%20An%20End-to-End%20Neural%20Program%20Decompiler' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>C. Fu, H. Chen, H. Liu, X. Chen, Y. Tian, F. Koushanfar, J. Zhao</td>
	<td>NeurIPS</td>
	<td><p>Reverse engineering of binary executables is a critical problem in the computer security domain. On the one hand, malicious parties may recover interpretable source codes from the software products to gain commercial advantages. On the other hand, binary decompilation can be leveraged for code vulnerability analysis and malware detection. However, efficient binary decompilation is challenging. Conventional decompilers have the following major limitations: (i) they are only applicable to specific source-target language pair, hence incurs undesired development cost for new language tasks; (ii) their output high-level code cannot effectively preserve the correct functionality of the input binary; (iii) their output program does not capture the semantics of the input and the reversed program is hard to interpret. To address the above problems, we propose Coda1, the first end-to-end neural-based framework for code decompilation. Coda decomposes the decompilation task into of two key phases: First, Coda employs an instruction type-aware encoder and a tree decoder for generating an abstract syntax tree (AST) with attention feeding during the code sketch generation stage. Second, Coda then updates the code sketch using an iterative error correction machine guided by an ensembled neural error predictor. By finding a good approximate candidate and then fixing it towards perfect, Coda achieves superior with performance compared to baseline approaches. We assess Coda’s performance with extensive experiments on various benchmarks. Evaluation results show that Coda achieves an average of 82% program recovery accuracy on unseen binary samples, where the state-of-the-art decompilers yield 0% accuracy. Furthermore, Coda outperforms the sequence-to-sequence model with attention by a margin of 70% program accuracy. Our work reveals the vulnerability of binary executables and imposes a new threat to the protection of Intellectual Property (IP) for software development.</p>
</td>
	<td>decompilation </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/fernandes2019structured/">Structured Neural Summarization</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Structured Neural Summarization' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Structured Neural Summarization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Structured%20Neural%20Summarization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>P. Fernandes, M. Allamanis, M. Brockschmidt</td>
	<td>ICLR</td>
	<td><p>Summarization of long sequences into a concise statement is a core problem in natural language processing, requiring non-trivial understanding of the input. Based on the promising results of graph neural networks on highly structured data, we develop a framework to extend existing sequence encoders with a graph component that can reason about long-distance relationships in weakly structured data such as text. In an extensive evaluation, we show that the resulting hybrid sequence-graph models outperform both pure sequence models as well as pure graph models on a range of summarization tasks.</p>
</td>
	<td>summarization GNN documentation </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/wei2019code/">Code Generation as a Dual Task of Code Summarization</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Code Generation as a Dual Task of Code Summarization' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Code Generation as a Dual Task of Code Summarization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Code%20Generation%20as%20a%20Dual%20Task%20of%20Code%20Summarization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>B. Wei, G. Li, X. Xia, Z. Fu, Z. Jin</td>
	<td>NeurIPS</td>
	<td><p>Code summarization (CS) and code generation (CG) are two crucial tasks in the field of automatic software development. Various neural network-based approaches are proposed to solve these two tasks separately. However, there exists a specific intuitive correlation between CS and CG, which have not been exploited in previous work. In this paper, we apply the relations between two tasks to improve the performance of both tasks. In other words, exploiting the duality between the two tasks, we propose a dual training framework to train the two tasks simultaneously. In this framework, we consider the dualities on probability and attention weights, and design corresponding regularization terms to constrain the duality. We evaluate our approach on two datasets collected from GitHub, and experimental results show that our dual framework can improve the performance of CS and CG tasks over baselines.</p>
</td>
	<td>generation summarization </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/efstathiou2019semantic/">Semantic Source Code Models Using Identifier Embeddings</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Semantic Source Code Models Using Identifier Embeddings' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Semantic Source Code Models Using Identifier Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Semantic%20Source%20Code%20Models%20Using%20Identifier%20Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>V. Efstathiou, D. Spinellis</td>
	<td>MSR</td>
	<td><p>The emergence of online open source repositories in the recent years has led to an explosion in the volume of openly available source code, coupled with metadata that relate to a variety of software development activities. As an effect, in line with recent advances in machine learning research, software maintenance activities are switching from symbolic formal methods to data-driven methods. In this context, the rich semantics hidden in source code identifiers provide opportunities for building semantic representations of code which can assist tasks of code search and reuse. To this end, we deliver in the form of pretrained vector space models, distributed code representations for six popular programming languages, namely, Java, Python, PHP, C, C++, and C#. The models are produced using fastText, a state-of-the-art library for learning word representations. Each model is trained on data from a single programming language; the code mined for producing all models amounts to over 13.000 repositories. We indicate dissimilarities between natural language and source code, as well as variations in coding conventions in between the different programming languages we processed. We describe how these heterogeneities guided the data preprocessing decisions we took and the selection of the training parameters in the released models. Finally, we propose potential applications of the models and discuss limitations of the models.</p>
</td>
	<td>representation </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/ederhardt2019unsupervised/">Unsupervised Learning of API Aliasing Specifications</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Learning of API Aliasing Specifications' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Unsupervised Learning of API Aliasing Specifications' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Unsupervised%20Learning%20of%20API%20Aliasing%20Specifications' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>J. Eberhardt, S. Steffen, V. Raychev, M. Vechev</td>
	<td>PLDI</td>
	<td><p>Real world applications make heavy use of powerful libraries
and frameworks, posing a significant challenge for static analysis
as the library implementation may be very complex or unavailable.
Thus, obtaining specifications that summarize the behaviors of
the library is important as it enables static analyzers to precisely
track the effects of APIs on the client program, without requiring
the actual API implementation.</p>

<p>In this work, we propose a novel method
for discovering aliasing specifications of APIs by learning from a large
dataset of programs. Unlike prior work, our method does not require
manual annotation, access to the library’s source code or ability to
run its APIs. Instead, it learns specifications in a fully unsupervised manner,
by statically observing usages of APIs in the dataset. The core idea is to
learn a probabilistic model of interactions between API methods and aliasing
objects, enabling identification of additional likely aliasing relations,
and to then infer aliasing specifications ofAPIs that explain these relations.
The learned specifications are then used to augment an API-aware points-to analysis.</p>

<p>We implemented our approach in a tool called USpec and used it to automatically
learn aliasing specifications from millions of source code files.
USpec learned over 2000 specifications of various Java and Python APIs, in the process
improving the results of the points-to analysis and its clients.</p>
</td>
	<td>API program analysis </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/edelmann2019neural/">Neural-Network Guided Expression Transformation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Neural-Network Guided Expression Transformation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Neural-Network Guided Expression Transformation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Neural-Network%20Guided%20Expression%20Transformation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>R. Edelmann, V. Kuncak</td>
	<td></td>
	<td><p>Optimizing compilers, as well as other translator systems, often work by rewriting expressions according to equivalence preserving rules. Given an input expression and its optimized form, finding the sequence of rules that were applied is a non-trivial task. Most of the time, the tools provide no proof, of any kind, of the equivalence between the original expression and its optimized form. In this work, we propose to reconstruct proofs of equivalence of simple mathematical expressions, after the fact, by finding paths of equivalence preserving transformations between expressions. We propose to find those sequences of transformations using a search algorithm, guided by a neural network heuristic. Using a Tree-LSTM recursive neural network, we learn a distributed representation of expressions where the Manhattan distance between vectors approximately corresponds to the rewrite distance between expressions. We then show how the neural network can be efficiently used to search for transformation paths, leading to substantial gain in speed compared to an uninformed exhaustive search. In one of our experiments, our neural-network guided search algorithm is able to solve more instances with a 2 seconds timeout per instance than breadth-first search does with a 5 minutes timeout per instance.</p>
</td>
	<td>optimization grammar </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/ding2019asm2vec/">Asm2Vec: Boosting Static Representation Robustness for Binary Clone Search against Code Obfuscation and Compiler Optimization</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Asm2Vec: Boosting Static Representation Robustness for Binary Clone Search against Code Obfuscation and Compiler Optimization' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Asm2Vec: Boosting Static Representation Robustness for Binary Clone Search against Code Obfuscation and Compiler Optimization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Asm2Vec:%20Boosting%20Static%20Representation%20Robustness%20for%20Binary%20Clone%20Search%20against%20Code%20Obfuscation%20and%20Compiler%20Optimization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>S. H. H. Ding, B. C. M. Fung, P. Charland</td>
	<td>IEEE Symposium on Security and Privacy</td>
	<td><p>Reverse engineering is a manually intensive but necessary technique for understanding the inner workings of new malware, finding vulnerabilities in existing systems, and detecting patent infringements in released software. An assembly clone search engine facilitates the work of reverse engineers by identifying those duplicated or known parts. However, it is challenging to design a robust clone search engine, since there exist various compiler optimization options and code obfuscation techniques that make logically similar assembly functions appear to be very different. A practical clone search engine relies on a robust vector representation of assembly code. However, the existing clone search approaches, which rely on a manual feature engineering process to form a feature vector for an assembly function, fail to consider the relationships between features and identify those unique patterns that can statistically distinguish assembly functions. To address this problem, we propose to jointly learn the lexical semantic relationships and the vector representation of assembly functions based on assembly code. We have developed an assembly code representation learning model \emph{Asm2Vec}. It only needs assembly code as input and does not require any prior knowledge such as the correct mapping between assembly functions. It can find and incorporate rich semantic relationships among tokens appearing in assembly code. We conduct extensive experiments and benchmark the learning model with state-of-the-art static and dynamic clone search approaches. We show that the learned representation is more robust and significantly outperforms existing methods against changes introduced by obfuscation and optimizations.</p>
</td>
	<td>representation clone </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/waunakh2019evaluating/">Evaluating Semantic Representations of Source Code</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Evaluating Semantic Representations of Source Code' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Evaluating Semantic Representations of Source Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Evaluating%20Semantic%20Representations%20of%20Source%20Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Y. Wainakh, M. Rauf, M. Pradel</td>
	<td></td>
	<td><p>Learned representations of source code enable various software developer tools, e.g., to detect bugs or to predict program properties. At the core of code representations often are word embeddings of identifier names in source code, because identifiers account for the majority of source code vocabulary and convey important semantic information. Unfortunately, there currently is no generally accepted way of evaluating the quality of word embeddings of identifiers, and current evaluations are biased toward specific downstream tasks. This paper presents IdBench, the first benchmark for evaluating to what extent word embeddings of identifiers represent semantic relatedness and similarity. The benchmark is based on thousands of ratings gathered by surveying 500 software developers. We use IdBench to evaluate state-of-the-art embedding techniques proposed for natural language, an embedding technique specifically designed for source code, and lexical string distance functions, as these are often used in current developer tools. Our results show that the effectiveness of embeddings varies significantly across different embedding techniques and that the best available embeddings successfully represent semantic relatedness. On the downside, no existing embedding provides a satisfactory representation of semantic similarities, e.g., because embeddings consider identifiers with opposing meanings as similar, which may lead to fatal mistakes in downstream developer tools. IdBench provides a gold standard to guide the development of novel embeddings that address the current limitations.</p>
</td>
	<td>representation </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/schrouff2019inferring/">Inferring Javascript types using Graph Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Inferring Javascript types using Graph Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Inferring Javascript types using Graph Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Inferring%20Javascript%20types%20using%20Graph%20Neural%20Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>J. Schrouff, K. Wohlfahrt, B. Marnette, L. Atkinson</td>
	<td>Representation Learning on Graphs and Manifolds ICLR 2019 workshop</td>
	<td><p>The recent use of `Big Code’ with state-of-the-art deep learning methods offers promising avenues to ease program source code writing and correction. As a first step towards automatic code repair, we implemented a graph neural network model that predicts token types for Javascript programs. The predictions achieve an accuracy above 90%, which improves on previous similar work.</p>
</td>
	<td>GNN types program analysis </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/david2019neural/">Neural Reverse Engineering of Stripped Binaries</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Neural Reverse Engineering of Stripped Binaries' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Neural Reverse Engineering of Stripped Binaries' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Neural%20Reverse%20Engineering%20of%20Stripped%20Binaries' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Y. David, U. Alon, E. Yahav</td>
	<td>ICLR</td>
	<td><p>We address the problem of predicting procedure names in stripped executables which contain no debug information.
Predicting procedure names can dramatically ease the task of reverse engineering, saving precious time and human effort. 
We present a novel approach that leverages static analysis of binaries with encoder-decoder-based neural networks.
The main idea is to use static analysis to obtain enriched representations of API call sites; encode a set of sequences
of these call sites; and finally, attend to the encoded sequences while decoding the target name token-by-token. 
We evaluate our model by predicting procedure names over 60,000 procedures in 10,000 stripped executables.
Our model achieves 81.70 precision and 80.12 recall in predicting procedure names within GNU packages, and 55.48
precision and 51.31 recall in a diverse, cross-package, dataset. Comparing to previous approaches,
the predictions made by our model are much more accurate and informative.</p>
</td>
	<td>naming deobfuscation </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/sharma2019feasibility/">On the Feasibility of Transfer-learning Code Smells using Deep Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=On the Feasibility of Transfer-learning Code Smells using Deep Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=On the Feasibility of Transfer-learning Code Smells using Deep Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=On%20the%20Feasibility%20of%20Transfer-learning%20Code%20Smells%20using%20Deep%20Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>T. Sharma, V. Eftathiou, P. Louridas, D. Spinellis</td>
	<td></td>
	<td><p><strong>Context</strong>: A substantial amount of work has been done to detect smells in source code using metrics-based and heuristics-based methods. Machine learning methods have been recently applied to detect source code smells; however, the current practices are considered far from mature.</p>

<p><strong>Objective</strong>: First, explore the feasibility of applying deep learning models to detect smells without extensive feature engineering, just by feeding the source code in tokenized form. Second, investigate the possibility of applying transfer-learning in the context of deep learning models for smell detection.</p>

<p><strong>Method</strong>: We use existing metric-based state-of-the-art methods for detecting three implementation smells and one design smell in C# code. Using these results as the annotated gold standard, we train smell detection models on three different deep learning architectures. These architectures use Convolution Neural Networks (CNNs) of one or two dimensions, or Recurrent Neural Networks (RNNs) as their principal hidden layers. For the first objective of our study, we perform training and evaluation on C# samples, whereas for the second objective, we train the models from C# code and evaluate the models over Java code samples. We perform the experiments with various combinations of hyper-parameters for each model.</p>

<p><strong>Results</strong>: We find it feasible to detect smells using deep learning methods. Our comparative experiments find that there is no clearly superior method between CNN-1D and CNN-2D. We also observe that performance of the deep learning models is smell-specific. Our transfer-learning experiments show that transfer-learning is definitely feasible for implementation smells with performance comparable to that of direct-learning. This work opens up a new paradigm to detect code smells by transfer-learning especially for the programming languages where the comprehensive code smell detection tools are not available.</p>
</td>
	<td>representation program analysis </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/she2019neuzz/">NEUZZ: Efficient Fuzzing with Neural Program Smoothing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=NEUZZ: Efficient Fuzzing with Neural Program Smoothing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=NEUZZ: Efficient Fuzzing with Neural Program Smoothing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=NEUZZ:%20Efficient%20Fuzzing%20with%20Neural%20Program%20Smoothing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>D. She, K. Pei, D. Epstein, J. Yang, B. Ray, S. Jana</td>
	<td>IEEE S&P</td>
	<td><p>Fuzzing has become the de facto standard technique for finding software vulnerabilities. However, even state-of-the-art fuzzers are not very efficient at finding hard-to-trigger software bugs. Most popular fuzzers use evolutionary guidance to generate inputs that can trigger different bugs. Such evolutionary algorithms, while fast and simple to implement, often get stuck in fruitless sequences of random mutations. Gradient-guided optimization presents a promising alternative to evolutionary guidance. Gradient-guided techniques have been shown to significantly outperform evolutionary algorithms at solving high-dimensional structured optimization problems in domains like machine learning by efficiently utilizing gradients or higher-order derivatives of the underlying function. However, gradient-guided approaches are not directly applicable to fuzzing as real-world program behaviors contain many discontinuities, plateaus, and ridges where the gradient-based methods often get stuck. We observe that this problem can be addressed by creating a smooth surrogate function approximating the discrete branching behavior of target program. In this paper, we propose a novel program smoothing technique using surrogate neural network models that can incrementally learn smooth approximations of a complex, real-world program’s branching behaviors. We further demonstrate that such neural network models can be used together with gradient-guided input generation schemes to significantly improve the fuzzing efficiency. Our extensive evaluations demonstrate that NEUZZ significantly outperforms 10 state-of-the-art graybox fuzzers on 10 real-world programs both at finding new bugs and achieving higher edge coverage. NEUZZ found 31 unknown bugs that other fuzzers failed to find in 10 real world programs and achieved 3X more edge coverage than all of the tested graybox fuzzers for 24 hours running.</p>
</td>
	<td>fuzzing </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/shi2019learning/">Learning Execution through Neural Code Fusion</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning Execution through Neural Code Fusion' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning Execution through Neural Code Fusion' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20Execution%20through%20Neural%20Code%20Fusion' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Z. Shi, K. Swersky, D. Tarlow, P. Ranganathan, M. Hashemi</td>
	<td></td>
	<td><p>As the performance of computer systems stagnates due to the end of Moore’s Law, there is a need for new models that can understand and optimize the execution of general purpose code. While there is a growing body of work on using Graph Neural Networks (GNNs) to learn representations of source code, these representations do not understand how code dynamically executes. In this work, we propose a new approach to use GNNs to learn fused representations of general source code and its execution. Our approach defines a multi-task GNN over low-level representations of source code and program state (i.e., assembly code and dynamic memory states), converting complex source code constructs and complex data structures into a simpler, more uniform format. We show that this leads to improved performance over similar methods that do not use execution and it opens the door to applying GNN models to new tasks that would not be feasible from static code alone. As an illustration of this, we apply the new model to challenging dynamic tasks (branch prediction and prefetching) from the SPEC CPU benchmark suite, outperforming the state-of-the-art by 26% and 45% respectively. Moreover, we use the learned fused graph embeddings to demonstrate transfer learning with high performance on an indirectly related task (algorithm classification).</p>
</td>
	<td>representation </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/vasic2019neural/">Neural Program Repair by Jointly Learning to Localize and Repair</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Neural Program Repair by Jointly Learning to Localize and Repair' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Neural Program Repair by Jointly Learning to Localize and Repair' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Neural%20Program%20Repair%20by%20Jointly%20Learning%20to%20Localize%20and%20Repair' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>M. Vasic, A. Kanade, P. Maniatis, D. Bieber, R. Singh</td>
	<td>ICLR</td>
	<td><p>Due to its potential to improve programmer productivity and software quality, automated program repair has been an active topic of research. Newer techniques harness neural networks to learn directly from examples of buggy programs and their fixes. In this work, we consider a recently identified class of bugs called variable-misuse bugs. The state-of-the-art solution for variable misuse enumerates potential fixes for all possible bug locations in a program, before selecting the best prediction. We show that it is beneficial to train a model that jointly and directly localizes and repairs variable-misuse bugs. We present multi-headed pointer networks for this purpose, with one head each for localization and repair. The experimental results show that the joint model significantly outperforms an enumerative solution that uses a pointer based model for repair alone.</p>
</td>
	<td>repair program analysis variable misuse </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/brockschmidt2019generative/">Generative Code Modeling with Graphs</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Generative Code Modeling with Graphs' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Generative Code Modeling with Graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Generative%20Code%20Modeling%20with%20Graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>M. Brockscmidt, M. Allamanis A. L. Gaunt, O. Polozov</td>
	<td>ICLR</td>
	<td><p>Generative models forsource code are an interesting structured prediction problem, requiring to reason about both hard syntactic and semantic constraints as well as about natural, likely programs. We present a novel model for this problem that uses a graph to represent the intermediate state of the generated output. Our model generates code by interleaving grammar-driven expansion steps with graph augmentation and neural message passing steps. An experimental evaluation shows that our new model can generate semantically meaningful expressions, outperforming a range of strong baselines.</p>
</td>
	<td>grammar generation GNN </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/commit2vec2019lozoya/">Commit2Vec: Learning Distributed Representations of Code Changes</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Commit2Vec: Learning Distributed Representations of Code Changes' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Commit2Vec: Learning Distributed Representations of Code Changes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Commit2Vec:%20Learning%20Distributed%20Representations%20of%20Code%20Changes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>R. C. Lozoya, A. Baumann, A. Sabetta, M. Bezzi</td>
	<td></td>
	<td><p>Deep learning methods, which have found successful applications in fields like image classification and natural language processing, have recently been applied to source code analysis too, due to the enormous amount of freely available source code (e.g., from open-source software repositories).</p>

<p>In this work, we elaborate upon a state-of-the-art approach to the representation of source code that uses information about its syntactic structure, and we adapt it to represent source changes (i.e., commits). We use this representation to classify security-relevant commits.</p>

<p>Because our method uses transfer learning (that is, we train a network on a “pretext task” for which abundant labeled data is available, and then we use such network for the target task of commit classification, for which fewer labeled instances are available), we studied the impact of pre-training the network using two different pretext tasks versus a randomly initialized model.</p>

<p>Our results indicate that representations that leverage the structural information obtained through code syntax outperform token-based representations. Furthermore, the performance metrics obtained when pre-training on a loosely related pretext task with a very large dataset (&gt;10e6 samples) were surpassed when pretraining on a smaller dataset (&gt;10e4 samples) but for a pretext task that is more closely related to the target task.</p>
</td>
	<td>edit </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/buech2019learning/">Learning-based Recursive Aggregation of Abstract Syntax Trees for Code Clone Detection</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning-based Recursive Aggregation of Abstract Syntax Trees for Code Clone Detection' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning-based Recursive Aggregation of Abstract Syntax Trees for Code Clone Detection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning-based%20Recursive%20Aggregation%20of%20Abstract%20Syntax%20Trees%20for%20Code%20Clone%20Detection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>L. Büch, A. Andrzejak</td>
	<td>SANER</td>
	<td><p>Code clone detection remains a crucial challenge in maintaining software projects. Many classic approaches rely on handcrafted aggregation schemes, while recent work uses supervised or unsupervised learning. In this work, we study several aspects of aggregation schemes for code clone detection based on supervised learning. To this aim, we implement an AST-based Recursive Neural Network. Firstly, our ablation study shows the influence of model choices and hyperparameters. We introduce error scaling as a way to effectively and efficiently address the class imbalance problem arising in code clone detection. Secondly, we study the influence of pretrained embeddings representing nodes in ASTs. We show that simply averaging all node vectors of a given AST yields strong baseline aggregation scheme. Further, learned AST aggregation schemes greatly benefit from pretrained node embeddings. Finally, we show the importance of carefully separating training and test data by clone clusters, to reliably measure generalization of models learned with supervision.</p>
</td>
	<td>AST grammar clone </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/siow2019core/">CORE: Automating Review Recommendation for Code Changes</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=CORE: Automating Review Recommendation for Code Changes' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=CORE: Automating Review Recommendation for Code Changes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=CORE:%20Automating%20Review%20Recommendation%20for%20Code%20Changes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>J. Siow, C. Gao, L. Fan, S. Chen, Y. Liu</td>
	<td>SANER</td>
	<td><p>Code review is a common process that is used by developers, in which a reviewer provides useful comments or points out defects in the submitted source code changes via pull request. Code review has been widely used for both industry and open-source projects due to its capacity in early defect identification, project maintenance, and code improvement. With rapid updates on project developments, code review becomes a non-trivial and labor-intensive task for reviewers. Thus, an automated code review engine can be beneficial and useful for project development in practice. Although there exist prior studies on automating the code review process by adopting static analysis tools or deep learning techniques, they often require external sources such as partial or full source code for accurate review suggestion. In this paper, we aim at automating the code review process only based on code changes and the corresponding reviews but with better performance. The hinge of accurate code review suggestion is to learn good representations for both code changes and reviews. To achieve this with limited source, we design a multi-level embedding (i.e., word embedding and character embedding) approach to represent the semantics provided by code changes and reviews. The embeddings are then well trained through a proposed attentional deep learning model, as a whole named CORE. We evaluate the effectiveness of CORE on code changes and reviews collected from 19 popular Java projects hosted on Github. Experimental results show that our model CORE can achieve significantly better performance than the state-of-the-art model (DeepMem), with an increase of 131.03% in terms of Recall@10 and 150.69% in terms of Mean Reciprocal Rank. Qualitative general word analysis among project developers also demonstrates the performance of CORE in automating code review.</p>
</td>
	<td>review </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/chibotaru2019scalable/">Scalable Taint Specification Inference with Big Code</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Scalable Taint Specification Inference with Big Code' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Scalable Taint Specification Inference with Big Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Scalable%20Taint%20Specification%20Inference%20with%20Big%20Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>V. Chibotaru, B. Bichsel, V. Raychev, M. Vechev</td>
	<td>PLDI</td>
	<td><p>We present a new scalable, semi-supervised method for inferring
taint analysis specifications by learning from a large dataset of programs.
Taint specifications capture the role of library APIs (source, sink, sanitizer)
and are a critical ingredient of any taint analyzer that aims to detect
security violations based on information flow.</p>

<p>The core idea of our method
is to formulate the taint specification learning problem as a linear
optimization task over a large set of information flow constraints.
The resulting constraint system can then be efficiently solved with
state-of-the-art solvers. Thanks to its scalability, our method can infer
many new and interesting taint specifications by simultaneously learning from
a large dataset of programs (e.g., as found on GitHub), while requiring 
few manual annotations.</p>

<p>We implemented our method in an end-to-end system,
called Seldon, targeting Python, a language where static specification
inference is particularly hard due to lack of typing information.
We show that Seldon is practically effective: it learned almost 7,000 API
roles from over 210,000 candidate APIs with very little supervision
(less than 300 annotations) and with high estimated precision (67%).
Further,using the learned specifications, our taint analyzer flagged more than
20,000 violations in open source projects, 97% of which were
undetectable without the inferred specifications.</p>
</td>
	<td>defect program analysis </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/sun2019grammar/">A Grammar-Based Structural CNN Decoder for Code Generation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Grammar-Based Structural CNN Decoder for Code Generation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Grammar-Based Structural CNN Decoder for Code Generation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=A%20Grammar-Based%20Structural%20CNN%20Decoder%20for%20Code%20Generation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Z. Sun, Q. Zhu, L. Mou, Y. Xiong, G. Li, L. Zhang</td>
	<td>AAAI</td>
	<td><p>Code  generation  maps  a  program  description  to  executable
source code in a programming language. Existing approaches
mainly rely on a recurrent neural network (RNN) as the decoder. However, we find that a program contains significantly
more tokens than a natural language sentence, and thus it may
be inappropriate for RNN to capture such a long sequence. In
this paper, we propose a grammar-based structural convolutional neural network (CNN) for code generation. Our model
generates a program by predicting the grammar rules of the
programming language; we design several CNN modules, including the tree-based convolution and pre-order convolution,
whose information is further aggregated by dedicated attentive pooling layers. Experimental results on the HearthStone
benchmark dataset show that our CNN code generator significantly outperforms the previous state-of-the-art method by 5
percentage points; additional experiments on several semantic parsing tasks demonstrate the robustness of our model. We
also conduct in-depth ablation test to better understand each
component of our model.</p>
</td>
	<td>generation grammar </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/svyatkovskiy2019pythia/">Pythia: AI-assisted Code Completion System</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Pythia: AI-assisted Code Completion System' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Pythia: AI-assisted Code Completion System' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Pythia:%20AI-assisted%20Code%20Completion%20System' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>A. Svyatkovskiy, Y. Zhao, S. Fu, Neel Sundaresan</td>
	<td>KDD</td>
	<td><p>In this paper, we propose a novel end-to-end approach for AI-assisted code completion called Pythia. It generates ranked lists of method and API recommendations which can be used by software developers at edit time. The system is currently deployed as part of Intellicode extension in Visual Studio Code IDE. Pythia exploits state-of-the-art large-scale deep learning models trained on code contexts extracted from abstract syntax trees. It is designed to work at a high throughput predicting the best matching code completions on the order of 100 ms.</p>

<p>We describe the architecture of the system, perform comparisons to frequency-based approach and invocation-based Markov Chain language model, and discuss challenges serving Pythia models on lightweight client devices.</p>

<p>The offline evaluation results obtained on 2700 Python open source software GitHub repositories show a top-5 accuracy of 92%, surpassing the baseline models by 20% averaged over classes, for both intra and cross-project settings.</p>

</td>
	<td>autocomplete language model </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/wan2019multimodal/">Multi-Modal Attention Network Learning for Semantic Source Code Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Multi-Modal Attention Network Learning for Semantic Source Code Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Multi-Modal Attention Network Learning for Semantic Source Code Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Multi-Modal%20Attention%20Network%20Learning%20for%20Semantic%20Source%20Code%20Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Y. Wan, J. Shu, Y. Sui, G. Xu, Z. Zhao, J. Wu, P. S. Yu</td>
	<td></td>
	<td><p>Code retrieval techniques and tools have been playing a key role in facilitating software developers to retrieve existing code fragments from available open-source repositories given a user query. Despite the existing efforts in improving the effectiveness of code retrieval, there are still two main issues hindering them from being used to accurately retrieve satisfiable code fragments from large-scale repositories when answering complicated queries. First, the existing approaches only consider shallow features of source code such as method names and code tokens, but ignoring structured features such as abstract syntax trees (ASTs) and control-flow graphs (CFGs) of source code, which contains rich and well-defined semantics of source code. Second, although the deep learning-based approach performs well on the representation of source code, it lacks the explainability, making it hard to interpret the retrieval results and almost impossible to understand which features of source code contribute more to the final results.</p>

<p>To tackle the two aforementioned issues, this paper proposes MMAN, a novel Multi-Modal Attention Network for semantic source code retrieval. A comprehensive multi-modal representation is developed for representing unstructured and structured features of source code, with one LSTM for the sequential tokens of code, a Tree-LSTM for the AST of code and a GGNN (Gated Graph Neural Network) for the CFG of code. Furthermore, a multi-modal attention fusion layer is applied to assign weights to different parts of each modality of source code and then integrate them into a single hybrid representation. Comprehensive experiments and analysis on a large-scale real-world dataset show that our proposed model can accurately retrieve code snippets and outperforms the state-of-the-art methods.</p>
</td>
	<td>search </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/bui2019learning/">SAR: Learning Cross-Language API Mappings with Little Knowledge</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=SAR: Learning Cross-Language API Mappings with Little Knowledge' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=SAR: Learning Cross-Language API Mappings with Little Knowledge' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=SAR:%20Learning%20Cross-Language%20API%20Mappings%20with%20Little%20Knowledge' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>N. D. Q. Bui, Y. Yu, L. Jiang</td>
	<td>FSE</td>
	<td><p>To save manual effort, developers often translate programs from one programming language to another, instead of implementing it from scratch. Translating application program interfaces (APIs) used in one language to functionally equivalent ones available in another language is an important aspect of program translation. Existing approaches facilitate the translation by automatically identifying the API mappings across programming languages. However, all these approaches still require large amount of manual effort in preparing parallel program corpora, ranging from pairs of APIs, to manually identified code in different languages that are considered as functionally equivalent. To minimize the manual effort in identifying parallel program corpora and API mappings, this paper aims at an automated approach to map APIs across languages with much less knowledge a priori needed than other existing approaches. The approach is based on an realization of the notion of domain adaption combined with code embedding, which can better align two vector spaces: taking as input large sets of programs, our approach first generates numeric vector representations of the programs, especially the APIs used in each language, and it adapts generative adversarial networks (GAN) to align the vectors from the spaces of two languages. For a better alignment, we initialize the GAN with parameters derived from optional API mapping seeds that can be identified accurately with a simple automatic signature-based matching heuristic. Then the cross-language API mappings can be identified via nearest-neighbors queries in the aligned vector spaces.</p>
</td>
	<td>representation API </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/cambronero2019deep/">When Deep Learning Met Code Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=When Deep Learning Met Code Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=When Deep Learning Met Code Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=When%20Deep%20Learning%20Met%20Code%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>J. Cambronero, H. Li, S. Kim, K. Sen, S. Chandra</td>
	<td></td>
	<td><p>There have been multiple recent proposals on using deep neural networks for code search using natural language. Common across these proposals is the idea of embedding code and natural language queries, into real vectors and then using vector distance to approximate semantic correlation between code and the query. Multiple approaches exist for learning these embeddings, including unsupervised techniques, which rely only on a corpus of code examples, and supervised techniques, which use an aligned corpus of paired code and natural language descriptions. The goal of this supervision is to produce embeddings that are more similar for a query and the corresponding desired code snippet.</p>

<p>Clearly, there are choices in whether to use supervised techniques at all, and if one does, what sort of network and training to use for supervision. This paper is the first to evaluate these choices systematically. To this end, we assembled implementations of state-of-the-art techniques to run on a common platform, training and evaluation corpora. To explore the design space in network complexity, we also introduced a new design point that is a minimal supervision extension to an existing unsupervised technique.</p>

<p>Our evaluation shows that: 1. adding supervision to an existing unsupervised technique can improve performance, though not necessarily by much; 2. simple networks for supervision can be more effective that more sophisticated sequence-based networks for code search; 3. while it is common to use docstrings to carry out supervision, there is a sizeable gap between the effectiveness of docstrings and a more query-appropriate supervision corpus.</p>
</td>
	<td>search </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/tarlow2019learning/">Learning to Fix Build Errors with Graph2Diff Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning to Fix Build Errors with Graph2Diff Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning to Fix Build Errors with Graph2Diff Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20to%20Fix%20Build%20Errors%20with%20Graph2Diff%20Neural%20Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>D. Tarlow, S. Moitra, A. Rcie, Z. Chen, P.A. Manzagol, C. Sutton, E. Aftandilian</td>
	<td></td>
	<td><p>Professional software developers spend a significant amount oftime fixing builds, but this has received little attention as a prob-lem in automatic program repair. We present a new deep learningarchitecture, called Graph2Diff, for automatically localizing andfixing build errors. We represent source code, build configurationfiles, and compiler diagnostic messages as a graph, and then use aGraph Neural Network model to predict a diff. A diff specifies howto modify the code’s abstract syntax tree, represented in the neuralnetwork as a sequence of tokens and of pointers to code locations.Our network is an instance of a more general abstraction which wecall Graph2Tocopo, which is potentially useful in any developmenttool for predicting source code changes. We evaluate the model ona dataset of over 500k real build errors and their resolutions fromprofessional developers. Compared to the approach of DeepDelta, our approach tackles the harder task of predicting a moreprecise diff but still achieves over double the accuracy.</p>
</td>
	<td>edit repair </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/theeten2019import2vec/">Import2vec - Learning Embeddings for Software Libraries</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Import2vec - Learning Embeddings for Software Libraries' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Import2vec - Learning Embeddings for Software Libraries' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Import2vec%20-%20Learning%20Embeddings%20for%20Software%20Libraries' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>B. Theeten, F. Vandeputte, T.Van Cutsem</td>
	<td>MSR</td>
	<td><p>We consider the problem of developing suitable learning representations (embeddings) for library packages that capture semantic similarity among libraries. Such representations are known to improve the performance of downstream learning tasks (e.g. classification) or applications such as contextual search and analogical reasoning.</p>

<p>We apply word embedding techniques from natural language processing (NLP) to train embeddings for library packages (“library vectors”). Library vectors represent libraries by similar context of use as determined by import statements present in source code. Experimental results obtained from training such embeddings on three large open source software corpora reveals that library vectors capture semantically meaningful relationships among software libraries, such as the relationship between frameworks and their plug-ins and libraries commonly used together within ecosystems such as big data infrastructure projects (in Java), front-end and back-end web development frameworks (in JavaScript) and data science toolkits (in Python).</p>
</td>
	<td>representation </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/tomczak2019simulating/">Simulating Execution Time of Tensor Programs using Graph Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Simulating Execution Time of Tensor Programs using Graph Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Simulating Execution Time of Tensor Programs using Graph Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Simulating%20Execution%20Time%20of%20Tensor%20Programs%20using%20Graph%20Neural%20Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>J. M. Tomczak, R. Lepert, A. Wiggers</td>
	<td>Representation Learning on Graphs and Manifolds at ICLR</td>
	<td><p>Optimizing the execution time of tensor program, e.g., a convolution, involves finding its optimal configuration. Searching the configuration space exhaustively is typically infeasible in practice. In line with recent research using TVM, we propose to learn a surrogate model to overcome this issue. The model is trained on an acyclic graph called an abstract syntax tree, and utilizes a graph convolutional network to exploit structure in the graph. We claim that a learnable graph-based data processing is a strong competitor to heuristic-based feature extraction. We present a new dataset of graphs corresponding to configurations and their execution time for various tensor programs. We provide baselines for a runtime prediction task.</p>
</td>
	<td>GNN </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/tran2019recovering/">Recovering Variable Names for Minified Code with Usage Contexts</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Recovering Variable Names for Minified Code with Usage Contexts' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Recovering Variable Names for Minified Code with Usage Contexts' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Recovering%20Variable%20Names%20for%20Minified%20Code%20with%20Usage%20Contexts' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>H. Tran, N. Tran, S. Nguyen, H. Nguyen, T. Nguyen</td>
	<td>ICSE</td>
	<td><p>In modern Web technology, JavaScript (JS) code plays an important role. To avoid the exposure of original source code, the variable names in JS code deployed in the wild are often replaced by short, meaningless names, thus making the code extremely difficult to manually understand and analysis. This paper presents JSNeat, an information retrieval (IR)-based approach to recover the variable names in minified JS code. JSNeat follows a data-driven approach to recover names by searching for them in a large corpus of open-source JS code. We use three types of contexts to match a variable in given minified code against the corpus including the context of properties and roles of the variable, the context of that variable and relations with other variables under recovery, and the context of the task of the function to which the variable contributes. We performed several empirical experiments to evaluate JSNeat on the dataset of more than 322K JS files with 1M functions, and 3.5M variables with 176K unique variable names. We found that JSNeat achieves a high accuracy of 69.1%, which is the relative improvements of 66.1% and 43% over two state-of-the-art approaches JSNice and JSNaughty, respectively. The time to recover for a file or for a variable with JSNeat is twice as fast as with JSNice and 4x as fast as with JNaughty, respectively.</p>
</td>
	<td>naming deobfuscation </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/chen2019sequencer/">SequenceR: Sequence-to-Sequence Learning for End-to-End Program Repair</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=SequenceR: Sequence-to-Sequence Learning for End-to-End Program Repair' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=SequenceR: Sequence-to-Sequence Learning for End-to-End Program Repair' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=SequenceR:%20Sequence-to-Sequence%20Learning%20for%20End-to-End%20Program%20Repair' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Z. Chen, S. Kommrusch, M. Tufano, L. Pouchet, D. Poshyvanyk, M. Monperrus</td>
	<td></td>
	<td><p>This paper presents a novel end-to-end approach to program repair based on sequence-to-sequence learning. We devise, implement, and evaluate a system, called SequenceR, for fixing bugs based on sequence-to-sequence learning on source code. This approach uses the copy mechanism to overcome the unlimited vocabulary problem that occurs with big code. Our system is data-driven; we train it on 35,578 commits, carefully curated from open-source repositories. We evaluate it on 4,711 independent real bug fixes, as well on the Defects4J benchmark used in program repair research. SequenceR is able to perfectly predict the fixed line for 950/4711 testing samples. It captures a wide range of repair operators without any domain-specific top-down design.</p>
</td>
	<td>repair generation </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/chen2019mining/">Mining Likely Analogical APIs across Third-Party Libraries via Large-Scale Unsupervised API Semantics Embedding</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Mining Likely Analogical APIs across Third-Party Libraries via Large-Scale Unsupervised API Semantics Embedding' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Mining Likely Analogical APIs across Third-Party Libraries via Large-Scale Unsupervised API Semantics Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Mining%20Likely%20Analogical%20APIs%20across%20Third-Party%20Libraries%20via%20Large-Scale%20Unsupervised%20API%20Semantics%20Embedding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>C. Chen ; Z. Xing ; Y. Liu, K. L. Xiong Ong</td>
	<td>TSE</td>
	<td><p>Establishing API mappings between third-party libraries is a prerequisite step for library migration tasks. Manually establishing API mappings is tedious due to the large number of APIs to be examined. Having an automatic technique to create a database of likely API mappings can significantly ease the task. Unfortunately, existing techniques either adopt supervised learning mechanism that requires already-ported or functionality similar applications across major programming languages or platforms, which are difficult to come by for an arbitrary pair of third-party libraries, or cannot deal with lexical gap in the API descriptions of different libraries. To overcome these limitations, we present an unsupervised deep learning based approach to embed both API usage semantics and API description (name and document) semantics into vector space for inferring likely analogical API mappings between libraries. Based on deep learning models trained using tens of millions of API call sequences, method names and comments of 2.8 millions of methods from 135,127 GitHub projects, our approach significantly outperforms other deep learning or traditional information retrieval (IR) methods for inferring likely analogical APIs. We implement a proof-of-concept website which can recommend analogical APIs for 583,501 APIs of 111 pairs of analogical Java libraries with diverse functionalities. This scale of third-party analogical-API database has never been achieved before.</p>
</td>
	<td>API representation </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/chen2019literature/">A Literature Study of Embeddings on Source Code</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Literature Study of Embeddings on Source Code' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Literature Study of Embeddings on Source Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=A%20Literature%20Study%20of%20Embeddings%20on%20Source%20Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Z. Chen, M. Monperrus</td>
	<td></td>
	<td><p>Natural language processing has improved tremendously after the success of word embedding techniques such as word2vec. Recently, the same idea has been applied on source code with encouraging results. In this survey, we aim to collect and discuss the usage of word embedding techniques on programs and source code. The articles in this survey have been collected by asking authors of related work and with an extensive search on Google Scholar. Each article is categorized into five categories: 1. embedding of tokens 2. embedding of functions or methods 3. embedding of sequences or sets of method calls 4. embedding of binary code 5. other embeddings. We also provide links to experimental data and show some remarkable visualization of code embeddings. In summary, word embedding has been successfully applied on different granularities of source code. With access to countless open-source repositories, we see a great potential of applying other data-driven natural language processing techniques on source code in the future.</p>
</td>
	<td>representation </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/chen2019capturing/">Capturing source code semantics via tree-based convolution over API-enhanced AST</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Capturing source code semantics via tree-based convolution over API-enhanced AST' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Capturing source code semantics via tree-based convolution over API-enhanced AST' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Capturing%20source%20code%20semantics%20via%20tree-based%20convolution%20over%20API-enhanced%20AST' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>L. Chen, W. Ye, S. Zheng</td>
	<td>Computing Frontiers</td>
	<td><p>When deep learning meets big code, a key question is how to efficiently learn a distributed representation for source code that can capture its semantics effectively. We propose to use tree-based convolution over API-enhanced AST. To demonstrate the effectiveness of our approach, we apply it to detect semantic clones—code fragments with similar semantics but dissimilar syntax. Experiment results show that our approach outperforms an existing state-of-the-art approach that uses tree-based LSTM, with an increase of 0.39 and 0.12 in F1-score on OJClone and BigCloneBench respectively. We further propose architectures that incorporate our approach for code search and code summarization.</p>
</td>
	<td>AST representation </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/tufano2019learning/">On Learning Meaningful Code Changes via Neural Machine Translation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=On Learning Meaningful Code Changes via Neural Machine Translation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=On Learning Meaningful Code Changes via Neural Machine Translation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=On%20Learning%20Meaningful%20Code%20Changes%20via%20Neural%20Machine%20Translation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>M. Tufano, C. Watson, G. Bavota, M. Di Penta, M. White, D. Poshyvanyk</td>
	<td>ICSE</td>
	<td><p>Recent years have seen the rise of Deep Learning (DL) techniques applied to source code. Researchers have exploited DL to automate several development and maintenance tasks, such as writing commit messages, generating comments and detecting vulnerabilities among others. One of the long lasting dreams of applying DL to code is the possibility to automate non-trivial coding activities. While some steps in this direction have been taken (e.g., learning how to fix bugs), there is still a lack of empirical evidence on the types of code changes that can be learned and automatically applied by DL. Our goal is to make this first step by quantitatively and qualitatively investigating the ability of a Neural Machine Translation (NMT) model to learn how to automatically apply code changes implemented by developers during pull requests. We train and experiment with the NMT model on a set of 236k pairs of code components before and after the implementation of the changes provided in the pull requests. We show that, when applied in a narrow enough context (i.e., small/medium-sized pairs of methods before/after the pull request changes), NMT can automatically replicate the changes implemented by developers during pull requests in up to 36% of the cases. Moreover, our qualitative analysis shows that the model is capable of learning and replicating a wide variety of meaningful code changes, especially refactorings and bug-fixing activities. Our results pave the way to novel research in the area of DL on code, such as the automatic learning and applications of refactoring.</p>
</td>
	<td>repair edit </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/wang2019learning/">Learning Scalable and Precise Representation of Program Semantics</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning Scalable and Precise Representation of Program Semantics' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning Scalable and Precise Representation of Program Semantics' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20Scalable%20and%20Precise%20Representation%20of%20Program%20Semantics' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>K. Wang</td>
	<td></td>
	<td><p>Neural program embedding has shown potential in aiding the analysis of large-scale, complicated software. Newly proposed deep neural architectures pride themselves on learning program semantics rather than superficial syntactic features. However, by considering the source code only, the vast majority of neural networks do not capture a deep, precise representation of program semantics. In this paper, we present \dypro, a novel deep neural network that learns from program execution traces. Compared to the prior dynamic models, not only is \dypro capable of generalizing across multiple executions for learning a program’s dynamic semantics in its entirety, but \dypro is also more efficient when dealing with programs yielding long execution traces. For evaluation, we task \dypro with semantic classification (i.e. categorizing programs based on their semantics) and compared it against two prominent static models: Gated Graph Neural Network and TreeLSTM. We find that \dypro achieves the highest prediction accuracy among all models. To further reveal the capacity of all aforementioned deep neural architectures, we examine if the models can learn to detect deeper semantic properties of a program. In particular given a task of recognizing loop invariants, we show \dypro beats all static models by a wide margin.</p>
</td>
	<td>representation dynamic </td>
</tr>



<tr>
	<td>2018</td>
	<td><a href="/publications/zhong2018generating/">Generating Regular Expressions from Natural Language Specifications: Are We There Yet?</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Generating Regular Expressions from Natural Language Specifications: Are We There Yet?' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Generating Regular Expressions from Natural Language Specifications: Are We There Yet?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Generating%20Regular%20Expressions%20from%20Natural%20Language%20Specifications:%20Are%20We%20There%20Yet?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Z. Zhong, J. Guo, W. Yang, T. Xie, JG Lou, Y. Liu, D. Zhang</td>
	<td>NLSE</td>
	<td><p>Recent  state-of-the-art  approaches  automatically  generate
regular  expressions  from  natural  language  specifications.
Given that these approaches use only synthetic data in both
training datasets and validation/test datasets, a natural question arises: are these approaches effective to address various
real-world  situations?  To  explore  this  question,  in  this  paper, we conduct a characteristic study on comparing two synthetic datasets used by the recent research and a real-world
dataset  collected  from  the  Internet,  and  conduct  an  experimental study on applying a state-of-the-art approach on the
real-world dataset. Our study results suggest the existence of
distinct characteristics between the synthetic datasets and the
real-world  dataset,  and  the  state-of-the-art  approach  (based
on  a  model  trained  from  a  synthetic  dataset)  achieves  extremely low effectiveness when evaluated on real-world data,
much lower than the effectiveness when evaluated on the synthetic  dataset.  We  also  provide  initial  analysis  on  some  of
those challenging cases and discuss future directions.</p>
</td>
	<td>bimodal generation </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/ott2018deep/">A Deep Learning Approach to Identifying Source Code in Images and Video</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Deep Learning Approach to Identifying Source Code in Images and Video' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Deep Learning Approach to Identifying Source Code in Images and Video' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=A%20Deep%20Learning%20Approach%20to%20Identifying%20Source%20Code%20in%20Images%20and%20Video' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>J. Ott, A. Atchison, P. Harnack, A. Bergh, E. Linstead</td>
	<td>MSR</td>
	<td><p>While substantial progress has been made in mining code on an
Internet scale, efforts to date have been overwhelmingly focused on
data sets where source code is represented natively as text. Large
volumes of source code available online and embedded in technical
videos have remained largely unexplored, due in part to the complexity of extraction when code is represented with images. Existing
approaches to code extraction and indexing in this environment rely
heavily on computationally intense optical character recognition.
To improve the ease and efficiency of identifying this embedded
code, as well as identifying similar code examples, we develop a
deep learning solution based on convolutional neural networks and
autoencoders. Focusing on Java for proof of concept, our technique
is able to identify the presence of typeset and handwritten source
code in thousands of video images with 85.6%-98.6% accuracy based
on syntactic and contextual features learned through deep architectures. When combined with traditional approaches, this provides
a more scalable basis for video indexing that can be incorporated
into existing software search and mining tools.</p>
</td>
	<td>information extraction </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/iyer2018mapping/">Mapping Language to Code in Programmatic Context</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Mapping Language to Code in Programmatic Context' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Mapping Language to Code in Programmatic Context' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Mapping%20Language%20to%20Code%20in%20Programmatic%20Context' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>S. Iyer, I. Konstas, A. Cheung, L. Zettlemoyer</td>
	<td>EMNLP</td>
	<td><p>Source code is rarely written in isolation. It depends significantly on the programmatic context, such as the class that the code would reside in. To study this phenomenon, we introduce the task of generating class member functions given English documentation and the programmatic context provided by the rest of the class. This task is challenging because the desired code can vary greatly depending on the functionality the class provides (e.g., a sort function may or may not be available when we are asked to “return the smallest element” in a particular member variable list). We introduce CONCODE, a new large dataset with over 100,000 examples consisting of Java classes from online code repositories, and develop a new encoder-decoder architecture that models the interaction between the method documentation and the class environment. We also present a detailed error analysis suggesting that there is significant room for future work on this task.</p>
</td>
	<td>bimodal generation </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/hellendoorn2018deep/">Deep Learning Type Inference</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Learning Type Inference' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Learning Type Inference' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Learning%20Type%20Inference' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>V. J. Hellendoorn, C. Bird, E. T. Barr, M. Allamanis</td>
	<td>FSE</td>
	<td><p>Dynamically typed languages such as JavaScript and Python are
increasingly popular, yet static typing has not been totally eclipsed:
Python now supports type annotations and languages like TypeScript offer a middle-ground for JavaScript: a strict superset of
JavaScript, to which it transpiles, coupled with a type system that
permits partially typed programs. However, static typing has a cost:
adding annotations, reading the added syntax, and wrestling with
the type system to fix type errors. Type inference can ease the
transition to more statically typed code and unlock the benefits of
richer compile-time information, but is limited in languages like
JavaScript as it cannot soundly handle duck-typing or runtime evaluation
via eval. We propose DeepTyper, a deep learning model
that understands which types naturally occur in certain contexts
and relations and can provide type suggestions, which can often
be verified by the type checker, even if it could not infer the type
initially. DeepTyper, leverages an automatically aligned corpus
of tokens and types to accurately predict thousands of variable
and function type annotations. Furthermore, we demonstrate that
context is key in accurately assigning these types and introduce a
technique to reduce overfitting on local cues while highlighting the
need for further improvements. Finally, we show that our model
can interact with a compiler to provide more than 4,000 additional
type annotations with over 95% precision that could not be inferred
without the aid of DeepTyper.</p>
</td>
	<td>representation types </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/hata2018learning/">Learning to Generate Corrective Patches using Neural Machine Translation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning to Generate Corrective Patches using Neural Machine Translation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning to Generate Corrective Patches using Neural Machine Translation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20to%20Generate%20Corrective%20Patches%20using%20Neural%20Machine%20Translation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>H. Hata, E. Shihab, G. Neubig</td>
	<td></td>
	<td><p>Bug fixing is generally a manually-intensive task. However, recent work has proposed the idea of automated program repair, which aims to repair (at least a subset of) bugs in different ways such as code mutation, etc. Following in the same line of work as automated bug repair, in this paper we aim to leverage past fixes to propose fixes of current/future bugs. Specifically, we propose Ratchet, a corrective patch generation system using neural machine translation. By learning corresponding pre-correction and post-correction code in past fixes with a neural sequence-to-sequence model, Ratchet is able to generate a fix code for a given bug-prone code query. We perform an empirical study with five open source projects, namely Ambari, Camel, Hadoop, Jetty and Wicket, to evaluate the effectiveness of Ratchet. Our findings show that Ratchet can generate syntactically valid statements 98.7% of the time, and achieve an F1-measure between 0.41-0.83 with respect to the actual fixes adopted in the code base. In addition, we perform a qualitative validation using 20 participants to see whether the generated statements can be helpful in correcting bugs. Our survey showed that Ratchet’s output was considered to be helpful in fixing the bugs on many occasions, even if fix was not 100% correct.</p>
</td>
	<td>repair generation </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/hashimoto2018retrieve/">A Retrieve-and-Edit Framework for Predicting Structured Outputs</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Retrieve-and-Edit Framework for Predicting Structured Outputs' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Retrieve-and-Edit Framework for Predicting Structured Outputs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=A%20Retrieve-and-Edit%20Framework%20for%20Predicting%20Structured%20Outputs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>T. B. Hashimoto, K. Guu, Y. Oren, P. Liang</td>
	<td>NIPS</td>
	<td><p>For the task of generating complex outputs such as source code, editing existing
outputs can be easier than generating complex outputs from scratch.  With this
motivation, we propose an approach that first retrieves a training example based on
the input (e.g., natural language description) and then edits it to the desired output
(e.g., code). Our contribution is a computationally efficient method for learning
a retrieval model that embeds the input in a task-dependent way without relying
on a hand-crafted metric or incurring the expense of jointly training the retriever
with the editor.  Our retrieve-and-edit framework can be applied on top of any
base model. We show that on a new autocomplete task for GitHub Python code
and the Hearthstone cards benchmark, retrieve-and-edit significantly boosts the
performance of a vanilla sequence-to-sequence model on both tasks.</p>
</td>
	<td>bimodal search generation </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/harer2018learning/">Learning to Repair Software Vulnerabilities with Generative Adversarial Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning to Repair Software Vulnerabilities with Generative Adversarial Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning to Repair Software Vulnerabilities with Generative Adversarial Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20to%20Repair%20Software%20Vulnerabilities%20with%20Generative%20Adversarial%20Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>J. A. Harer, O. Ozdemir, T. Lazovich, C. P. Reale, R. L. Russell, L. Y. Kim</td>
	<td>arXiv:1805.07475</td>
	<td><p>Motivated by the problem of automated repair of software vulnerabilities, we propose an adversarial learning approach that maps from one discrete source domain to another target domain without requiring paired labeled examples or source and target domains to be bijections. We demonstrate that the proposed adversarial learning approach is an effective technique for repairing software vulnerabilities, performing close to seq2seq approaches that require labeled pairs. The proposed Generative Adversarial Network approach is application-agnostic in that it can be applied to other problems similar to code repair, such as grammar correction or sentiment translation.</p>
</td>
	<td>repair generation </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/raghothaman2018user/">User-guided program reasoning using Bayesian inference</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=User-guided program reasoning using Bayesian inference' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=User-guided program reasoning using Bayesian inference' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=User-guided%20program%20reasoning%20using%20Bayesian%20inference' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>M. Raghothaman, S. Kulkarni, K. Helo, M. Naik</td>
	<td>PLDI</td>
	<td><p>Program analyses necessarily make approximations that often lead them to report true alarms interspersed with many false alarms. We propose a new approach to leverage user feedback to guide program analyses towards true alarms and away from false alarms. Our approach associates each alarm with a confidence value by performing Bayesian inference on a probabilistic model derived from the analysis rules. In each iteration, the user inspects the alarm with the highest confidence and labels its ground truth, and the approach recomputes the confidences of the remaining alarms given this feedback. It thereby maximizes the return on the effort by the user in inspecting each alarm. We have implemented our approach in a tool named Bingo for program analyses expressed in Datalog. Experiments with real users and two sophisticated analyses—a static datarace analysis for Java programs and a static taint analysis for Android apps—show significant improvements on a range of metrics, including false alarm rates and number of bugs found.</p>
</td>
	<td>program analysis </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/gupta2018intelligent/">Intelligent code reviews using deep learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Intelligent code reviews using deep learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Intelligent code reviews using deep learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Intelligent%20code%20reviews%20using%20deep%20learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>A. Gupta, N. Sundaresan</td>
	<td>KDD</td>
	<td><p>Peer code review is a best practice in Software Engineering where source code is reviewed manually by one or more peers(reviewers) of the code author. It is widely acceptable both in industry and open-source software (OSS) systems as a process for early detection and reduction of software defects. A larger chunk of reviews given during peer reviews are related to common issues such as coding style, documentations, and best practices. This makes the code review process less effective as reviewers focus less on finding important defects. Hence, there is a need to automatically find such common issues and help reviewers perform focused code reviews. Some of this is solved by rule based systems called linters but they are rigid and needs a lot of manual effort to adapt them for a new issue.</p>

<p>In this work, we present an automatic, flexible, and adaptive code analysis system called DeepCodeReviewer (DCR). DCR learns how to recommend code reviews related to common issues using historical peer reviews and deep learning. DCR uses deep learning to learn review relevance to a code snippet and recommend the right review from a repository of common reviews. DCR is trained on histroical peer reviews available from internal code repositories at Microsoft. Experiments demonstrate strong performance of developed deep learning model in classifying relevant and non-relevant reviews w.r.t to a code snippet, and ranking reviews given a code snippet. We have also evaluated DCR recommentations using a user study and survey. The results of our user study show good acceptance rate and answers of our survey questions are strongly correlated with our system’s goal of making code reviews focused on finding defects.</p>
</td>
	<td>representation review </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/gupta2018deep/">Deep Reinforcement Learning for Programming Language Correction</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Reinforcement Learning for Programming Language Correction' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Reinforcement Learning for Programming Language Correction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Reinforcement%20Learning%20for%20Programming%20Language%20Correction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>R. Gupta, A. Kanade, S. Shevade</td>
	<td></td>
	<td>Novice programmers often struggle with the formal
syntax of programming languages.  To assist them,
we design a novel programming language correction  framework  amenable  to  reinforcement  learning.  The framework allows an agent to mimic human  actions  for  text  navigation  and  editing.   We
demonstrate that the agent can be trained through
self-exploration directly from the raw input, that is,
program text itself, without any knowledge of the
formal syntax of the programming language.   We
leverage expert demonstrations for one tenth of the
training data to accelerate training.  The proposed
technique  is  evaluated  on 6975
erroneous  C  programs with typographic errors, written by students
during an introductory programming course.  Our
technique fixes 14%
more programs and 29% more
compiler error messages relative to those fixed by
a state-of-the-art tool, DeepFix, which uses a fully
supervised neural machine translation approach.
</td>
	<td>repair generation </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/gu2018deep/">Deep Code Search</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Code Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Code Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Code%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>X. Gu, H. Zhang, S. Kim</td>
	<td>ICSE</td>
	<td><p>To implement a program functionality, developers can reuse previously written code snippets by searching through a large-scale codebase. Over the years, many code search tools have been proposed to help developers. The existing approaches often treat source code as textual documents and utilize information retrieval models to retrieve relevant code snippets that match a given query. These approaches mainly rely on the textual similarity between source code and natural language query. They lack a deep understanding of the semantics of queries and source code.</p>

<p>In this paper, we propose a novel deep neural network named CODEnn (Code-Description Embedding Neural Network). Instead of matching text similarity, CODEnn jointly embeds code snippets and natural language descriptions into a high-dimensional vector space, in such a way that code snippet and its corresponding description have similar vectors. Using the unified vector representation, code snippets related to a natural language query can be retrieved according to their vectors. Semantically related words can also be recognized and irrelevant/noisy keywords in queries can be handled.</p>

<p>As a proof-of-concept application, we implement a code search tool named DeepCS using the proposed CODEnn model. We empirically evaluate DeepCS on a large scale codebase collected from GitHub. The experimental results show that our approach can effectively retrieve relevant code snippets and outperforms previous techniques.</p>

</td>
	<td>search </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/parvez2018building/">Building Language Models for Text with Named Entities</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Building Language Models for Text with Named Entities' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Building Language Models for Text with Named Entities' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Building%20Language%20Models%20for%20Text%20with%20Named%20Entities' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>M.R. Parvez, S. Chakraborty, B. Ray, KW Chang</td>
	<td>ACL</td>
	<td><p>Text  in  many  domains  involves  a  significant amount of named entities.   Predicting the entity names is often challenging
for a language model as they appear less
frequent  on  the  training  corpus.   In  this
paper,  we  propose  a  novel  and  effective
approach to building a discriminative language  model  which  can  learn  the  entity
names by leveraging their entity type information.  We also introduce two benchmark  datasets  based  on  recipes  and  Java
programming codes,  on which we evaluate the proposed model.  Experimental results show that our model achieves 52.2%
better perplexity in recipe generation and
22.06% on code generation than the state-of-the-art language models.</p>
</td>
	<td>language model </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/richardson2018polyglot/">Polyglot Semantic Parsing in APIs</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Polyglot Semantic Parsing in APIs' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Polyglot Semantic Parsing in APIs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Polyglot%20Semantic%20Parsing%20in%20APIs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Kyle Richardson, Jonathan Berant, Jonas Kuhn</td>
	<td>NAACL</td>
	<td><p>Traditional approaches to semantic parsing (SP) work by training individual models for each available parallel dataset of text-meaning pairs. In this paper, we explore the idea of polyglot semantic translation, or learning semantic parsing models that are trained on multiple datasets and natural languages. In particular, we focus on translating text to code signature representations using the software component datasets of Richardson and Kuhn (2017a,b). The advantage of such models is that they can be used for parsing a wide variety of input natural languages and output programming languages, or mixed input languages, using a single unified model. To facilitate modeling of this type, we develop a novel graph-based decoding framework that achieves state-of-the-art performance on the above datasets, and apply this method to two other benchmark SP tasks.</p>
</td>
	<td>bimodal API </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/russell2018automated/">Automated Vulnerability Detection in Source Code Using Deep Representation Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Automated Vulnerability Detection in Source Code Using Deep Representation Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Automated Vulnerability Detection in Source Code Using Deep Representation Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Automated%20Vulnerability%20Detection%20in%20Source%20Code%20Using%20Deep%20Representation%20Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>R. L. Russell, L. Kim, L. H. Hamilton, T. Lazovich, J. A. Harer, O. Ozdemir, P. M. Ellingwood, M. W. McConley</td>
	<td></td>
	<td><p>Increasing numbers of software vulnerabilities are discovered every year whether they are reported publicly or discovered internally in proprietary code. These vulnerabilities can pose serious risk of exploit and result in system compromise, information leaks, or denial of service. We leveraged the wealth of C and C++ open-source code available to develop a large-scale function-level vulnerability detection system using machine learning. To supplement existing labeled vulnerability datasets, we compiled a vast dataset of millions of open-source functions and labeled it with carefully-selected findings from three different static analyzers that indicate potential exploits. Using these datasets, we developed a fast and scalable vulnerability detection tool based on deep feature representation learning that directly interprets lexed source code. We evaluated our tool on code from both real software packages and the NIST SATE IV benchmark dataset. Our results demonstrate that deep feature representation learning on source code is a promising approach for automated software vulnerability detection.</p>
</td>
	<td>program analysis </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/saini2018oreo/">Oreo: detection of clones in the twilight zone</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Oreo: detection of clones in the twilight zone' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Oreo: detection of clones in the twilight zone' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Oreo:%20detection%20of%20clones%20in%20the%20twilight%20zone' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>V. Saini, F. Farmahinifarahani, Y. Lu, P. Baldi, C. Lopes</td>
	<td>ESEC/FSE</td>
	<td><p>Source code clones are categorized into four types of increasing difficulty of detection, ranging from purely textual (Type-1) to purely semantic (Type-4). Most clone detectors reported in the literature work well up to Type-3, which accounts for syntactic differences. In between Type-3 and Type-4, however, there lies a spectrum of clones that, although still exhibiting some syntactic similarities, are extremely hard to detect – the Twilight Zone. Most clone detectors reported in the literature fail to operate in this zone. We present Oreo, a novel approach to source code clone detection that not only detects Type-1 to Type-3 clones accurately, but is also capable of detecting harder-to-detect clones in the Twilight Zone. Oreo is built using a combination of machine learning, information retrieval, and software metrics. We evaluate the recall of Oreo on BigCloneBench, and perform manual evaluation for precision. Oreo has both high recall and precision. More importantly, it pushes the boundary in detection of clones with moderate to weak syntactic similarity in a scalable manner.</p>
</td>
	<td>clone metrics </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/santos2018syntax/">Syntax and Sensibility: Using language models to detect and correct syntax errors</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Syntax and Sensibility: Using language models to detect and correct syntax errors' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Syntax and Sensibility: Using language models to detect and correct syntax errors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Syntax%20and%20Sensibility:%20Using%20language%20models%20to%20detect%20and%20correct%20syntax%20errors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>E. A. Santos, J. C. Campbell, D. Patel, A. Hindle, J. N. Amaral</td>
	<td>SANER</td>
	<td><p>Syntax errors are made by novice and experienced programmers alike; however, novice programmers lack the years of experience that help them quickly resolve these frustrating errors. Standard LR parsers are of little help, typically resolving syntax errors and their precise location poorly. We propose a methodology that locates where syntax errors occur, and suggests possible changes to the token stream that can fix the error identified. This methodology finds syntax errors by using language models trained on correct source code to find tokens that seem out of place. Fixes are synthesized by consulting the language models to determine what tokens are more likely at the estimated error location. We compare <em>n</em>-gram and LSTM (long short-term memory) language models for this task, each trained on a large corpus of Java code collected from GitHub. Unlike prior work, our methodology does not rely that the problem source code comes from the same domain as the training data. We evaluated against a repository of real student mistakes. Our tools are able to find a syntactically-valid fix within its top-2 suggestions, often producing the exact fix that the student used to resolve the error. The results show that this tool and methodology can locate and suggest corrections for syntax errors. Our methodology is of practical use to all programmers, but will be especially useful to novices frustrated with incomprehensible syntax errors.</p>
</td>
	<td>repair language model </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/defreez2018path/">Path-Based Function Embedding and its Application to Specification Mining</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Path-Based Function Embedding and its Application to Specification Mining' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Path-Based Function Embedding and its Application to Specification Mining' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Path-Based%20Function%20Embedding%20and%20its%20Application%20to%20Specification%20Mining' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>D. DeFreez, A. V. Thakur, C. Rubio-González</td>
	<td>ICSE</td>
	<td><p>Identifying the relationships among program elements is useful
for program understanding, debugging, and analysis. One such
relationship is synonymy. Function synonyms are functions that
play a similar role in code, e.g. functions that perform initialization
for different device drivers, or functions that implement different
symmetric-key encryption schemes. Function synonyms are not
necessarily semantically equivalent and can be syntactically dissimilar; consequently, approaches for identifying code clones or
functional equivalence cannot be used to identify them. This paper presents <code class="language-plaintext highlighter-rouge">func2vec</code>, an algorithm that maps each function to a vector in a vector space such that function synonyms are grouped
together. We compute the function embedding by training a neu-
ral network on sentences generated from random walks over an
encoding of the program as a labeled pushdown system (ℓ-PDS).
We demonstrate that <code class="language-plaintext highlighter-rouge">func2vec</code>
is effective at identifying function
synonyms in the Linux kernel. Furthermore, we show how function
synonyms enable mining error-handling specifications with high
support in Linux file systems and drivers.</p>
</td>
	<td>program analysis representation </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/lanchantin2018exploring/">Exploring the Naturalness of Buggy Code with Recurrent Neural Network</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Exploring the Naturalness of Buggy Code with Recurrent Neural Network' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Exploring the Naturalness of Buggy Code with Recurrent Neural Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Exploring%20the%20Naturalness%20of%20Buggy%20Code%20with%20Recurrent%20Neural%20Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>J. Lanchantin, J. Gao</td>
	<td></td>
	<td><p>Statistical   language   models   are   powerful   tools
which  have  been  used  for  many  tasks  within  natural language processing. Recently, they have been
used for other sequential data such as source code.
(Ray et al., 2015) showed that it is possible train an
n-gram
source code language mode,  and use it to
predict buggy lines in code by determining “unnatural” lines via entropy with respect to the language
model.  In this work, we propose using a more advanced language modeling technique, Long Short-term Memory recurrent neural networks, to model
source code and classify buggy lines based on entropy.   We  show  that  our  method  slightly  outperforms an
n-gram model in the buggy line classification task using AUC</p>
</td>
	<td>language model defect </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/dash2018refinym/">RefiNym: Using Names to Refine Types</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=RefiNym: Using Names to Refine Types' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=RefiNym: Using Names to Refine Types' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=RefiNym:%20Using%20Names%20to%20Refine%20Types' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>S. Dash, M. Allamanis, E. T. Barr</td>
	<td>FSE</td>
	<td><p>Source code is bimodal: it combines a formal algorithmic channel and a natural language channel of identifiers and comments. In this work, we model the bimodality of code with name lows, an assignment low graph augmented to track identiier names. Conceptual types are logically distinct types that do not always coincide with program types. Passwords and URLs are example conceptual types that can share the program type string. Our tool, RefiNym, is an unsupervised method that mines a lattice of conceptual types from name lows and reiies them into distinct nominal types. For string, RefiNym inds and splits conceptual types originally merged into a single type, reducing the number of same-type variables per scope from 8.7 to 2.2 while eliminating 21.9% of scopes that have more than one same-type variable in scope. This makes the code more self-documenting and frees the type system to prevent a developer from inadvertently assigning data across conceptual types.</p>
</td>
	<td>program analysis types </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/cvitkovic2018open/">Open Vocabulary Learning on Source Code with a Graph-Structured Cache</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Open Vocabulary Learning on Source Code with a Graph-Structured Cache' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Open Vocabulary Learning on Source Code with a Graph-Structured Cache' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Open%20Vocabulary%20Learning%20on%20Source%20Code%20with%20a%20Graph-Structured%20Cache' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>M. Cvitkovic, B. Singh, A. Anandkumar</td>
	<td></td>
	<td><p>Machine learning models that take computer program source code as input typically use Natural Language Processing (NLP) techniques. However, a major challenge is that code is written using an open, rapidly changing vocabulary due to, e.g., the coinage of new variable and method names. Reasoning over such a vocabulary is not something for which most NLP methods are designed. We introduce a Graph-Structured Cache to address this problem; this cache contains a node for each new word the model encounters with edges connecting each word to its occurrences in the code. We find that combining this graph-structured cache strategy with recent Graph-Neural-Network-based models for supervised learning on code improves the models’ performance on a code completion task and a variable naming task — with over 100% relative improvement on the latter — at the cost of a moderate increase in computation time.</p>
</td>
	<td>GNN variable misuse defect representation </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/cummins2018compiler/">Compiler Fuzzing through Deep Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Compiler Fuzzing through Deep Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Compiler Fuzzing through Deep Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Compiler%20Fuzzing%20through%20Deep%20Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>C. Cummins, P. Petoumenos, H. Leather, A. Murray</td>
	<td>ISSTA</td>
	<td><p>Random program generation — fuzzing — is an effective technique
for discovering bugs in compilers but successful fuzzers require
extensive development effort for every language supported by the
compiler, and often leave parts of the language space untested.</p>

<p>We introduce DeepSmith, a novel machine learning approach
to accelerating compiler validation through the inference of generative models for compiler inputs. Our approach
infers a learned
model of the structure of real world code based on a large corpus of open source code. Then, it uses the model to automatically
generate tens of thousands of realistic programs. Finally, we apply
established differential testing methodologies on them to expose
bugs in compilers. We apply our approach to the OpenCL programming language, automatically exposing bugs with little effort on our
side. In 1,000 hours of automated testing of commercial and open
source compilers, we discover bugs in all of them, submitting 67
bug reports. Our test cases are on average two orders of magnitude
smaller than the state-of-the-art, require 3.03× less time to generate
and evaluate, and expose bugs which the state-of-the-art cannot.
Our random program generator, comprising only 500 lines of code,
took 12 hours to train for OpenCL versus the state-of-the-art taking
9 man months to port from a generator for C and 50,000 lines of
code. With 18 lines of code we extended our program generator to
a second language, uncovering crashes in Solidity compilers in 12
hours of automated testing.</p>
</td>
	<td>fuzzing generation </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/shirani2018evaluation/">Evaluation of Type Inference with Textual Cues</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Evaluation of Type Inference with Textual Cues' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Evaluation of Type Inference with Textual Cues' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Evaluation%20of%20Type%20Inference%20with%20Textual%20Cues' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>A. Shirani, A. P. Lopez-Monroy, F. Gonzalez, T. Solorio, M.A. Alipour</td>
	<td>NLSE</td>
	<td><p>Type information plays an important role in the success of information retrieval and recommendation systems in software
engineering. Thus, the absence of types in dynamically-typed
languages poses a challenge to adapt these systems to support
dynamic languages.</p>

<p>In this paper, we explore the viability of type inference using
textual cues.  That is, we formulate the type inference problem as a classification problem which uses the textual features
in  the  source  code  to  predict  the type  of  variables.   In  this
approach, a classifier learns a model to distinguish between
types of variables in a program.  The model is subsequently
used to (approximately) infer the types of other variables.</p>

<p>We  evaluate  the  feasibility  of  this  approach  on  four  Java
projects wherein type information is already available in the
source code and can be used to train and test a classifier. Our
experiments show this approach can predict the type of new
variables  with  relatively  high  accuracy  (80% F-measure).
These results suggest that textual cues can be
complementary
tools in inferring types for dynamic languages.</p>
</td>
	<td>information extraction </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/si2018learning/">Learning Loop Invariants for Program Verification</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning Loop Invariants for Program Verification' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning Loop Invariants for Program Verification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20Loop%20Invariants%20for%20Program%20Verification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>X. Si, H. Dai, M. Raghothaman, M. Naik, L. Song</td>
	<td>NIPS</td>
	<td><p>A fundamental problem in program verification concerns inferring loop invariants.
The problem is undecidable and even practical instances are challenging. Inspired
by how human experts construct loop invariants, we propose a reasoning framework
CODE2INV
that constructs the solution by multi-step decision making and querying
an external program graph memory block. By training with reinforcement learning,
CODE2INV
captures rich program features and avoids the need for ground truth
solutions as supervision.  Compared to previous learning tasks in domains with
graph-structured data, it addresses unique challenges, such as a binary objective
function and an extremely sparse reward that is given by an automated theorem
prover only after the complete loop invariant is proposed. We evaluate
CODE2INV on
a suite of 133 benchmark problems and compare it to three state-of-the-art systems.
It solves 106 problems compared to 73 by a stochastic search-based system, 77 by
a heuristic search-based system, and 100 by a decision tree learning-based system.
Moreover, the strategy learned can be generalized to new programs: compared to
solving new instances from scratch, the pre-trained agent is more sample efficient
in finding solutions.</p>
</td>
	<td>program analysis verification </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/tufano2018deep/">Deep Learning Similarities from Different Representations of Source Code</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Learning Similarities from Different Representations of Source Code' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Learning Similarities from Different Representations of Source Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Learning%20Similarities%20from%20Different%20Representations%20of%20Source%20Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>M. Tufano, C. Watson, G. Bavota, M. Di Penta, M. White, D. Poshyvanyk</td>
	<td>MSR</td>
	<td><p>Assessing the similarity between code components plays a pivotal
role in a number of Software Engineering (SE) tasks, such as clone
detection, impact analysis, refactoring, <em>etc.</em> 
Code similarity is generally measured by relying on manually defined or hand-crafted
features,  e.g.,  by analyzing the overlap among identifiers or comparing the Abstract Syntax Trees of two code components. These
features represent a  best guess at what SE researchers can utilize to
exploit and reliably assess code similarity for a given task. Recent
work has shown, when using a stream of identifiers to represent
the code, that Deep Learning (DL) can effectively replace manual
feature engineering for the task of clone detection. However, source
code can be represented at different levels of abstraction: identifiers, Abstract Syntax Trees, Control Flow Graphs, and Bytecode.
We conjecture that each code representation can provide a different,
yet orthogonal view of the same code fragment, thus, enabling a
more reliable detection of similarities in code. In this paper, we
demonstrate how SE tasks can benefit from a DL-based approach,
which can automatically learn code similarities from different representations.</p>
</td>
	<td>representation clone </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/tufano2018empirical/">An Empirical Study on Learning Bug-Fixing Patches in the Wild via Neural Machine Translation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=An Empirical Study on Learning Bug-Fixing Patches in the Wild via Neural Machine Translation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=An Empirical Study on Learning Bug-Fixing Patches in the Wild via Neural Machine Translation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=An%20Empirical%20Study%20on%20Learning%20Bug-Fixing%20Patches%20in%20the%20Wild%20via%20Neural%20Machine%20Translation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>M. Tufano, C. Watson, G. Bavota, M. Di Penta, M. White, D. Poshyvanyk</td>
	<td></td>
	<td><p>Millions of open-source projects with numerous bug fixes are available in code repositories. This proliferation of software development histories can be leveraged to learn how to fix common programming bugs. To explore such a potential, we perform an empirical study to assess the feasibility of using Neural Machine Translation techniques for learning bug-fixing patches for real defects. First, we mine millions of bug-fixes from the change histories of projects hosted on GitHub, in order to extract meaningful examples of such bug-fixes. Next, we abstract the buggy and corresponding fixed code, and use them to train an Encoder-Decoder model able to translate buggy code into its fixed version. In our empirical investigation we found that such a model is able to fix thousands of unique buggy methods in the wild. Overall, this model is capable of predicting fixed patches generated by developers in 9-50% of the cases, depending on the number of candidate patches we allow it to generate. Also, the model is able to emulate a variety of different Abstract Syntax Tree operations and generate candidate patches in a split second.</p>
</td>
	<td>repair </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/tufano2018learning/">Learning How to Mutate Source Code from Bug-Fixes</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning How to Mutate Source Code from Bug-Fixes' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning How to Mutate Source Code from Bug-Fixes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20How%20to%20Mutate%20Source%20Code%20from%20Bug-Fixes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>M. Tufano, C. Watson, G. Bavota, M. Di Penta, M. White, D. Poshyvanyk</td>
	<td></td>
	<td><p>Mutation testing has been widely accepted as an approach to guide test case generation or to assess the effectiveness of test suites. Empirical studies have shown that mutants are representative of real faults; yet they also indicated a clear need for better, possibly customized, mutation operators and strategies. While some recent papers have tried to devise domain-specific or general purpose mutator operators by manually analyzing real faults, such an activity is effort- (and error-) prone and does not deal with an important practical question as to how to really mutate a given source code element. We propose a novel approach to automatically learn mutants from faults in real programs. First, our approach processes bug fixing changes using fine-grained differencing, code abstraction, and change clustering. Then, it learns mutation models using a deep learning strategy. We have trained and evaluated our technique on a set of ~787k bugs mined from GitHub. Starting from code fixed by developers in the context of a bug-fix, our empirical evaluation showed that our models are able to predict mutants that resemble original fixed bugs in between 9% and 45% of the cases (depending on the model). Moreover, over 98% of the automatically generated mutants are lexically and syntactically correct.</p>
</td>
	<td>repair edit </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/chakraborty2018tree2tree/">CODIT: Code Editing with Tree-Based Neural Machine Translation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=CODIT: Code Editing with Tree-Based Neural Machine Translation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=CODIT: Code Editing with Tree-Based Neural Machine Translation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=CODIT:%20Code%20Editing%20with%20Tree-Based%20Neural%20Machine%20Translation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>S. Chakraborty, M. Allamanis, B. Ray</td>
	<td></td>
	<td><p>The way developers edit day-to-day code tends to be repetitive, often using existing code elements. Many researchers have tried to automate repetitive code changes by learning from specific change templates which are applied to limited scope. The advancement of Neural Machine Translation (NMT) and the availability of vast open-source evolutionary data opens up the possibility of automatically learning those templates from the wild. However, unlike natural languages, for which NMT techniques were originally devised, source code and its changes have certain properties. For instance, compared to natural language, source code vocabulary can be significantly larger. Further, good changes in code do not break its syntactic structure. Thus, deploying state-of-the-art NMT models without adapting the methods to the source code domain yields sub-optimal results. To this end, we propose a novel Tree based NMT system to model source code changes and learn code change patterns from the wild. We realize our model with a change suggestion engine: CODIT and train the model with more than 30k real-world changes and evaluate it on 6k patches. Our evaluation shows the effectiveness of CODIT in learning and suggesting patches.CODIT also shows promise generating bug fix patches.</p>
</td>
	<td>grammar AST repair generation </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/wan2018improving/">Improving Automatic Source Code Summarization via Deep Reinforcement Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Improving Automatic Source Code Summarization via Deep Reinforcement Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Improving Automatic Source Code Summarization via Deep Reinforcement Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Improving%20Automatic%20Source%20Code%20Summarization%20via%20Deep%20Reinforcement%20Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Y. Wan, Z. Zhao, M. Yang, G. Xu, H. Ying, J. Wu, P.S. Yu</td>
	<td>ASE</td>
	<td><p>Code summarization provides a high level natural language description of the function performed by code, as it can benefit the software maintenance, code categorization and retrieval. To the best of our knowledge, most state-of-the-art approaches follow an encoder-decoder framework which encodes the code into a hidden space and then decode it into natural language space, suffering from two major drawbacks: a) Their encoders only consider the sequential content of code, ignoring the tree structure which is also critical for the task of code summarization; b) Their decoders are typically trained to predict the next word by maximizing the likelihood of next ground-truth word with previous ground-truth word given. However, it is expected to generate the entire sequence from scratch at test time. This discrepancy can cause an exposure bias issue, making the learnt decoder suboptimal. In this paper, we incorporate an abstract syntax tree structure as well as sequential content of code snippets into a deep reinforcement learning framework (i.e., actor-critic network). The actor network provides the confidence of predicting the next word according to current state. On the other hand, the critic network evaluates the reward value of all possible extensions of the current state and can provide global guidance for explorations. We employ an advantage reward composed of BLEU metric to train both networks. Comprehensive experiments on a real-world dataset show the effectiveness of our proposed model when compared with some state-of-the-art methods.</p>
</td>
	<td>summarization documentation </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/bui2018hierarchical/">Hierarchical Learning of Cross-Language Mappings through Distributed Vector Representations for Code</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Hierarchical Learning of Cross-Language Mappings through Distributed Vector Representations for Code' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Hierarchical Learning of Cross-Language Mappings through Distributed Vector Representations for Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Hierarchical%20Learning%20of%20Cross-Language%20Mappings%20through%20Distributed%20Vector%20Representations%20for%20Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>N. D. Q. Bui, L. Jiang</td>
	<td>ICSE</td>
	<td><p>Translating a program written in one programming language to another can be useful for software development tasks that need functionality implementations in different languages. Although past studies have considered this problem, they may be either specific to the language grammars, or specific to certain kinds of code elements (e.g., tokens, phrases, API uses). This paper proposes a new approach to automatically learn cross-language representations for various kinds of structural code elements that may be used for program translation. Our key idea is two folded: First, we normalize and enrich code token streams with additional structural and semantic information, and train cross-language vector representations for the tokens (a.k.a. shared embeddings based on word2vec, a neural-network-based technique for producing word embeddings; Second, hierarchically from bottom up, we construct shared embeddings for code elements of higher levels of granularity (e.g., expressions, statements, methods) from the embeddings for their constituents, and then build mappings among code elements across languages based on similarities among embeddings. 
Our preliminary evaluations on about 40,000 Java and C# source files from 9 software projects show that our approach can automatically learn shared embeddings for various code elements in different languages and identify their cross-language mappings with reasonable Mean Average Precision scores. When compared with an existing tool for mapping library API methods, our approach identifies many more mappings accurately. The mapping results and code can be accessed at this https URL. We believe that our idea for learning cross-language vector representations with code structural information can be a useful step towards automated program translation.</p>
</td>
	<td>representation </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/bui2018cross/">Cross-Language Learning for Program Classification using Bilateral Tree-Based Convolutional Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Cross-Language Learning for Program Classification using Bilateral Tree-Based Convolutional Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Cross-Language Learning for Program Classification using Bilateral Tree-Based Convolutional Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Cross-Language%20Learning%20for%20Program%20Classification%20using%20Bilateral%20Tree-Based%20Convolutional%20Neural%20Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>N. Bui, L. Jiang, Y. Yu</td>
	<td>NLSE</td>
	<td><p>Towards the vision of translating code that implements an algorithm from one programming language into another, this
paper  proposes  an  approach  for  automated  program  classification using
bilateral tree-based convolutional neural networks
(BiTBCNNs).  It  is  layered  on  top  of  two  tree-based
convolutional neural networks (TBCNNs), each of which recognizes the algorithm of code written in an individual programming language. The combination layer of the networks
recognizes the similarities and differences among code in different programming languages. The BiTBCNNs are trained
using  the  source  code  in  different  languages  but  known  to
implement  the  same  algorithms  and/or  functionalities.  For
a  preliminary  evaluation,  we  use  3591  Java  and  3534  C++
code snippets from 6 algorithms we crawled systematically
from GitHub. We obtained over 90% accuracy in the cross-language binary classification task to tell whether any given
two code snippets implement a same algorithm. Also, for the
algorithm classification task, i.e., to predict which one of the
six algorithm labels is implemented by an arbitrary C++ code
snippet, we achieved over 80% precision.</p>
</td>
	<td>representation grammar </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/lin2018nl2bash/">NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System.</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System.' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System.' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=NL2Bash:%20A%20Corpus%20and%20Semantic%20Parser%20for%20Natural%20Language%20Interface%20to%20the%20Linux%20Operating%20System.' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>X.V. Lin, C. Wang, L. Zettlemoyer and M.D. Ernst</td>
	<td>LREC</td>
	<td><p>We present new data and semantic parsing methods for the problem of mapping english sentences to Bash commands (NL2Bash). Our long-term goal is to enable any user to easily solve otherwise repetitive tasks (such as file manipulation, search, and application-specific scripting) by simply stating their intents in English. We take a first step in this domain, by providing a large new dataset of challenging but commonly used commands paired with their English descriptions, along with the baseline methods to establish performance levels on this task.</p>
</td>
	<td>bimodal generation </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/bui2018bilateral/">Bilateral Dependency Neural Networks for Cross-Language Algorithm Classification</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Bilateral Dependency Neural Networks for Cross-Language Algorithm Classification' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Bilateral Dependency Neural Networks for Cross-Language Algorithm Classification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Bilateral%20Dependency%20Neural%20Networks%20for%20Cross-Language%20Algorithm%20Classification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>N. D. Q. Bui, Y. Yu, L. Jiang</td>
	<td>SANER</td>
	<td><p>Algorithm  classification  is  to  automatically  identify
the  classes  of  a  program  based  on  the  algorithm(s)  and/or  data
structure(s)  implemented  in  the  program.  It  can  be  useful  for
various tasks, such as code reuse, code theft detection, and malware detection. Code similarity metrics, on the basis of features
extracted from syntax and semantics, have been used to classify
programs.  Such  features,  however,  often  need  manual  selection
effort  and  are  specific  to  individual  programming  languages,
limiting  the  classifiers  to  programs  in  the  same  language.
To recognize the similarities and differences among algorithms
implemented   in   different   languages,   this   paper   describes   a
framework  of  Bilateral  Neural  Networks  (Bi-NN)  that  builds  a
neural  network  on  top  of  two  underlying  sub-networks,  each  of
which encodes syntax and semantics of code in one language. A
whole  Bi-NN  can  be  trained  with  bilateral  programs  that  implement the same algorithms and/or data structures in different
languages  and  then  be  applied  to  recognize  algorithm  classes
across  languages.</p>

<p>We  have  instantiated  the  framework  with  several  kinds  of
token-,  tree-  and  graph-based  neural  networks  that  encode  and
learn  various  kinds  of  information  in  code.  We  have  applied
the  instances  of  the  framework  to  a  code  corpus  collected  from
GitHub containing thousands of Java and C++ programs imple-
menting 50 different algorithms and data structures. Our evalua-
tion results show that the use of Bi-NN indeed produces promising
algorithm  classification  results  both  within  one  language  and
across  languages,  and  the  encoding  of  dependencies  from  code
into  the  underlying  neural  networks  helps  improve  algorithm
classification  accuracy  further.  In  particular,  our  custom-built
dependency trees with tree-based convolutional neural networks
achieve  the  highest  classification  accuracy  among  the  different
instances  of  the  framework  that  we  have  evaluated.  Our  study
points  to  a  possible  future  research  direction  to  tailor  bilateral
and  multilateral  neural  networks  that  encode  more  relevant
semantics  for  code  learning,  mining  and  analysis  tasks</p>
</td>
	<td>representation </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/bhatia2018neurosymbolic/">Neuro-symbolic program corrector for introductory programming assignments</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Neuro-symbolic program corrector for introductory programming assignments' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Neuro-symbolic program corrector for introductory programming assignments' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Neuro-symbolic%20program%20corrector%20for%20introductory%20programming%20assignments' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>S. Bhatia, P. Kohli, R. Singh</td>
	<td>ICSE</td>
	<td><p>Automatic correction of programs is a challenging problem with numerous real world applications in security, verification, and education. One application that is becoming increasingly important is the correction of student submissions in online courses for providing feedback. Most existing program repair techniques analyze Abstract Syntax Trees (ASTs) of programs, which are unfortunately unavailable for programs with syntax errors. In this paper, we propose a novel Neuro-symbolic approach that combines neural networks with constraint-based reasoning. Specifically, our method first uses a Recurrent Neural Network (RNN) to perform syntax repairs for the buggy programs; subsequently, the resulting syntactically-fixed programs are repaired using constraint-based techniques to ensure functional correctness. The RNNs are trained using a corpus of syntactically correct submissions for a given programming assignment, and are then queried to fix syntax errors in an incorrect programming submission by replacing or inserting the predicted tokens at the error location. We evaluate our technique on a dataset comprising of over 14,500 student submissions with syntax errors. Our method is able to repair syntax errors in 60% (8689) of submissions, and finds functionally correct repairs for 23.8% (3455) submissions.</p>
</td>
	<td>repair </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/bennun2018neural/">Neural Code Comprehension: A Learnable Representation of Code Semantics</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Neural Code Comprehension: A Learnable Representation of Code Semantics' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Neural Code Comprehension: A Learnable Representation of Code Semantics' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Neural%20Code%20Comprehension:%20A%20Learnable%20Representation%20of%20Code%20Semantics' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>T. Ben-Nun A. S. Jakobovits, T. Hoefler</td>
	<td>NIPS</td>
	<td><p>With the recent success of embeddings in natural language processing, research has been conducted into applying similar methods to code analysis. Most works attempt to process the code directly or use a syntactic tree representation, treating it like sentences written in a natural language. However, none of the existing methods are sufficient to comprehend program semantics robustly, due to structural features such as function calls, branching, and interchangeable order of statements. In this paper, we propose a novel processing technique to learn code semantics, and apply it to a variety of program analysis tasks. In particular, we stipulate that a robust distributional hypothesis of code applies to both human- and machine-generated programs. Following this hypothesis, we define an embedding space, inst2vec, based on an Intermediate Representation (IR) of the code that is independent of the source programming language. We provide a novel definition of contextual flow for this IR, leveraging both the underlying data- and control-flow of the program. We then analyze the embeddings qualitatively using analogies and clustering, and evaluate the learned representation on three different high-level tasks. We show that with a single RNN architecture and pre-trained fixed embeddings, inst2vec outperforms specialized approaches for performance prediction (compute device mapping, optimal thread coarsening); and algorithm classification from raw code (104 classes), where we set a new state-of-the-art.</p>
</td>
	<td>representation </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/liu2018neural/">Neural-Machine-Translation-Based Commit Message Generation: How Far Are We?</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Neural-Machine-Translation-Based Commit Message Generation: How Far Are We?' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Neural-Machine-Translation-Based Commit Message Generation: How Far Are We?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Neural-Machine-Translation-Based%20Commit%20Message%20Generation:%20How%20Far%20Are%20We?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Z. Liu, X. Xia, A.E. Hassan, D. Lo, Z. Xing, X. Wang</td>
	<td>ASE</td>
	<td><p>Commit messages can be regarded as the documentation of software changes. These messages describe the content and purposes of changes, hence are useful for program comprehension and software maintenance. However, due to the lack of time and direct motivation, commit messages sometimes are neglected by developers. To address this problem, Jiang et al. proposed an approach (we refer to it as NMT), which leverages a neural machine translation algorithm to automatically generate short commit messages from code. The reported performance of their approach is promising, however, they did not explore why their approach performs well. Thus, in this paper, we first perform an in-depth analysis of their experimental results. We find that (1) Most of the test &lt;pre&gt;diffs&lt;/pre&gt; from which NMT can generate high-quality messages are similar to one or more training &lt;pre&gt;diffs&lt;/pre&gt; at the token level. (2) About 16% of the commit messages in Jiang et al.’s dataset are noisy due to being automatically generated or due to them describing repetitive trivial changes. (3) The performance of NMT declines by a large amount after removing such noisy commit messages. In addition, NMT is complicated and time-consuming. Inspired by our first finding, we proposed a simpler and faster approach, named NNGen (Nearest Neighbor Generator), to generate concise commit messages using the nearest neighbor algorithm. Our experimental results show that NNGen is over 2,600 times faster than NMT, and outperforms NMT in terms of BLEU (an accuracy measure that is widely used to evaluate machine translation systems) by 21%. Finally, we also discuss some observations for the road ahead for automated commit message generation to inspire other researchers.</p>
</td>
	<td>edit summarization </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/yao2018staqc/">StaQC: A Systematically Mined Question-Code Dataset from Stack Overflow</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=StaQC: A Systematically Mined Question-Code Dataset from Stack Overflow' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=StaQC: A Systematically Mined Question-Code Dataset from Stack Overflow' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=StaQC:%20A%20Systematically%20Mined%20Question-Code%20Dataset%20from%20Stack%20Overflow' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Ziyu Yao, Daniel S. Weld, Wei-Peng Chen, Huan Sun</td>
	<td>WWW 2018</td>
	<td><p>Stack Overflow (SO) has been a great source of natural language questions and their code solutions (i.e., question-code pairs), which are critical for many tasks including code retrieval and annotation. In most existing research, question-code pairs were collected heuristically and tend to have low quality. In this paper, we investigate a new problem of systematically mining question-code pairs from Stack Overflow (in contrast to heuristically collecting them). It is formulated as predicting whether or not a code snippet is a standalone solution to a question. We propose a novel Bi-View Hierarchical Neural Network which can capture both the programming content and the textual context of a code snippet (i.e., two views) to make a prediction. On two manually annotated datasets in Python and SQL domain, our framework substantially outperforms heuristic methods with at least 15% higher F1 and accuracy. Furthermore, we present StaQC (Stack Overflow Question-Code pairs), the largest dataset to date of ∼148K Python and ∼120K SQL question-code pairs, automatically mined from SO using our framework. Under various case studies, we demonstrate that StaQC can greatly help develop data-hungry models for associating natural language with programming language.</p>
</td>
	<td>dataset </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/markovtsev2018public/">Public Git Archive: a Big Code dataset for all</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Public Git Archive: a Big Code dataset for all' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Public Git Archive: a Big Code dataset for all' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Public%20Git%20Archive:%20a%20Big%20Code%20dataset%20for%20all' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>V. Markovtsev, W. Long</td>
	<td>MSR</td>
	<td><p>The number of open source software projects has been growing exponentially. The major online software repository host, GitHub, has accumulated tens of millions of publicly available Git version-controlled repositories. Although the research potential enabled by the available open source code is clearly substantial, no significant large-scale open source code datasets exist. In this paper, we present the Public Git Archive – dataset of 182,014 top-bookmarked Git repositories from GitHub. We describe the novel data retrieval pipeline to reproduce it. We also elaborate on the strategy for performing dataset updates and legal issues. The Public Git Archive occupies 3.0 TB on disk and is an order of magnitude larger than the current source code datasets. The dataset is made available through HTTP and provides the source code of the projects, the related metadata, and development history. The data retrieval pipeline employs an optimized worker queue model and an optimized archive format to efficiently store forked Git repositories, reducing the amount of data to download and persist. Public Git Archive aims to open a myriad of new opportunities for Big Code research.</p>
</td>
	<td>dataset </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/alon2018general/">A General Path-Based Representation for Predicting Program Properties</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A General Path-Based Representation for Predicting Program Properties' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A General Path-Based Representation for Predicting Program Properties' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=A%20General%20Path-Based%20Representation%20for%20Predicting%20Program%20Properties' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>U. Alon, M. Zilberstein, O. Levy, E. Yahav</td>
	<td>PLDI</td>
	<td><p>Predicting program properties such as names or expression types has a wide range of applications. It can ease the task of programming and increase programmer productivity. A major challenge when learning from programs is how to represent programs in a way that facilitates effective learning. 
We present a general path-based representation for learning from programs. Our representation is purely syntactic and extracted automatically. The main idea is to represent a program using paths in its abstract syntax tree (AST). This allows a learning model to leverage the structured nature of code rather than treating it as a flat sequence of tokens. 
We show that this representation is general and can: (i) cover different prediction tasks, (ii) drive different learning algorithms (for both generative and discriminative models), and (iii) work across different programming languages. 
We evaluate our approach on the tasks of predicting variable names, method names, and full types. We use our representation to drive both CRF-based and word2vec-based learning, for programs of four languages: JavaScript, Java, Python and C#. Our evaluation shows that our approach obtains better results than task-specific handcrafted representations across different tasks and programming languages.</p>
</td>
	<td>naming representation </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/yin2018mining/">Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20to%20Mine%20Aligned%20Code%20and%20Natural%20Language%20Pairs%20from%20Stack%20Overflow' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>P. Yin, B. Deng, E. Chen, B. Vasilescu, G. Neubig</td>
	<td>MSR</td>
	<td><p>For tasks like code synthesis from natural language, code retrieval, and code summarization, data-driven models have shown great promise. However, creating these models require parallel data between natural language (NL) and code with fine-grained alignments. Stack Overflow (SO) is a promising source to create such a data set: the questions are diverse and most of them have corresponding answers with high-quality code snippets. However, existing heuristic methods (e.g., pairing the title of a post with the code in the accepted answer) are limited both in their coverage and the correctness of the NL-code pairs obtained. In this paper, we propose a novel method to mine high-quality aligned data from SO using two sets of features: hand-crafted features considering the structure of the extracted snippets, and correspondence features obtained by training a probabilistic model to capture the correlation between NL and code using neural networks. These features are fed into a classifier that determines the quality of mined NL-code pairs. Experiments using Python and Java as test beds show that the proposed method greatly expands coverage and accuracy over existing mining methods, even when using only a small number of labeled examples. Further, we find that reasonable results are achieved even when training the classifier on one language and testing on another, showing promise for scaling NL-code mining to a wide variety of programming languages beyond those for which we are able to annotate data.</p>

</td>
	<td>dataset </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/louis2018deep/">Deep Learning to Detect Redundant Method Comments</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Learning to Detect Redundant Method Comments' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Learning to Detect Redundant Method Comments' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Learning%20to%20Detect%20Redundant%20Method%20Comments' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>A. Louis, S. K. Dash, E. T. Barr, C. Sutton</td>
	<td></td>
	<td><p>Comments in software are critical for maintenance and reuse. But apart from prescriptive advice, there is little practical support or quantitative understanding of what makes a comment useful. In this paper, we introduce the task of identifying comments which are uninformative about the code they are meant to document. To address this problem, we introduce the notion of comment entailment from code, high entailment indicating that a comment’s natural language semantics can be inferred directly from the code. Although not all entailed comments are low quality, comments that are too easily inferred, for example, comments that restate the code, are widely discouraged by authorities on software style. Based on this, we develop a tool called CRAIC which scores method-level comments for redundancy. Highly redundant comments can then be expanded or alternately removed by the developer. CRAIC uses deep language models to exploit large software corpora without requiring expensive manual annotations of entailment. We show that CRAIC can perform the comment entailment task with good agreement with human judgements. Our findings also have implications for documentation tools. For example, we find that common tags in Javadoc are at least two times more predictable from code than non-Javadoc sentences, suggesting that Javadoc tags are less informative than more free-form comments</p>
</td>
	<td>bimodal documentation </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/allamanis2018learning/">Learning to Represent Programs with Graphs</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning to Represent Programs with Graphs' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning to Represent Programs with Graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20to%20Represent%20Programs%20with%20Graphs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>M. Allamanis, M. Brockscmidt, M. Khademi</td>
	<td>ICLR</td>
	<td><p>Learning tasks on source code (i.e., formal languages) have been considered recently, but most work has tried to transfer natural language methods and does not capitalize on the unique opportunities offered by code’s known syntax. For example, long-range dependencies induced by using the same variable or function in distant locations are often not considered. We propose to use graphs to represent both the syntactic and semantic structure of code and use graph-based deep learning methods to learn to reason over program structures.</p>

<p>In this work, we present how to construct graphs from source code and how to scale Gated Graph Neural Networks training to such large graphs. We evaluate our method on two tasks: VarNaming, in which a network attempts to predict the name of a variable given its usage, and VarMisuse, in which the network learns to reason about selecting the correct variable that should be used at a given program location. Our comparison to methods that use less structured program representations shows the advantages of modeling known structure, and suggests that our models learn to infer meaningful names and to solve the VarMisuse task in many cases. Additionally, our testing showed that VarMisuse identifies a number of bugs in mature open-source projects.</p>
</td>
	<td>naming GNN representation variable misuse defect </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/zhao2018neural/">Neural-Augumented Static Analysis of Android Communication</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Neural-Augumented Static Analysis of Android Communication' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Neural-Augumented Static Analysis of Android Communication' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Neural-Augumented%20Static%20Analysis%20of%20Android%20Communication' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>J. Zhao, A. Albarghouthi, V. Rastogi, S. Jha, D. Octeau</td>
	<td>FSE</td>
	<td><p>We address the problem of discovering communication links between applications in the popular Android mobile operating system, an important problem for security and privacy in Android. Any scalable static analysis in this complex setting is bound to produce an excessive amount of false-positives, rendering it impractical. To improve precision, we propose to augment static analysis with a trained neural-network model that estimates the probability that a communication link truly exists. We describe a neural-network architecture that encodes abstractions of communicating objects in two applications and estimates the probability with which a link indeed exists. At the heart of our architecture are type-directed encoders (TDE), a general framework for elegantly constructing encoders of a compound data type by recursively composing encoders for its constituent types. We evaluate our approach on a large corpus of Android applications, and demonstrate that it achieves very high accuracy. Further, we conduct thorough interpretability studies to understand the internals of the learned neural networks.</p>
</td>
	<td>program analysis </td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/loyola2018content/">Content Aware Source Code Change Description Generation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Content Aware Source Code Change Description Generation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Content Aware Source Code Change Description Generation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Content%20Aware%20Source%20Code%20Change%20Description%20Generation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>P. Loyola, E. Marrese-Taylor, J.A. Balazs, Y. Matsuo, F. Satoh</td>
	<td>International Natural Language Generation Conference</td>
	<td><p>We propose to study the generation of descriptions from source code changes by integrating  the  messages  included  on  code
commits and the intra-code documentation
inside the source in the form of docstrings.
Our hypothesis is that although both types
of descriptions are not directly aligned in
semantic terms —one explaining a change
and  the  other  the  actual  functionality  of
the code being modified— there could be
certain common ground that is useful for
the  generation.   To  this  end,  we  propose
an architecture that uses the source code-docstring relationship to guide the description generation.  We discuss the results of
the approach comparing against a baseline
based  on  a  sequence-to-sequence  model,
using standard automatic natural language
generation metrics as well as with a human
study, thus offering a comprehensive view
of the feasibility of the approach.</p>
</td>
	<td>edit summarization </td>
</tr>



<tr>
	<td>2017</td>
	<td><a href="/publications/fowkes2017autofolding/">Autofolding for Source Code Summarization</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Autofolding for Source Code Summarization' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Autofolding for Source Code Summarization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Autofolding%20for%20Source%20Code%20Summarization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>J. Fowkes, R. Ranca, M. Allamanis, M. Lapata, C. Sutton</td>
	<td>TSE</td>
	<td><p>Developers spend much of their time reading and browsing source code, raising new opportunities for summarization methods. Indeed, modern code editors provide code folding, which allows one to selectively hide blocks of code. However this is impractical to use as folding decisions must be made manually or based on simple rules. We introduce the
autofolding problem, which is to automatically create a code summary by folding less informative code regions. We present a novel solution by formulating the problem as a sequence of AST folding decisions, leveraging a scoped topic model for code tokens. On an annotated set of popular open source projects, we show that our summarizer outperforms simpler baselines, yielding a 28% error reduction. Furthermore, we find through a case study that our summarizer is strongly preferred by experienced developers. More broadly, we hope this work will aid program comprehension by turning code folding into a usable and valuable tool.</p>
</td>
	<td>summarization </td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/richardson2017function/">Function Assistant: A Tool for NL Querying of APIs</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Function Assistant: A Tool for NL Querying of APIs' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Function Assistant: A Tool for NL Querying of APIs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Function%20Assistant:%20A%20Tool%20for%20NL%20Querying%20of%20APIs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Kyle Richardson, Jonas Kuhn</td>
	<td>EMNLP</td>
	<td><p>In this paper, we describe Function Assistant, a lightweight Python-based toolkit for querying and exploring source code repositories using natural language. The toolkit is designed to help end-users of a target API quickly find information about functions through high-level natural language queries and descriptions. For a given text query and background API, the tool finds candidate functions by performing a translation from the text to known representations in the API using the semantic parsing approach of Richardson and Kuhn (2017). Translations are automatically learned from example text-code pairs in example APIs. The toolkit includes features for building translation pipelines and query engines for arbitrary source code projects. To explore this last feature, we perform new experiments on 27 well-known Python projects hosted on Github.</p>
</td>
	<td>bimodal API </td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/richardson2017code2text/">The Code2Text Challenge: Text Generation in Source Code Libraries</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=The Code2Text Challenge: Text Generation in Source Code Libraries' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=The Code2Text Challenge: Text Generation in Source Code Libraries' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=The%20Code2Text%20Challenge:%20Text%20Generation%20in%20Source%20Code%20Libraries' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Kyle Richardson, Sina Zarrieß, Jonas Kuhn</td>
	<td>INLG</td>
	<td><p>We propose a new shared task for tactical data-to-text generation in the domain of source code libraries. Specifically, we focus on text generation of function descriptions from example software projects. Data is drawn from existing resources used for studying the related problem of semantic parser induction (Richardson and Kuhn, 2017b; Richardson and Kuhn, 2017a), and spans a wide variety of both natural languages and programming languages. In this paper, we describe these existing resources, which will serve as training and development data for the task, and discuss plans for building new independent test sets.</p>
</td>
	<td>bimodal </td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/richardson2017learning/">Learning Technical Correspondences in Technical Documentation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning Technical Correspondences in Technical Documentation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning Technical Correspondences in Technical Documentation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20Technical%20Correspondences%20in%20Technical%20Documentation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Kyle Richardson, Jonas Kuhn</td>
	<td>ACL</td>
	<td><p>We consider the problem of translating high-level textual descriptions to formal representations in technical documentation as part of an effort to model the meaning of such documentation. We focus specifically on the problem of learning translational correspondences between text descriptions and grounded representations in the target documentation, such as formal representation of functions or code templates. Our approach exploits the parallel nature of such documentation, or the tight coupling between high-level text and the low-level representations we aim to learn. Data is collected by mining technical documents for such parallel text-representation pairs, which we use to train a simple semantic parsing model. We report new baseline results on sixteen novel datasets, including the standard library documentation for nine popular programming languages across seven natural languages, and a small collection of Unix utility manuals.</p>
</td>
	<td>documentation API bimodal </td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/devlin2017semantic/">Semantic Code Repair using Neuro-Symbolic Transformation Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Semantic Code Repair using Neuro-Symbolic Transformation Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Semantic Code Repair using Neuro-Symbolic Transformation Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Semantic%20Code%20Repair%20using%20Neuro-Symbolic%20Transformation%20Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>J. Devlin, J. Uesato, R. Shingh, P. Kohli</td>
	<td></td>
	<td><p>We study the problem of semantic code repair, which can be broadly defined as automatically fixing
non-syntactic bugs in source code. The majority of past work in semantic code repair assumed access
to unit tests against which candidate repairs could be validated. In contrast, the goal here is to
develop a strong statistical model to accurately predict both bug locations and exact fixes without
access to information about the intended correct behavior of the program. Achieving such a goal
requires a robust contextual repair model, which we train on a large corpus of real-world source
code that has been augmented with synthetically injected bugs. Our framework adopts a two-stage
approach where first a large set of repair candidates are generated by rule-based processors, and
then these candidates are scored by a statistical model using a novel neural network architecture
which we refer to as Share, Specialize, and Compete. Specifically, the architecture (1) generates
a shared encoding of the source code using an RNN over the abstract syntax tree, 
(2) scores each candidate repair using specialized network modules, and (3) then normalizes these
scores together so they can compete against one another in comparable probability space. We evaluate
our model on a real-world test set gathered from GitHub containing four common categories of bugs.
Our model is able to predict the exact correct repair 41% of the time with a single guess, compared
to 13% accuracy for an attentional sequence-to-sequence model.</p>
</td>
	<td>repair </td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/murali2017finding/">Finding Likely Errors with Bayesian Specifications</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Finding Likely Errors with Bayesian Specifications' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Finding Likely Errors with Bayesian Specifications' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Finding%20Likely%20Errors%20with%20Bayesian%20Specifications' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>V. Murali, S. Chaudhuri, C. Jermaine</td>
	<td>arXiv preprint 1703.01370</td>
	<td><p>We present a Bayesian framework for learning probabilistic specifications from large, unstructured code corpora, and
a method to use this framework to statically detect anomalous, hence likely buggy, program behavior. The distinctive
insight here is to build a statistical model that correlates all
specifications hidden inside a corpus with the syntax and
observed behavior of programs that implement these specifications. During the analysis of a particular program, this
model is conditioned into a posterior distribution that prioritizes specifications that are relevant to this program. This
allows accurate program analysis even if the corpus is highly
heterogeneous. The problem of finding anomalies is now
framed quantitatively, as a problem of computing a distance
between a “reference distribution” over program behaviors
that our model expects from the program, and the distribution over behaviors that the program actually produces.</p>

<p>We present a concrete embodiment of our framework that
combines a topic model and a neural network model to learn
specifications, and queries the learned models to compute
anomaly scores. We evaluate this implementation on the
task of detecting anomalous usage of Android APIs. Our
encouraging experimental results show that the method can
automatically discover subtle errors in Android applications
in the wild, and has high precision and recall compared to
competing probabilistic approaches.</p>
</td>
	<td>program analysis API </td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/cummins2017synthesizing/">Synthesizing benchmarks for predictive modeling</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Synthesizing benchmarks for predictive modeling' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Synthesizing benchmarks for predictive modeling' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Synthesizing%20benchmarks%20for%20predictive%20modeling' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>C. Cummin, P. Petoumenos, Z. Wang, H. Leather</td>
	<td>CGO</td>
	<td><p>Predictive modeling using machine learning is an effective method for building compiler heuristics, but there is a shortage of benchmarks. Typical machine learning experiments outside of the compilation field train over thousands or millions of examples. In machine learning for compilers, however, there are typically only a few dozen common benchmarks available. This limits the quality of learned models, as they have very sparse training data for what are often high-dimensional feature spaces. What is needed is a way to generate an unbounded number of training programs that finely cover the feature space. At the same time the generated programs must be similar to the types of programs that human developers actually write, otherwise the learning will target the wrong parts of the feature space. We mine open source repositories for program fragments and apply deep learning techniques to automatically construct models for how humans write programs. We sample these models to generate an unbounded number of runnable training programs. The quality of the programs is such that even human developers struggle to distinguish our generated programs from hand-written code. We use our generator for OpenCL programs, CLgen, to automatically synthesize thousands of programs and show that learning over these improves the performance of a state of the art predictive model by 1.27x. In addition, the fine covering of the feature space automatically exposes weaknesses in the feature design which are invisible with the sparse training examples from existing benchmark suites. Correcting these weaknesses further increases performance by 4.30x.</p>
</td>
	<td>optimization generation </td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/gu2017deepam/">DeepAM: Migrate APIs with Multi-modal Sequence to Sequence Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=DeepAM: Migrate APIs with Multi-modal Sequence to Sequence Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=DeepAM: Migrate APIs with Multi-modal Sequence to Sequence Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=DeepAM:%20Migrate%20APIs%20with%20Multi-modal%20Sequence%20to%20Sequence%20Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>X. Gu, H. Zhang, D. Zhang, S. Kim</td>
	<td>IJCAI</td>
	<td><p>Computer programs written in one language are often required to be ported to other languages to support multiple devices and environments. When programs use language specific APIs (Application Programming Interfaces), it is very challenging to migrate these APIs to the corresponding APIs written in other languages. Existing approaches mine API mappings from projects that have corresponding versions in two languages. They rely on the sparse availability of bilingual projects, thus producing a limited number of API mappings. In this paper, we propose an intelligent system called DeepAM for automatically mining API mappings from a large-scale code corpus without bilingual projects. The key component of DeepAM is based on the multimodal sequence to sequence learning architecture that aims to learn joint semantic representations of bilingual API sequences from big source code data. Experimental results indicate that DeepAM significantly increases the accuracy of API mappings as well as the number of API mappings, when compared with the state-of-the-art approaches.</p>
</td>
	<td>API </td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/cummins2017end/">End-to-end Deep Learning of Optimization Heuristics</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=End-to-end Deep Learning of Optimization Heuristics' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=End-to-end Deep Learning of Optimization Heuristics' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=End-to-end%20Deep%20Learning%20of%20Optimization%20Heuristics' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>C. Cummins, P. Petoumenos, Z. Wang, H. Leather</td>
	<td></td>
	<td><p>Accurate automatic optimization heuristics are necessary for dealing with the complexity and diversity of modern hardware and software. Machine learning is a proven technique for learning such heuristics, but its success is bound by the quality of the features used. These features must be hand crafted by developers through a combination of expert domain knowledge and trial and error. This makes the quality of the final model directly dependent on the skill and available time of the system architect.</p>

<p>Our work introduces a better way for building heuristics. We develop a deep neural network that learns heuristics over raw code, entirely without using code features. The neural network simultaneously constructs appropriate representations of the code and learns how best to optimize, removing the need for manual feature creation. Further, we show that our neural nets can transfer learning from one optimization problem to another, improving the accuracy of new models, without the help of human experts.</p>

<p>We compare the effectiveness of our automatically generated heuristics against ones with features hand-picked by experts. We examine two challenging tasks: predicting optimal mapping for heterogeneous parallelism and GPU thread coarsening factors. In 89% of the cases, the quality of our fully automatic heuristics matches or surpasses that of state-of-the-art predictive models using hand-crafted features, providing on average 14% and 12% more performance with no human effort expended on designing features.</p>
</td>
	<td>optimization </td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/levy2017learning/">Learning to Align the Source Code to the Compiled Object Code</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning to Align the Source Code to the Compiled Object Code' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning to Align the Source Code to the Compiled Object Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20to%20Align%20the%20Source%20Code%20to%20the%20Compiled%20Object%20Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>D. Levy, L. Wolf</td>
	<td>ICML</td>
	<td><p>We propose a new neural network architecture
and use it for the task of statement-by-statement
alignment of source code and its compiled object code. Our architecture learns the alignment
between the two sequences – one being the translation of the other – by mapping each statement
to a context-dependent representation vector and
aligning such vectors using a grid of the two sequence domains. Our experiments include short
C functions, both artificial and human-written,
and show that our neural network architecture
is able to predict the alignment with high accuracy, outperforming known baselines. We also
demonstrate that our model is general and can
learn to solve graph problems such as the Traveling Salesman Problem.</p>
</td>
	<td>decompilation </td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/guo2017semantically/">Semantically enhanced software traceability using deep learning techniques</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Semantically enhanced software traceability using deep learning techniques' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Semantically enhanced software traceability using deep learning techniques' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Semantically%20enhanced%20software%20traceability%20using%20deep%20learning%20techniques' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>J. Guo, J. Cheng, J. Cleland-Huang</td>
	<td>ICSE</td>
	<td><p>In most safety-critical domains the need for traceability is prescribed by certifying bodies. Trace links are generally created among requirements, design, source code, test cases and other artifacts; however, creating such links manually is time consuming and error prone. Automated solutions use information retrieval and machine learning techniques to generate trace links; however, current techniques fail to understand semantics of the software artifacts or to integrate domain knowledge into the tracing process and therefore tend to deliver imprecise and inaccurate results. In this paper, we present a solution that uses deep learning to incorporate requirements artifact semantics and domain knowledge into the tracing solution. We propose a tracing network architecture that utilizes Word Embedding and Recurrent Neural Network (RNN) models to generate trace links. Word embedding learns word vectors that represent knowledge of the domain corpus and RNN uses these word vectors to learn the sentence semantics of requirements artifacts. We trained 360 different configurations of the tracing network using existing trace links in the Positive Train Control domain and identified the Bidirectional Gated Recurrent Unit (BI-GRU) as the best model for the tracing task. BI-GRU significantly out-performed state-of-the-art tracing methods including the Vector Space Model and Latent Semantic Indexing.</p>
</td>
	<td>traceability representation </td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/gupta2017deepfix/">DeepFix: Fixing Common C Language Errors by Deep Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=DeepFix: Fixing Common C Language Errors by Deep Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=DeepFix: Fixing Common C Language Errors by Deep Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=DeepFix:%20Fixing%20Common%20C%20Language%20Errors%20by%20Deep%20Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>R. Gupta, S. Pal, A. Kanade, S. Shevade</td>
	<td>AAAI</td>
	<td><p>The problem of automatically fixing programming errors is a
very active research topic in software engineering. This is a
challenging problem as fixing even a single error may require
analysis of the entire program. In practice, a number of errors
arise due to programmer’s inexperience with the programming language or lack of attention to detail. We call these
common programming errors. These are analogous to grammatical errors in natural languages. Compilers detect such errors, but their error messages are usually inaccurate. In this
work, we present an end-to-end solution, called DeepFix, that
can fix multiple such errors in a program without relying on
any external tool to locate or fix them. At the heart of DeepFix
is a multi-layered sequence-to-sequence neural network with
attention which is trained to predict erroneous program locations along with the required correct statements. On a set of
6971 erroneous C programs written by students for 93 programming tasks, DeepFix could fix 1881 (27%) programs
completely and 1338 (19%) programs partially.</p>
</td>
	<td>repair generation </td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/koc2017learning/">Learning a Classifier for False Positive Error Reports Emitted by Static Code Analysis Tools</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning a Classifier for False Positive Error Reports Emitted by Static Code Analysis Tools' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning a Classifier for False Positive Error Reports Emitted by Static Code Analysis Tools' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20a%20Classifier%20for%20False%20Positive%20Error%20Reports%20Emitted%20by%20Static%20Code%20Analysis%20Tools' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>U. Koc, P. Saadatpanah, J. S. Foster, A. A. Porter</td>
	<td>MAPL</td>
	<td><p>The large scale and high complexity of modern software systems
make perfectly precise static code analysis (SCA) infeasible. Therefore SCA tools often over-approximate, so not to miss any real
problems. This, however, comes at the expense of raising false
alarms, which, in practice, reduces the usability of these tools.</p>

<p>To partially address this problem, we propose a novel learning
process whose goal is to discover program structures that cause
a given SCA tool to emit false error reports, and then to use this
information to predict whether a new error report is likely to be a
false positive as well. To do this, we first preprocess code to isolate
the locations that are related to the error report. Then, we apply
machine learning techniques to the preprocessed code to discover
correlations and to learn a classifier.</p>

<p>We evaluated this approach in an initial case study of a widely-used SCA tool for Java. Our results showed that for our dataset
we could accurately classify a large majority of false positive error
reports. Moreover, we identified some common coding patterns that
led to false positive errors. We believe that SCA developers may be
able to redesign their methods to address these patterns and reduce
false positive error reports.</p>
</td>
	<td>static analysis </td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/li2017code/">Code Completion with Neural Attention and Pointer Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Code Completion with Neural Attention and Pointer Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Code Completion with Neural Attention and Pointer Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Code%20Completion%20with%20Neural%20Attention%20and%20Pointer%20Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>J. Li, Y. Wang, I. King, M. R. Lyu</td>
	<td></td>
	<td><p>Intelligent code completion has become an essential tool to accelerate modern software development. To facilitate effective code completion for dynamically-typed programming languages, we apply neural language models by learning from large codebases, and investigate the effectiveness of attention mechanism on the code completion task. However, standard neural language models even with attention mechanism cannot correctly predict out-of-vocabulary (OoV) words thus restrict the code completion performance. In this paper, inspired by the prevalence of locally repeated terms in program source code, and the recently proposed pointer networks which can reproduce words from local context, we propose a pointer mixture network for better predicting OoV words in code completion. Based on the context, the pointer mixture network learns to either generate a within-vocabulary word through an RNN component, or copy an OoV word from local context through a pointer component. Experiments on two benchmarked datasets demonstrate the effectiveness of our attention mechanism and pointer mixture network on the code completion task.</p>

</td>
	<td>language model autocomplete </td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/li2017software/">Software Defect Prediction via Convolutional Neural Network</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Software Defect Prediction via Convolutional Neural Network' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Software Defect Prediction via Convolutional Neural Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Software%20Defect%20Prediction%20via%20Convolutional%20Neural%20Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>J. Li, P. He, J. Zhu, and M. R. Lyu</td>
	<td>QRS</td>
	<td><p>To improve software reliability, software defect prediction is utilized to assist developers in finding potential bugs
and allocating their testing efforts. Traditional defect prediction
studies mainly focus on designing hand-crafted features, which
are input into machine learning classifiers to identify defective
code. However, these hand-crafted features often fail to capture
the semantic and structural information of programs. Such
information is important in modeling program functionality and
can lead to more accurate defect prediction.
In this paper, we propose a framework called Defect Prediction
via Convolutional Neural Network (DP-CNN), which leverages
deep learning for effective feature generation. Specifically, based
on the programs’ Abstract Syntax Trees (ASTs), we first extract
token vectors, which are then encoded as numerical vectors
via mapping and word embedding. We feed the numerical
vectors into Convolutional Neural Network to automatically
learn semantic and structural features of programs. After that,
we combine the learned features with traditional hand-crafted
features, for accurate software defect prediction. We evaluate our
method on seven open source projects in terms of F-measure in
defect prediction. The experimental results show that in average,
DP-CNN improves the state-of-the-art method by 12%.</p>

</td>
	<td>defect </td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/murali2017bayesian/">Bayesian Sketch Learning for Program Synthesis</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Bayesian Sketch Learning for Program Synthesis' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Bayesian Sketch Learning for Program Synthesis' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Bayesian%20Sketch%20Learning%20for%20Program%20Synthesis' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>V. Murali, S. Chaudhuri, C. Jermaine</td>
	<td>arXiv preprint 1703.05698</td>
	<td><p>We present a Bayesian statistical approach to the problem of automatic program synthesis. Our synthesizer starts
by learning, offline and from an existing corpus, a probabilistic model of real-world programs. During synthesis,
it is provided some ambiguous and incomplete evidence about the nature of the programming task that the user
wants automated, for example sets of API calls or data types that are relevant for the task. Given this input, the
synthesizer infers a posterior distribution over type-safe programs that assigns higher likelihood to programs
that, according to the learned model, are more likely to match the evidence.</p>

<p>We realize this approach using two key ideas. First, our learning techniques operate not over code but
syntactic abstractions, or sketches, of programs. During synthesis, we infer a posterior distribution over sketches,
then concretize samples from this distribution into type-safe programs using combinatorial techniques. Second,
our statistical model explicitly models the full intent behind a synthesis task as a latent variable. To infer
sketches, we first estimate a posterior distribution on the intent, then use samples from this posterior to generate
a distribution over possible sketches. We show that our model can be implemented effectively using the new
neural architecture of Bayesian encoder-decoders, which can be trained with stochastic gradient descent and
yields a simple inference procedure.</p>

<p>We implement our ideas in a system, called BAYOU , for the synthesis of API-heavy Java methods. We train
BAYOU on a large corpus of Android apps, and find that the trained system can often synthesize complex
methods given just a few API method names or data types as evidence. The experiments also justify the design
choice of using a latent intent variable and the levels of abstraction at which sketches and evidence are defined.</p>
</td>
	<td>generation API </td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/vasilescu2017recovering/">Recovering Clear, Natural Identifiers from Obfuscated JS Names</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Recovering Clear, Natural Identifiers from Obfuscated JS Names' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Recovering Clear, Natural Identifiers from Obfuscated JS Names' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Recovering%20Clear,%20Natural%20Identifiers%20from%20Obfuscated%20JS%20Names' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>B. Vasilescu, C. Casalnuovo, P. Devanbu</td>
	<td>FSE</td>
	<td><p>Well-chosen variable names are critical to source code readability, reusability, and maintainability. Unfortunately, in deployed JavaScript code (which is ubiquitous on the web) the identifier names are frequently minified and overloaded. This is done both for efficiency and also to protect potentially proprietary intellectual property. In this paper, we describe an approach based on statistical machine translation (SMT) that recovers some of the original names from the JavaScript programs minified by the very popular UglifyJS. This simple tool, Autonym, performs comparably to the best currently available deobfuscator for JavaScript, JSNice, which uses sophisticated static analysis. In fact, Autonym is quite complementary to JSNice, performing well when it does not, and vice versa. We also introduce a new tool, JSNaughty, which blends Autonym and JSNice, and significantly outperforms both at identifier name recovery, while remaining just as easy to use as JSNice. JSNaughty is available online at http://jsnaughty.org.</p>
</td>
	<td>deobfuscation naming </td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/nguyen2017exploring/">Exploring API Embedding for API Usages and Applications</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Exploring API Embedding for API Usages and Applications' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Exploring API Embedding for API Usages and Applications' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Exploring%20API%20Embedding%20for%20API%20Usages%20and%20Applications' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>T.D. Nguyen, A.T. Nguyen, H.D. Phan, T.N. Nguyen</td>
	<td>ICSE</td>
	<td><p>Word2Vec is a class of neural network models that
as being trained from a large corpus of texts, they can produce for
each unique word a corresponding vector in a continuous space in
which linguistic contexts of words can be observed. In this work,
we study the characteristics of Word2Vec vectors, called API 2 VEC
or API embeddings, for the API elements within the API sequences in source code. Our empirical study shows that the close
proximity of the API 2 VEC vectors for API elements reflects the
similar usage contexts containing the surrounding APIs of those
API elements. Moreover, API 2 VEC can capture several similar
semantic relations between API elements in API usages via vector
offsets. We demonstrate the usefulness of API 2 VEC vectors for
API elements in three applications. First, we build a tool that mines the pairs of API elements that share the same usage relations
among them. The other applications are in the code migration
domain. We develop API 2 API , a tool to automatically learn the
API mappings between Java and C# using a characteristic of the
API 2 VEC vectors for API elements in the two languages: semantic
relations among API elements in their usages are observed in the
two vector spaces for the two languages as similar geometric
arrangements among their API 2 VEC vectors. Our empirical
evaluation shows that API 2 API relatively improves 22.6% and
40.1% top-1 and top-5 accuracy over a state-of-the-art mining
approach for API mappings. Finally, as another application in
code migration, we are able to migrate equivalent API usages
from Java to C# with up to 90.6% recall and 87.2% precision.</p>
</td>
	<td>API representation </td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/lin2017program/">Program Synthesis from Natural Language Using Recurrent Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Program Synthesis from Natural Language Using Recurrent Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Program Synthesis from Natural Language Using Recurrent Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Program%20Synthesis%20from%20Natural%20Language%20Using%20Recurrent%20Neural%20Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>X.V. Lin, C. Wang, D. Pang, K. Vu, L. Zettlemoyer, M.D. Ernst</td>
	<td>Technical Report UW-CSE-17-03-01, University of Washington Department of Computer Science and Engineering</td>
	<td><p>Oftentimes, a programmer may have difficulty implementing a
desired operation. Even when the programmer can describe her
goal in English, it can be difficult to translate into code. Existing
resources, such as question-and-answer websites, tabulate specific
operations that someone has wanted to perform in the past, but
they are not effective in generalizing to new tasks, to compound
tasks that require combining previous questions, or sometimes even
to variations of listed tasks.</p>

<p>Our goal is to make programming easier and more productive by
letting programmers use their own words and concepts to express
the intended operation, rather than forcing them to accommodate
the machine by memorizing its grammar. We have built a system
that lets a programmer describe a desired operation in natural language, then automatically translates it to a programming language
for review and approval by the programmer. Our system, Tellina,
does the translation using recurrent neural networks (RNNs), a
state-of-the-art natural language processing technique that we augmented with slot (argument) filling and other enhancements.</p>

<p>We evaluated Tellina in the context of shell scripting. We trained
Tellina’s RNNs on textual descriptions of file system operations
and bash one-liners, scraped from the web. Although recovering
completely correct commands is challenging, Tellina achieves top-3
accuracy of 80% for producing the correct command structure. In a
controlled study, programmers who had access to Tellina outperformed those who did not, even when Tellina’s predictions were
not completely correct, to a statistically significant degree.</p>
</td>
	<td>bimodal generation </td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/beltramelli2017pix2code/">pix2code: Generating Code from a Graphical User Interface Screenshot</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=pix2code: Generating Code from a Graphical User Interface Screenshot' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=pix2code: Generating Code from a Graphical User Interface Screenshot' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=pix2code:%20Generating%20Code%20from%20a%20Graphical%20User%20Interface%20Screenshot' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>T. Beltramelli</td>
	<td>ArXiV 1705.07962</td>
	<td><p>Transforming a graphical user interface screenshot created by a designer into computer code is a typical task conducted by a developer in order to build customized software, websites and mobile applications. In this paper, we show that Deep Learning techniques can be leveraged to automatically generate code given a graphical user interface screenshot as input. Our model is able to generate code targeting three different platforms (i.e. iOS, Android and web-based technologies) from a single input image with over 77% of accuracy.</p>

</td>
	<td>generation bimodal </td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/bavishi2017context2name/">Context2Name: A Deep Learning-Based Approach to Infer Natural Variable Names from Usage Contexts</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Context2Name: A Deep Learning-Based Approach to Infer Natural Variable Names from Usage Contexts' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Context2Name: A Deep Learning-Based Approach to Infer Natural Variable Names from Usage Contexts' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Context2Name:%20A%20Deep%20Learning-Based%20Approach%20to%20Infer%20Natural%20Variable%20Names%20from%20Usage%20Contexts' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>R. Bavishi, M. Pradel, K. Sen</td>
	<td></td>
	<td><p>Most of the JavaScript code deployed in the wild has been minified, a process in which identifier names are replaced
with short, arbitrary and meaningless names. Minified code occupies less space, but also makes the code extremely difficult to manually inspect and understand. This paper presents Context2Name, a deep learning-based technique that partially reverses the effect of minification by predicting natural
identifier names for minified names. The core idea is to predict from the usage context of a variable a name that captures
the meaning of the variable. The approach combines a lightweight, token-based static analysis with an auto-encoder
neural network that summarizes usage contexts and a recurrent neural network that predict natural names for a given
usage context. We evaluate Context2Name
with a large corpus of real-world JavaScript code and show that it successfully predicts 60.4% of all minified identifiers. A comparison
with the state-of-the-art tools JSNice and JSNaughty shows
that our approach predicts 17% and 43% more names than the
best existing approaches, while taking only 2.6 milliseconds
to predict a name, on average.</p>
</td>
	<td>naming </td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/rabinovich2017abstract/">Abstract Syntax Networks for Code Generation and Semantic Parsing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Abstract Syntax Networks for Code Generation and Semantic Parsing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Abstract Syntax Networks for Code Generation and Semantic Parsing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Abstract%20Syntax%20Networks%20for%20Code%20Generation%20and%20Semantic%20Parsing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>M. Rabinovich, M. Stern, D. Klein</td>
	<td>ACL</td>
	<td><p>Tasks like code generation and semantic parsing require mapping unstructured (or partially structured) inputs to well-formed, executable outputs. We introduce abstract syntax networks, a modeling framework for these problems. The outputs are represented as abstract syntax trees (ASTs) and constructed by a decoder with a dynamically-determined modular structure paralleling the structure of the output tree. On the benchmark Hearthstone dataset for code generation, our model obtains 79.2 BLEU and 22.7% exact match accuracy, compared to previous state-of-the-art values of 67.1 and 6.1%. Furthermore, we perform competitively on the Atis, Jobs, and Geo semantic parsing datasets with no task-specific engineering.</p>
</td>
	<td>generation AST </td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/barone2017parallel/">A parallel corpus of Python functions and documentation strings for automated code documentation and code generation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A parallel corpus of Python functions and documentation strings for automated code documentation and code generation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A parallel corpus of Python functions and documentation strings for automated code documentation and code generation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=A%20parallel%20corpus%20of%20Python%20functions%20and%20documentation%20strings%20for%20automated%20code%20documentation%20and%20code%20generation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>A.V.M. Barone, R. Sennrich</td>
	<td>ArXiV 1707.02275</td>
	<td><p>Automated documentation of programming source code and automated code generation from natural language are challenging tasks of both practical and scientific interest. Progress in these areas has been limited by the low availability of parallel corpora of code and natural language descriptions, which tend to be small and constrained to specific domains.</p>

<p>In this work we introduce a large and diverse parallel corpus of a hundred thousands Python functions with their documentation strings (“docstrings”) generated by scraping open source repositories on GitHub. We describe baseline results for the code documentation and code generation tasks obtained by neural machine translation. We also experiment with 
data augmentation techniques to further increase the amount of training data.</p>

<p>We release our datasets and processing scripts in order to stimulate research in these areas.</p>

</td>
	<td>documentation summarization dataset </td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/hellendoorn2017deep/">Are Deep Neural Networks the Best Choice for Modeling Source Code?</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Are Deep Neural Networks the Best Choice for Modeling Source Code?' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Are Deep Neural Networks the Best Choice for Modeling Source Code?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Are%20Deep%20Neural%20Networks%20the%20Best%20Choice%20for%20Modeling%20Source%20Code?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>V. J. Hellendoorn, P. Devanbu</td>
	<td>FSE</td>
	<td><p>Current statistical language modeling techniques, including deep-learning based models, have proven to be quite effective for source
code. We argue here that the special properties of source code can
be exploited for further improvements. In this work, we enhance
established language modeling approaches to handle the special
challenges of modeling source code, such as: frequent changes,
larger, changing vocabularies, deeply nested scopes, etc. We present
a fast, nested language modeling toolkit specifically designed for
software, with the ability to add &amp; remove text, and mix &amp; swap out
many models. Specifically, we improve upon prior cache-modeling
work and present a model with a much more expansive, multi-level
notion of locality that we show to be well-suited for modeling
software. We present results on varying corpora in comparison
with traditional N -gram, as well as RNN, and LSTM deep-learning
language models, and release all our source code for public use.
Our evaluations suggest that carefully adapting N-gram models for
source code can yield performance that surpasses even RNN and
LSTM based deep-learning models.</p>
</td>
	<td>language model </td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/white2017sorting/">Sorting and Transforming Program Repair Ingredients via Deep Learning Code Similarities</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Sorting and Transforming Program Repair Ingredients via Deep Learning Code Similarities' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Sorting and Transforming Program Repair Ingredients via Deep Learning Code Similarities' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Sorting%20and%20Transforming%20Program%20Repair%20Ingredients%20via%20Deep%20Learning%20Code%20Similarities' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>M. White, M. Tufano, M. Martínez, M. Monperrus, D. Poshyvanyk</td>
	<td></td>
	<td><p>In  the  field  of  automated  program  repair,  the  redundancy  assumption  claims  large  programs  contain  the  seeds
of  their  own  repair.  However,  most  redundancy-based  program
repair  techniques  do  not  reason  about  the  repair  ingredients—the code that is reused to craft a patch. We aim to reason about
the repair ingredients by using code similarities to prioritize and
transform  statements  in  a  codebase  for  patch  generation.  Our
approach,  DeepRepair,  relies  on  deep  learning  to  reason  about
code  similarities.  Code  fragments  at  well-defined  levels  of  granularity in a codebase can be sorted according to their similarity
to suspicious elements (i.e., code elements that contain suspicious
statements) and statements can be transformed by mapping out-of-scope  identifiers  to  similar  identifiers  in  scope.  We  examined
these new search strategies for patch generation with respect to
effectiveness  from  the  viewpoint  of  a  software  maintainer.  Our
comparative experiments were executed on six open-source Java
projects  including  374  buggy  program  revisions  and  consisted
of  19,949  trials  spanning  2,616  days  of  computation  time.  DeepRepair’s  search  strategy  using  code  similarities  generally  found
compilable  ingredients  faster  than  the  baseline,  jGenProg,  but
this improvement neither yielded test-adequate patches in fewer
attempts (on average) nor found significantly more patches than
the  baseline.  Although  the  patch  counts  were  not  statistically
different,  there  were  notable  differences  between  the  nature  of
DeepRepair  patches  and  baseline  patches.  The  results  demonstrate that our learning-based approach finds patches that cannot
be  found  by  existing  redundancy-based  repair  techniques</p>
</td>
	<td>repair </td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/yang2017language/">A Language Model for Statements of Software Code</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Language Model for Statements of Software Code' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Language Model for Statements of Software Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=A%20Language%20Model%20for%20Statements%20of%20Software%20Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Y. Yang, Y. Jiang, M. Gu, J. Sun, J. Gao, H. Liu</td>
	<td>ASE</td>
	<td><p>Building language models for source code enables a large set of improvements on traditional software engineering tasks. One promising application is automatic code completion. State-of-the-art techniques capture code regularities at token level with lexical information. Such language models are more suitable for predicting short token sequences, but become less effective with respect to long statement level predictions. In this paper, we have proposed PCC to optimize the token level based language modeling. Specifically, PCC introduced an intermediate representation (IR) for source code, which puts tokens into groups using lexeme and variable relative order. In this way, PCC is able to handle long token sequences, i.e., group sequences, to suggest a complete statement with the precise synthesizer. Further more, PCC employed a fuzzy matching technique which combined genetic and longest common sub-sequence algorithms to make the prediction more accurate. We have implemented a code completion plugin for Eclipse and evaluated it on open-source Java projects. The results have demonstrated the potential of PCC in generating precise long statement level predictions. In 30%-60% of the cases, it can correctly suggest the complete statement with only six candidates, and 40%-90% of the cases with ten candidates.</p>
</td>
	<td>language model </td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/amodio2017neural/">Neural Attribute Machines for Program Generation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Neural Attribute Machines for Program Generation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Neural Attribute Machines for Program Generation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Neural%20Attribute%20Machines%20for%20Program%20Generation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>M. Amodio, S. Chaudhuri, T. Reps</td>
	<td></td>
	<td><p>Recurrent neural networks have achieved remarkable success at generating sequences with complex structures, thanks to advances that include richer embeddings of input and cures for vanishing gradients. Trained only on sequences from a known grammar, though, they can still struggle to learn rules and constraints of the grammar. Neural Attribute Machines (NAMs) are equipped with a logical machine that represents the underlying grammar, which is used to teach the constraints to the neural machine by (i) augmenting the input sequence, and (ii) optimizing a custom loss function. Unlike traditional RNNs, NAMs are exposed to the grammar, as well as samples from the language of the grammar. During generation, NAMs make significantly fewer violations of the constraints of the underlying grammar than RNNs trained only on samples from the language of the grammar.</p>

</td>
	<td>grammar generation representation </td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/yin2017syntactic/">A Syntactic Neural Model for General-Purpose Code Generation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Syntactic Neural Model for General-Purpose Code Generation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Syntactic Neural Model for General-Purpose Code Generation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=A%20Syntactic%20Neural%20Model%20for%20General-Purpose%20Code%20Generation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>P. Yin, G. Neubig</td>
	<td>ACL</td>
	<td><p>We consider the problem of parsing natural language descriptions into source code
written in a general-purpose programming
language like Python. Existing data-driven methods treat this problem as a language generation task without considering
the underlying syntax of the target programming language. Informed by previous work in semantic parsing, in this paper we propose a novel neural architecture
powered by a grammar model to explicitly
capture the target syntax as prior knowledge. Experiments find this an effective
way to scale up to generation of complex
programs from natural language descriptions, achieving state-of-the-art results that
well outperform previous code generation
and semantic parsing approaches.</p>
</td>
	<td>generation AST bimodal </td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/markovtsev2017topic/">Topic modeling of public repositories at scale using names in source code</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Topic modeling of public repositories at scale using names in source code' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Topic modeling of public repositories at scale using names in source code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Topic%20modeling%20of%20public%20repositories%20at%20scale%20using%20names%20in%20source%20code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>V. Markovtsev, E. Kant</td>
	<td>ArXiV 1704.00135</td>
	<td><p>Programming languages themselves have a limited number of reserved keywords and character based tokens that
define the language specification. However, programmers have a rich use of natural language within their code
through comments, text literals and naming entities. The programmer defined names that can be found in source
code are a rich source of information to build a high level understanding of the project. The goal of this paper
is to apply topic modeling to names used in over 13.6 million repositories and perceive the inferred topics.
One of the problems in such a study is the occurrence of duplicate repositories not officially marked as forks (obscure forks).
We show how to address it using the same identifiers which are extracted for topic modeling.</p>

<p>We open with a discussion on naming in source code, we then elaborate on our approach to remove exact duplicate
and fuzzy duplicate repositories using Locality Sensitive Hashing on the bag-of-words model and then discuss our work
on topic modeling; and finally present the results from our data analysis together with open-access to the source code,
tools and datasets.</p>
</td>
	<td>topic modeling pattern mining </td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/hu2017codesum/">CodeSum: Translate Program Language to Natural Language</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=CodeSum: Translate Program Language to Natural Language' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=CodeSum: Translate Program Language to Natural Language' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=CodeSum:%20Translate%20Program%20Language%20to%20Natural%20Language' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>X. Hu, Y. Wei, G. Li, Z. Jin</td>
	<td>ArXiV 1708.01837</td>
	<td><p>During software maintenance, programmers spend a lot of time on code comprehension. Reading comments is an effective way for programmers to reduce the reading and navigating time when comprehending source code. Therefore, as a critical task in software engineering, code summarization aims to generate brief natural language descriptions for source code. In this paper, we propose a new code summarization model named CodeSum. CodeSum exploits the attention-based sequence-to-sequence (Seq2Seq) neural network with Structure-based Traversal (SBT) of Abstract Syntax Trees (AST). The AST sequences generated by SBT can better present the structure of ASTs and keep unambiguous. We conduct experiments on three large-scale corpora in different program languages, i.e., Java, C#, and SQL, in which Java corpus is our new proposed industry code extracted from Github. Experimental results show that our method CodeSum outperforms the state-of-the-art significantly.</p>
</td>
	<td>bimodal summarization </td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/pradel2017deep/">Deep Learning to Find Bugs</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Learning to Find Bugs' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Learning to Find Bugs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Learning%20to%20Find%20Bugs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>M. Pradel, K. Sen</td>
	<td></td>
	<td><p>Automated bug detection, e.g., through pattern-based static
analysis, is an increasingly popular technique to find programming errors and other code quality issues. Traditionally,
bug detectors are program analyses that are manually written and carefully tuned by an analysis expert. Unfortunately,
the huge amount of possible bug patterns makes it difficult
to cover more than a small fraction of all bugs. This paper
presents a new approach toward creating bug detectors. The
basic idea is to replace manually writing a program analysis
with training a machine learning model that distinguishes
buggy from non-buggy code. To address the challenge that
effective learning requires both positive and negative train-
ing examples, we use simple code transformations that create likely incorrect code from existing code examples. We
present a general framework, called DeepBugs, that extracts
positive training examples from a code corpus, leverages
simple program transformations to create negative training
examples, trains a model to distinguish these two, and then
uses the trained model for identifying programming mistakes in previously unseen code. As a proof of concept, we
create four bug detectors for JavaScript that find a diverse set
of programming mistakes, e.g., accidentally swapped function arguments, incorrect assignments, and incorrect binary
operations. To find bugs, the trained models use information
that is usually discarded by program analyses, such as identifier names of variables and functions. Applying the approach
to a corpus of 150,000 JavaScript files shows that learned bug
detectors have a high accuracy, are very efficient, and reveal
132 programming mistakes in real-world code.</p>

</td>
	<td>defect program analysis </td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/yuan2017abridging/">Abridging Source Code</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Abridging Source Code' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Abridging Source Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Abridging%20Source%20Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>B. Yuan, V. Murali, C. Jermain</td>
	<td>OOPSLA</td>
	<td><p>In this paper, we consider the problem of source code abridgment, where the goal is to remove statements from a source code in order to display the source code in a small space, while at the same time leaving the ``important’’ parts of the source code intact, so that an engineer can read the code and quickly understand purpose of the code. To this end, we develop an algorithm that looks at a number of examples, human-created source code abridgments, and learns how to remove lines from the code in order to mimic the human abridger. The learning algorithm takes into account syntactic features of the code, as well as semantic features such as control flow and data dependencies. Through a comprehensive user study, we show that the abridgments that our system produces can decrease the time that a user must look at code in order to understand its functionality, as well as increase the accuracy of the assessment, while displaying the code in a greatly reduced area.</p>
</td>
	<td>summarization </td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/allamanis2017smartpaste/">SmartPaste: Learning to Adapt Source Code</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=SmartPaste: Learning to Adapt Source Code' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=SmartPaste: Learning to Adapt Source Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=SmartPaste:%20Learning%20to%20Adapt%20Source%20Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>M. Allamanis, M. Brockscmidt</td>
	<td></td>
	<td><p>Deep Neural Networks have been shown to succeed at a range of natural
language tasks such as machine translation and text summarization.
While tasks on source code (ie, formal languages) have been considered
recently, most work in this area does not attempt to capitalize on the
unique opportunities offered by its known syntax and structure. In this
work, we introduce SmartPaste, a first task that requires to use such
information. The task is a variant of the program repair problem that
requires to adapt a given (pasted) snippet of code to surrounding,
existing source code. As first solutions, we design a set of deep
neural models that learn to represent the context of each variable
location and variable usage in a data flow-sensitive way. Our
evaluation suggests that our models can learn to solve the SmartPaste
task in many cases, achieving 58.6% accuracy, while learning meaningful
representation of variable usages.</p>
</td>
	<td>representation variable misuse </td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/allamanis2017mining/">Mining Semantic Loop Idioms from Big Code</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Mining Semantic Loop Idioms from Big Code' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Mining Semantic Loop Idioms from Big Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Mining%20Semantic%20Loop%20Idioms%20from%20Big%20Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>M. Allamanis, E. T. Barr, C. Bird, M. Marron, C. Sutton</td>
	<td></td>
	<td><p>During maintenance, developers spend a lot of time transforming existing code: refactoring, optimizing, and adding checks to make it more robust. Much of this work is the drudgery of identifying and replacing specific patterns, yet it resists automation, because of meaningful patterns are hard to automatically find. We present a technique for mining loop idioms, surprisingly probable semantic patterns that occur in loops, from big code to find meaningful patterns. First, we show that automatically identifiable patterns exist, in great numbers, with a large scale empirical study of loop over 25 MLOC. We find that loops in this corpus are simple and predictable: 90% of them have fewer than 15LOC and 90% have no nesting and very simple control structure. Encouraged by this result, we coil loops to abstract away syntactic diversity to define information rich loop idioms. We show that only 50 loop idioms cover 50% of the concrete loops. We show how loop idioms can help a tool developers identify and prioritize refactorings. We also show how our framework opens the door to data-driven tool and language design discovering opportunities to introduce new API calls and language constructs: loop idioms show that LINQ would benefit from an Enumerate operator, a result confirmed by the fact that precisely this feature is one of the most requested features on StackOverflow with 197 votes and 95k views.</p>
</td>
	<td>pattern mining grammar </td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/loyola2017neural/">A Neural Architecture for Generating Natural Language Descriptions from Source Code Changes</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Neural Architecture for Generating Natural Language Descriptions from Source Code Changes' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Neural Architecture for Generating Natural Language Descriptions from Source Code Changes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=A%20Neural%20Architecture%20for%20Generating%20Natural%20Language%20Descriptions%20from%20Source%20Code%20Changes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>P. Loyola, E. Marrese-Taylor, Y. Matsuo</td>
	<td>ArXiV 1704.04856</td>
	<td><p>We propose a model to automatically describe changes introduced in the source code of a program using natural language. Our method receives as input a set of code commits, which contains both the modifications and message introduced by an user. These two modalities are used to train an encoder-decoder architecture. We evaluated our approach on twelve real world open source projects from four different programming languages. Quantitative and qualitative results showed that the proposed approach can generate feasible and semantically sound descriptions not only in standard in-project settings, but also in a cross-project setting.</p>
</td>
	<td>edit summarization </td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/jiang2017automatically/">Automatically Generating Commit Messages from Diffs using Neural Machine Translation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Automatically Generating Commit Messages from Diffs using Neural Machine Translation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Automatically Generating Commit Messages from Diffs using Neural Machine Translation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Automatically%20Generating%20Commit%20Messages%20from%20Diffs%20using%20Neural%20Machine%20Translation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>S. Jiang, A. Armaly, C. McMillan</td>
	<td>ArXiV 1708.09492</td>
	<td><p>Commit messages are a valuable resource in comprehension of software evolution, since they provide a record of changes such as feature additions and bug repairs. Unfortunately, programmers often neglect to write good commit messages. Different techniques have been proposed to help programmers by automatically writing these messages. These techniques are effective at describing what changed, but are often verbose and lack context for understanding the rationale behind a change. In contrast, humans write messages that are short and summarize the high level rationale. In this paper, we adapt Neural Machine Translation (NMT) to automatically “translate” diffs into commit messages. We trained an NMT algorithm using a corpus of diffs and human-written commit messages from the top 1k Github projects. We designed a filter to help ensure that we only trained the algorithm on higher-quality commit messages. Our evaluation uncovered a pattern in which the messages we generate tend to be either very high or very low quality. Therefore, we created a quality-assurance filter to detect cases in which we are unable to produce good messages, and return a warning instead.</p>
</td>
	<td>edit bimodal </td>
</tr>



<tr>
	<td>2016</td>
	<td><a href="/publications/chae2016automatically/">Automatically generating features for learning program analysis heuristics</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Automatically generating features for learning program analysis heuristics' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Automatically generating features for learning program analysis heuristics' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Automatically%20generating%20features%20for%20learning%20program%20analysis%20heuristics' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>K. Chae, H. Oh, K. Heo, H. Yang</td>
	<td></td>
	<td><p>We present a technique for automatically generating features for data-driven program analyses. Recently data-driven approaches for building a program analysis have been proposed, which mine existing codebases and automatically learn heuristics for finding a cost-effective abstraction for a given analysis task. Such approaches reduce the burden of the analysis designers, but they do not remove it completely; they still leave the highly nontrivial task of designing so called features to the hands of the designers. Our technique automates this feature design process. The idea is to use programs as features after reducing and abstracting them. Our technique goes through selected program-query pairs in codebases, and it reduces and abstracts the program in each pair to a few lines of code, while ensuring that the analysis behaves similarly for the original and the new programs with respect to the query. Each reduced program serves as a boolean feature for program-query pairs. This feature evaluates to true for a given program-query pair when (as a program) it is included in the program part of the pair. We have implemented our approach for three real-world program analyses. Our experimental evaluation shows that these analyses with automatically-generated features perform comparably to those with manually crafted features.</p>
</td>
	<td>representation </td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/nguyen2016mapping/">Mapping API Elements for Code Migration with Vector Representations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Mapping API Elements for Code Migration with Vector Representations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Mapping API Elements for Code Migration with Vector Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Mapping%20API%20Elements%20for%20Code%20Migration%20with%20Vector%20Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>T.D. Nguyen, A.T. Nguyen, T.N. Nguyen</td>
	<td>ICSE</td>
	<td>
</td>
	<td>migration API </td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/mou2016convolutional/">Convolutional Neural Networks over Tree Structures for Programming Language Processing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Convolutional Neural Networks over Tree Structures for Programming Language Processing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Convolutional Neural Networks over Tree Structures for Programming Language Processing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Convolutional%20Neural%20Networks%20over%20Tree%20Structures%20for%20Programming%20Language%20Processing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>L. Mou, G. Li, L. Zhang, T. Wang, Z. Jin</td>
	<td>AAAI</td>
	<td><p>Programming language processing (similar to natural language processing) is a hot research topic in the field of software engineering; it has also aroused growing interest in the
artificial intelligence community. However, different from a
natural language sentence, a program contains rich, explicit,
and complicated structural information. Hence, traditional
NLP models may be inappropriate for programs. In this paper, we propose a novel tree-based convolutional neural network (TBCNN) for programming language processing, in
which a convolution kernel is designed over programs’ abstract syntax trees to capture structural information. TBCNN
is a generic architecture for programming language processing; our experiments show its effectiveness in two different program analysis tasks: classifying programs according
to functionality, and detecting code snippets of certain patterns. TBCNN outperforms baseline methods, including several neural models for NLP.</p>
</td>
	<td>representation AST </td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/wang2016automatically/">Automatically Learning Semantic Features for Defect Prediction</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Automatically Learning Semantic Features for Defect Prediction' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Automatically Learning Semantic Features for Defect Prediction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Automatically%20Learning%20Semantic%20Features%20for%20Defect%20Prediction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>S. Wang, T. Liu, L. Tan</td>
	<td>ICSE</td>
	<td><p>Software defect prediction, which predicts defective code regions, can help developers find bugs and prioritize their testing efforts. To build accurate prediction models, previous
studies focus on manually designing features that encode the
characteristics of programs and exploring different machine
learning algorithms. Existing traditional features often fail
to capture the semantic differences of programs, and such a
capability is needed for building accurate prediction models.</p>

<p>To bridge the gap between programs’ semantics and
defect prediction features, this paper proposes to leverage a
powerful representation-learning algorithm, deep learning,
to learn semantic representation of programs automatically
from source code. Specifically, we leverage Deep Belief
Network (DBN) to automatically learn semantic features
from token vectors extracted from programs’ Abstract
Syntax Trees (ASTs).</p>

<p>Our evaluation on ten open source projects shows that
our automatically learned semantic features significantly improve both within-project defect prediction (WPDP) and
cross-project defect prediction (CPDP) compared to traditional features. Our semantic features improve WPDP on
average by 14.7% in precision, 11.5% in recall, and 14.2%
in F1. For CPDP, our semantic features based approach
outperforms the state-of-the-art technique TCA+ with traditional features by 8.9% in F1.</p>
</td>
	<td>defect representation </td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/singh2016question/">Question Independent Grading using Machine Learning: The Case of Computer Program Grading</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Question Independent Grading using Machine Learning: The Case of Computer Program Grading' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Question Independent Grading using Machine Learning: The Case of Computer Program Grading' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Question%20Independent%20Grading%20using%20Machine%20Learning:%20The%20Case%20of%20Computer%20Program%20Grading' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>G. Singh, S. Srikant, V. Aggarwal</td>
	<td>KDD</td>
	<td><p>Learning supervised models to grade open-ended responses is an expensive process. A model has to be trained for every prompt/question separately, which in turn requires graded samples. In automatic programming evaluation specifically, the focus of this work, this issue is amplified. The models have to be trained not only for every question but also for every language the question is offered in. Moreover, the availability and time taken by experts to create a labeled set of programs for each question is a major bottleneck in scaling such a system. We address this issue by presenting a method to grade computer programs which requires no manually assigned labeled samples for grading responses to a new, unseen question. We extend our previous work (by Srikant, Aggarwal; KDD 2014) wherein we introduced a grammar of features to learn question specific models. In this work, we propose a method to transform those features into a set of features that maintain their structural relation with the labels across questions. Using these features we learn one supervised model, across questions for a given language, which can then be applied to an ungraded response to an unseen question. We show that our method rivals the performance of both, question specific models and the consensus among human experts while substantially outperforming extant ways of evaluating codes. We demonstrate the system single s value by deploying it to grade programs in a high stakes assessment. The learning from this work is transferable to other grading tasks such as math question grading and also provides a new variation to the supervised learning approach.</p>
</td>
	<td>education </td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/li2016gated/">Gated Graph Sequence Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Gated Graph Sequence Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Gated Graph Sequence Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Gated%20Graph%20Sequence%20Neural%20Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Y. Li, R. Zemel, M. Brockschmidt, D. Tarlow</td>
	<td>ICLR</td>
	<td><p>Graph-structured data appears frequently in domains including chemistry, natural
language semantics, social networks, and knowledge bases. In this work, we study
feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify
to use gated recurrent units and modern optimization techniques and then extend
to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based
models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the
capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We
then show it achieves state-of-the-art performance on a problem from program
verification, in which subgraphs need to be described as abstract data structures.</p>

</td>
	<td>GNN program analysis </td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/wang2016bugram/">Bugram: bug detection with n-gram language models</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Bugram: bug detection with n-gram language models' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Bugram: bug detection with n-gram language models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Bugram:%20bug%20detection%20with%20n-gram%20language%20models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>S. Wang, D. Chollak, D. Movshovitz-Attias, L. Tan</td>
	<td>ASE</td>
	<td><p>To improve software reliability, many rule-based techniques have been proposed to infer programming rules and detect violations of these rules as bugs. These rule-based approaches often rely on the highly frequent appearances of certain patterns in a project to infer rules. It is known that if a pattern does not appear frequently enough, rules are not learned, thus missing many bugs.</p>

<p>In this paper, we propose a new approach—Bugram—that leverages n-gram language models instead of rules to detect bugs. Bugram models program tokens sequentially, using the n-gram language model. Token sequences from the program are then assessed according to their probability in the learned model, and low probability sequences are marked as potential bugs. The assumption is that low probability token sequences in a program are unusual, which may indicate bugs, bad practices, or unusual/special uses of code of which developers may want to be aware.</p>

<p>We evaluate Bugram in two ways. First, we apply Bugram on the latest versions of 16 open source Java projects. Results show that Bugram detects 59 bugs, 42 of which are manually verified as correct, 25 of which are true bugs and 17 are code snippets that should be refactored. Among the 25 true bugs, 23 cannot be detected by PR-Miner. We have reported these bugs to developers, 7 of which have already been confirmed by developers (4 of them have already been fixed), while the rest await confirmation. Second, we further compare Bugram with three additional graph- and rule-based bug detection tools, i.e., JADET, Tikanga, and GrouMiner. We apply Bugram on 14 Java projects evaluated in these three studies. Bugram detects 21 true bugs, at least 10 of which cannot be detected by these three tools. Our results suggest that Bugram is complementary to existing rule-based bug detection approaches.</p>

</td>
	<td>defect representation </td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/wang2016neural/">Neural Code Completion</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Neural Code Completion' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Neural Code Completion' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Neural%20Code%20Completion' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>C. Liu, X. Wang, R. Shin, J.E. Gonzalez, D. Song</td>
	<td></td>
	<td><p>Code completion, an essential part of modern software development, yet can be
challenging for dynamically typed programming languages. In this paper we explore the use of neural network techniques to automatically learn code completion
from a large corpus of dynamically typed JavaScript code. We show different
neural networks that leverage not only token level information but also structural
information, and evaluate their performance on different prediction tasks. We
demonstrate that our models can outperform the state-of-the-art approach, which
is based on decision tree techniques, on both next non-terminal and next terminal
prediction tasks by 3.8 points and 0.5 points respectively. We believe that neural
network techniques can play a transformative role in helping software developers
manage the growing complexity of software systems, and we see this work as a
first step in that direction.</p>
</td>
	<td>autocomplete </td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/nguyen2016learning/">Learning API Usages from Bytecode: A Statistical Approach</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning API Usages from Bytecode: A Statistical Approach' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning API Usages from Bytecode: A Statistical Approach' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20API%20Usages%20from%20Bytecode:%20A%20Statistical%20Approach' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>T.T. Nguyen, H.V. Pham, P.M. Vu, T.T. Nguyen</td>
	<td>ICSE</td>
	<td><p>Mobile app developers rely heavily on standard API frameworks and libraries. However, learning API usages is often challenging due to the fast-changing nature of API frameworks for mobile systems and the insufficiency of API documentation and source code examples. In this paper, we propose a novel approach to learn API usages from bytecode of Android mobile apps. Our core contributions include HAPI, a statistical model of API usages and three algorithms to extract method call sequences from apps’ bytecode, to train HAPI based on those sequences, and to recommend method calls in code completion using the trained HAPIs. Our empirical evaluation shows that our prototype tool can effectively learn API usages from 200 thousand apps containing 350 million method sequences. It recommends next method calls with top-3 accuracy of 90% and outperforms baseline approaches on average 10-20%.</p>
</td>
	<td>representation API </td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/gu2016deep/">Deep API Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep API Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep API Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20API%20Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>X. Gu, H. Zhang, D. Zhang, S. Kim</td>
	<td>FSE</td>
	<td><p>Developers often wonder how to implement a certain functionality (e.g., how to parse XML files) using APIs. Obtaining an API usage sequence based on an API-related natural language query is very helpful in this regard. Given a query, existing approaches utilize information retrieval models to search for matching API sequences. These approaches treat queries and APIs as bag-of-words (i.e., keyword matching or word-to-word alignment) and lack a deep understanding of the semantics of the query.</p>

<p>We propose DeepAPI, a deep learning based approach to generate API usage sequences for a given natural language query. Instead of a bags-of-words assumption, it learns the
sequence of words in a query and the sequence of associated APIs. DeepAPI adapts a neural language model named RNN Encoder-Decoder. It encodes a word sequence (user query) into a fixed-length context vector, and generates an API sequence based on the context vector. We also augment the RNN Encoder-Decoder by considering the importance of individual APIs. We empirically evaluate our approach with more than 7 million annotated code snippets collected from GitHub. The results show that our approach generates largely accurate API sequences and outperforms the related approaches.</p>

</td>
	<td>API search </td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/bielik2016phog/">PHOG: Probabilistic Model for Code</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=PHOG: Probabilistic Model for Code' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=PHOG: Probabilistic Model for Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=PHOG:%20Probabilistic%20Model%20for%20Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>P. Bielik, V. Raychev, M. Vechev</td>
	<td>ICML</td>
	<td><p>We introduce a new generative model for code called probabilistic higher order grammar (PHOG). PHOG generalizes probabilistic context free grammars (PCFGs) by allowing conditioning of a production rule beyond the parent non-terminal, thus capturing rich contexts relevant to programs. Even though PHOG is more powerful than a PCFG, it can be learned from data just as efficiently. We trained a PHOG model on a large JavaScript code corpus and show that it is more precise than existing models, while similarly fast. As a result, PHOG can immediately benefit existing programming tools based on probabilistic models of code.</p>
</td>
	<td>grammar generation language model </td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/bichsel2016statistical/">Statistical Deobfuscation of Android Applications</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Statistical Deobfuscation of Android Applications' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Statistical Deobfuscation of Android Applications' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Statistical%20Deobfuscation%20of%20Android%20Applications' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>B. Bichsel, V. Raychev, P. Tsankov, M. Vechev</td>
	<td>CCS</td>
	<td><p>This work presents a new approach for deobfuscating Android APKs based on probabilistic learning of large code bases (termed “Big Code”). The key idea is to learn a probabilistic model over thousands of non-obfuscated Android applications and to use this probabilistic model to deobfuscate new, unseen Android APKs. The concrete focus of the paper is on reversing layout obfuscation, a popular transformation which renames key program elements such as classes, packages, and methods, thus making it difficult to understand what the program does. Concretely, the paper: (i) phrases the layout deobfuscation problem of Android APKs as structured prediction in a probabilistic graphical model, (ii) instantiates this model with a rich set of features and constraints that capture the Android setting, ensuring both semantic equivalence and high prediction accuracy, and (iii) shows how to leverage powerful inference and learning algorithms to achieve overall precision and scalability of the probabilistic predictions.</p>

<p>We implemented our approach in a tool called DeGuard and used it to: (i) reverse the layout obfuscation performed by the popular ProGuard system on benign, open-source applications, (ii) predict third-party libraries imported by benign APKs (also obfuscated by ProGuard), and (iii) rename obfuscated program elements of Android malware. The experimental results indicate that DeGuard is practically effective: it recovers 79.1% of the program element names obfuscated with ProGuard, it predicts third-party libraries with accuracy of 91.3%, and it reveals string decoders and classes that handle sensitive data in Android malware.</p>

</td>
	<td>deobfuscation naming </td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/dam2016deep/">A deep language model for software code</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A deep language model for software code' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A deep language model for software code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=A%20deep%20language%20model%20for%20software%20code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>H. K. Dam, T. Tran, T. Pham</td>
	<td>ArXiV 1608.02715</td>
	<td><p>Existing language models such as n-grams for software code often fail to capture a long context where dependent code elements scatter far apart. In this paper, we propose a novel approach to build a language model for software code to address this particular issue. Our language model, partly inspired by human memory, is built upon the powerful deep learning-based Long Short Term Memory architecture that is capable of learning long-term dependencies which occur frequently in software code. Results from our intrinsic evaluation on a corpus of Java projects have demonstrated the effectiveness of our language model. This work contributes to realizing our vision for DeepSoft, an end-to-end, generic deep learning-based framework for modeling software and its development process.</p>
</td>
	<td>language model generation </td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/bhoopchand2016learning/">Learning Python Code Suggestion with a Sparse Pointer Network</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning Python Code Suggestion with a Sparse Pointer Network' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning Python Code Suggestion with a Sparse Pointer Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20Python%20Code%20Suggestion%20with%20a%20Sparse%20Pointer%20Network' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>A. Bhoopchand, T. Rocktäschel, E.T. Barr, S. Riedel</td>
	<td>ArXiV 1611.08307</td>
	<td><p>To enhance developer productivity, all modern integrated development environments (IDEs) include code suggestion functionality that proposes likely next tokens at the cursor. While current IDEs work well for statically-typed languages, their reliance on type annotations means that they do not provide the same level of support for dynamic programming languages as for statically-typed languages. Moreover, suggestion engines in modern IDEs do not propose expressions or multi-statement idiomatic code. Recent work has shown that language models can improve code suggestion systems by learning from software repositories. This paper introduces a neural language model with a sparse pointer network aimed at capturing very long-range dependencies. We release a large-scale code suggestion corpus of 41M lines of Python code crawled from GitHub. On this corpus, we found standard neural language models to perform well at suggesting local phenomena, but struggle to refer to identifiers that are introduced many tokens in the past. By augmenting a neural language model with a pointer network specialized in referring to predefined classes of identifiers, we obtain a much lower perplexity and a 5 percentage points increase in accuracy for code suggestion compared to an LSTM baseline. In fact, this increase in code suggestion accuracy is due to a 13 times more accurate prediction of identifiers. Furthermore, a qualitative analysis shows this model indeed captures interesting long-range dependencies, like referring to a class member defined over 60 tokens in the past.</p>
</td>
	<td>language model autocomplete </td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/ling2016latent/">Latent Predictor Networks for Code Generation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Latent Predictor Networks for Code Generation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Latent Predictor Networks for Code Generation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Latent%20Predictor%20Networks%20for%20Code%20Generation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>W. Ling, E. Grefenstette, K. M. Hermann, T. Kocisky, A. Senior, F. Wang, P. Blunsom</td>
	<td>ACL</td>
	<td><p>Many  language  generation  tasks  require
the production of text conditioned on both
structured  and  unstructured  inputs.
We present  a  novel  neural  network  architecture  which  generates  an  output  sequence
conditioned on an arbitrary number of input  functions.
Crucially,  our  approach
allows  both  the  choice  of  conditioning
context and the granularity of generation,
for  example  characters  or  tokens,  to  be
marginalised, thus permitting scalable and
effective training.  Using this framework,
we address the problem of generating programming code from a mixed natural language  and  structured  specification.
We create two new data sets for this paradigm
derived  from  the  collectible  trading  card
games  Magic  the  Gathering  and  Hearthstone.   On  these,  and  a  third  preexisting
corpus,  we  demonstrate  that  marginalising multiple predictors allows our model
to outperform strong benchmarks.</p>

</td>
	<td>bimodal generation </td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/fowkes2016parameter/">Parameter-Free Probabilistic API Mining across GitHub</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Parameter-Free Probabilistic API Mining across GitHub' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Parameter-Free Probabilistic API Mining across GitHub' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Parameter-Free%20Probabilistic%20API%20Mining%20across%20GitHub' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>J. Fowkes, C. Sutton</td>
	<td>FSE</td>
	<td><p>Existing API mining algorithms can be difficult to use as they require expensive parameter tuning and the returned set of API calls can be large, highly redundant and difficult to understand. To address this, we present PAM (Probabilistic API Miner), a near parameter-free probabilistic algorithm for mining the most interesting API call patterns. We show that PAM significantly outperforms both MAPO and UPMiner, achieving 69% test-set precision, at retrieving relevant API call sequences from GitHub. Moreover, we focus on libraries for which the developers have explicitly provided code examples, yielding over 300,000 LOC of hand-written API example code from the 967 client projects in the data set. This evaluation suggests that the hand-written examples actually have limited coverage of real API usages.</p>

</td>
	<td>API pattern mining </td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/liu2016towards/">Towards Better Program Obfuscation: Optimization via Language Models</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards Better Program Obfuscation: Optimization via Language Models' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards Better Program Obfuscation: Optimization via Language Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Towards%20Better%20Program%20Obfuscation:%20Optimization%20via%20Language%20Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>H. Liu</td>
	<td>ICSE</td>
	<td><p>As a common practice in software development, program
obfuscation aims at deterring reverse engineering and malicious attacks on released source or binary code. Owning ample obfuscation techniques, we have relatively little
knowledge on how to most effectively use them. The biggest
challenge lies in identifying the most useful combination of
these techniques. We propose a unified framework to automatically generate and optimize obfuscation based on an
obscurity language model and a Monte Carlo Markov Chain
(MCMC) based search algorithm. We further instantiate it
for JavaScript programs and developed the Closure tool.
Compared to the well-known Google Closure Compiler, Closure outperforms its default setting by 26%. For programs
which have already been well obfuscated, Closure can still
outperform by 22%.</p>
</td>
	<td>deobfuscation </td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/white2016deep/">Deep Learning Code Fragments for Code Clone Detection</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Learning Code Fragments for Code Clone Detection' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Learning Code Fragments for Code Clone Detection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Deep%20Learning%20Code%20Fragments%20for%20Code%20Clone%20Detection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>M. White, M. Tufano, C. Vendome, D. Poshyvanyk</td>
	<td>ASE</td>
	<td><p>Code clone detection is an important problem for software
maintenance and evolution. Many approaches consider either structure or identifiers, but none of the existing detection techniques model both sources of information. These
techniques also depend on generic, handcrafted features to
represent code fragments. We introduce learning-based detection techniques where everything for representing terms
and fragments in source code is mined from the repository.
Our code analysis supports a framework, which relies on
deep learning, for automatically linking patterns mined at
the lexical level with patterns mined at the syntactic level.
We evaluated our novel learning-based approach for code
clone detection with respect to feasibility from the point
of view of software maintainers. We sampled and manually
evaluated 398 file- and 480 method-level pairs across eight
real-world Java systems; 93% of the file- and method-level
samples were evaluated to be true positives. Among the true
positives, we found pairs mapping to all four clone types. We
compared our approach to a traditional structure-oriented
technique and found that our learning-based approach detected clones that were either undetected or suboptimally
reported by the prominent tool Deckard. Our results affirm
that our learning-based approach is suitable for clone detection and a tenable technique for researchers.</p>
</td>
	<td>clone </td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/yadid2016extracting/">Extracting Code from Programming Tutorial Videos</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Extracting Code from Programming Tutorial Videos' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Extracting Code from Programming Tutorial Videos' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Extracting%20Code%20from%20Programming%20Tutorial%20Videos' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>S. Yadid, E. Yahav</td>
	<td>Onward!</td>
	<td><p>The number of programming tutorial videos on the web
increases daily. Video hosting sites such as YouTube host
millions of video lectures, with many programming tutorials for various languages and platforms. These videos contain a wealth of valuable information, including code that
may be of interest. However, two main challenges have so
far prevented the effective indexing of programming tutorial
videos: (i) code in tutorials is typically written on-the-fly,
with only parts of the code visible in each frame, and (ii) optical character recognition (OCR) is not precise enough to
produce quality results from videos.</p>

<p>We present a novel approach for extracting code from
videos that is based on: (i) consolidating code across frames,
and (ii) statistical language models for applying corrections
at different levels, allowing us to make corrections by choosing the most likely token, combination of tokens that form a
likely line structure, and combination of lines that lead to
a likely code fragment in a particular language. We implemented our approach in a tool called ACE , and used it to extract code from 40 Android video tutorials on YouTube . Our
evaluation shows that ACE extracts code with high accuracy,
enabling deep indexing of video tutorials.</p>
</td>
	<td>information extraction </td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/pu2016skp/">sk_p: a neural program corrector for MOOCs</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=sk_p: a neural program corrector for MOOCs' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=sk_p: a neural program corrector for MOOCs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=sk_p:%20a%20neural%20program%20corrector%20for%20MOOCs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Y. Pu, K. Narasimhan, A. Solar-Lezama, R. Barzilay</td>
	<td>SPLASH</td>
	<td><p>We present a novel technique for automatic program correction in MOOCs, capable of fixing both syntactic and semantic errors without manual, problem specific correction strategies. Given an incorrect student program, it generates candidate programs from a distribution of likely corrections, and checks each candidate for correctness against a test suite.</p>

<p>The key observation is that in MOOCs many programs share similar code fragments, and the seq2seq neural network model, used in the natural-language processing task of machine translation, can be modified and trained to recover these fragments.</p>

<p>Experiment shows our scheme can correct 29% of all incorrect submissions and out-performs state of the art approach which requires manual, problem specific correction strategies.</p>
</td>
	<td>repair </td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/iyer2016summarizing/">Summarizing Source Code using a Neural Attention Model</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Summarizing Source Code using a Neural Attention Model' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Summarizing Source Code using a Neural Attention Model' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Summarizing%20Source%20Code%20using%20a%20Neural%20Attention%20Model' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>S. Iyer, I. Konstas, A. Cheung, L. Zettlemoyer</td>
	<td>ACL</td>
	<td><p>High quality source code is often paired
with high level summaries of the computation it performs, for example in code
documentation or in descriptions posted
in online forums. Such summaries are
extremely useful for applications such as
code search but are expensive to manually
author, hence only done for a small fraction of all code that is produced. In this
paper, we present the first completely data-driven approach for generating high level
summaries of source code. Our model,
CODE-NN , uses Long Short Term Memory (LSTM) networks with attention to
produce sentences that describe C# code
snippets and SQL queries. CODE-NN
is trained on a new corpus that is automatically collected from StackOverflow,
which we release. Experiments demonstrate strong performance on two tasks:
(1) code summarization, where we establish the first end-to-end learning results
and outperform strong baselines, and (2)
code retrieval, where our learned model
improves the state of the art on a recently
introduced C# benchmark by a large margin.</p>
</td>
	<td>summarization bimodal </td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/allamanis2016convolutional/">A Convolutional Attention Network for Extreme Summarization of Source Code</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Convolutional Attention Network for Extreme Summarization of Source Code' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Convolutional Attention Network for Extreme Summarization of Source Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=A%20Convolutional%20Attention%20Network%20for%20Extreme%20Summarization%20of%20Source%20Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>M. Allamanis, H. Peng, C. Sutton</td>
	<td>ICML</td>
	<td><p>Attention mechanisms in neural networks have proved useful for problems in which
the input and output do not have fixed dimension. Often there exist features that
are locally translation invariant and would be valuable for directing the model’s attention,
but previous attentional architectures are not constructed to learn such features specifically.
We introduce an attentional neural network that employs convolution on the input tokens to detect
local time-invariant and long-range topical attention features in a context-dependent way. We
apply this architecture to the problem of extreme summarization of source code snippets into short,
descriptive function name-like summaries. Using those features, the model sequentially generates a
summary by marginalizing over two attention mechanisms: one that predicts the next summary token based 
n the attention weights of the input tokens and another that is able to copy a code token as-is directly
into the summary. We demonstrate our convolutional attention neural network’s performance on 10 popular Java
projects showing that it achieves better performance compared to previous attentional mechanisms.</p>
</td>
	<td>naming summarization </td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/patra2016learning/">Learning to Fuzz: Application-Independent Fuzz Testing with Probabilistic, Generative Models of Input Data</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning to Fuzz: Application-Independent Fuzz Testing with Probabilistic, Generative Models of Input Data' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning to Fuzz: Application-Independent Fuzz Testing with Probabilistic, Generative Models of Input Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20to%20Fuzz:%20Application-Independent%20Fuzz%20Testing%20with%20Probabilistic,%20Generative%20Models%20of%20Input%20Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>J. Patra, M. Pradel</td>
	<td></td>
	<td><p>Fuzzing is a popular technique to create test inputs for software that processes structured data. It has been successfully
applied in various domains, ranging from compilers and interpreters over program analyses to rendering engines, image manipulation tools, and word processors. Existing fuzz
testing techniques are tailored for a particular purpose and
rely on a carefully crafted model of the data to be generated.
This paper presents TreeFuzz, a generic approach for generating structured data without an a priori known model. The
key idea is to exploit a given corpus of example data to au-
tomatically infer probabilistic, generative models that create
new data with properties similar to the corpus. To support a
wide range of different properties, TreeFuzz is designed as a
framework with an extensible set of techniques to infer generative models. We apply the idea to JavaScript programs
and HTML documents and show that the approach generates mostly valid data for both of them: 96.3% of the generated JavaScript programs are syntactically valid and there are
only 2.06 validation errors per kilobyte of generated HTML.
The performance of both learning and generation scales linearly w.r.t. the size of the corpus. Using TreeFuzz-generated
JavaScript programs for differential testing of JavaScript engines exposes various inconsistencies among browsers, including browser bugs and unimplemented language features.</p>
</td>
	<td>fuzzing </td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/raychev2016learning/">Learning Programs from Noisy Data</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning Programs from Noisy Data' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning Programs from Noisy Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20Programs%20from%20Noisy%20Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>V. Raychev, P. Bielik, M. Vechev, A. Krause</td>
	<td>POPL</td>
	<td><p>We present a new approach for learning programs from noisy
datasets. Our approach is based on two new concepts: a regularized
program generator which produces a candidate program based on a
small sample of the entire dataset while avoiding overfitting, and a
dataset sampler which carefully samples the dataset by leveraging
the candidate program’s score on that dataset. The two components
are connected in a continuous feedback-directed loop.</p>

<p>We show how to apply this approach to two settings: one where
the dataset has a bound on the noise, and another without a noise
bound. The second setting leads to a new way of performing
approximate empirical risk minimization on hypotheses classes
formed by a discrete search space.</p>

<p>We then present two new kinds of program synthesizers which
target the two noise settings. First, we introduce a novel regularized
bitstream synthesizer that successfully generates programs even in
the presence of incorrect examples. We show that the synthesizer
can detect errors in the examples while combating overfitting –
a major problem in existing synthesis techniques. We also show
how the approach can be used in a setting where the dataset grows
dynamically via new examples (e.g., provided by a human).</p>

<p>Second, we present a novel technique for constructing statistical
code completion systems. These are systems trained on massive
datasets of open source programs, also known as “Big Code”. The
key idea is to introduce a domain specific language (DSL) over
trees and to learn functions in that DSL directly from the dataset.
These learned functions then condition the predictions made by the
system. This is a flexible and powerful technique which generalizes
several existing works as we no longer need to decide a priori on
what the prediction should be conditioned (another benefit is that
the learned functions are a natural mechanism for explaining the
prediction). As a result, our code completion system surpasses the
prediction capabilities of existing, hard-wired systems.</p>
</td>
	<td>generation grammar </td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/bhatia2016automated/">Automated Correction for Syntax Errors in Programming Assignments using Recurrent Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Automated Correction for Syntax Errors in Programming Assignments using Recurrent Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Automated Correction for Syntax Errors in Programming Assignments using Recurrent Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Automated%20Correction%20for%20Syntax%20Errors%20in%20Programming%20Assignments%20using%20Recurrent%20Neural%20Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>S. Bhatia, R. Singh</td>
	<td></td>
	<td><p>We present a method for automatically generating repair feedback for syntax errors for introductory programming problems. Syntax errors constitute one of the largest classes of errors (34%) in our dataset of student submissions obtained from a MOOC course on edX. The previous techniques for generating automated feedback on programming assignments have focused on functional correctness and style considerations of student programs. These techniques analyze the program AST of the program and then perform some dynamic and symbolic analyses to compute repair feedback. Unfortunately, it is not possible to generate ASTs for student programs with syntax errors and therefore the previous feedback techniques are not applicable in repairing syntax errors. We present a technique for providing feedback on syntax errors that uses Recurrent neural networks (RNNs) to model syntactically valid token sequences. Our approach is inspired from the recent work on learning language models from Big Code (large code corpus). For a given programming assignment, we first learn an RNN to model all valid token sequences using the set of syntactically correct student submissions. Then, for a student submission with
syntax errors, we query the learnt RNN model with the prefix token sequence to predict token sequences that can fix the error by either replacing or inserting the predicted token sequence at the error location. We evaluate our technique on over 14, 000 student submissions with syntax errors. Our technique can completely repair 31.69% (4501/14203) of submissions with syntax errors and in addition partially correct 6.39% (908/14203) of the submissions.</p>
</td>
	<td>repair </td>
</tr>



<tr>
	<td>2015</td>
	<td><a href="/publications/oh2015learning/">Learning a Strategy for Adapting a Program Analysis via Bayesian Optimisation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning a Strategy for Adapting a Program Analysis via Bayesian Optimisation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning a Strategy for Adapting a Program Analysis via Bayesian Optimisation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20a%20Strategy%20for%20Adapting%20a%20Program%20Analysis%20via%20Bayesian%20Optimisation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>H. Oh, H. Yang, K, Yi</td>
	<td>OOPSLA</td>
	<td><p>Building a cost-effective static analyser for real-world programs is still regarded an art. One key contributor to this
grim reputation is the difficulty in balancing the cost and the
precision of an analyser. An ideal analyser should be adap-
tive to a given analysis task, and avoid using techniques that
unnecessarily improve precision and increase analysis cost.
However, achieving this ideal is highly nontrivial, and it requires a large amount of engineering efforts.</p>

<p>In this paper we present a new approach for building
an adaptive static analyser. In our approach, the analyser
includes a sophisticated parameterised strategy that decides, for each part of a given program, whether to apply
a precision-improving technique to that part or not. We
present a method for learning a good parameter for such
a strategy from an existing codebase via Bayesian optimisation. The learnt strategy is then used for new, unseen programs. Using our approach, we developed partially flow-
and context-sensitive variants of a realistic C static analyser.
The experimental results demonstrate that using Bayesian
optimisation is crucial for learning from an existing codebase. Also, they show that among all program queries that
require flow- or context-sensitivity, our partially flow- and
context-sensitive analysis answers the 75% of them, while
increasing the analysis cost only by 3.3x of the baseline
flow- and context-insensitive analysis, rather than 40x or
more of the fully sensitive version.</p>
</td>
	<td>program analysis </td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/oda2015learning/">Learning to Generate Pseudo-code from Source Code using Statistical Machine Translation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning to Generate Pseudo-code from Source Code using Statistical Machine Translation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning to Generate Pseudo-code from Source Code using Statistical Machine Translation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20to%20Generate%20Pseudo-code%20from%20Source%20Code%20using%20Statistical%20Machine%20Translation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Y. Oda, H. Fudaba, G. Neubig, H. Hata, S. Sakti, T. Toda, and S. Nakamura</td>
	<td>ASE</td>
	<td><p>Pseudo-code written in natural language can aid
the comprehension of source code in unfamiliar programming
languages. However, the great majority of source code has no
corresponding pseudo-code, because pseudo-code is redundant
and laborious to create. If pseudo-code could be generated
automatically and instantly from given source code, we could
allow for on-demand production of pseudo-code without human
effort. In this paper, we propose a method to automatically
generate pseudo-code from source code, specifically adopting the
statistical machine translation (SMT) framework. SMT, which
was originally designed to translate between two natural languages, allows us to automatically learn the relationship between
source code/pseudo-code pairs, making it possible to create a
pseudo-code generator with less human effort. In experiments,
we generated English or Japanese pseudo-code from Python
statements using SMT, and find that the generated pseudo-code
is largely accurate, and aids code understanding.</p>
</td>
	<td>representation bimodal AST </td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/sharma2015nirmal/">NIRMAL: Automatic Identification of Software Relevant Tweets Leveraging Language Model</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=NIRMAL: Automatic Identification of Software Relevant Tweets Leveraging Language Model' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=NIRMAL: Automatic Identification of Software Relevant Tweets Leveraging Language Model' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=NIRMAL:%20Automatic%20Identification%20of%20Software%20Relevant%20Tweets%20Leveraging%20Language%20Model' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>A. Sharma, Y. Tian, D. Lo</td>
	<td>SANER</td>
	<td><p>Twitter is one of the most widely used social media
platforms today. It enables users to share and view short 140-character messages called “tweets”. About 284 million active
users generate close to 500 million tweets per day. Such rapid
generation of user generated content in large magnitudes results
in the problem of information overload. Users who are interested
in information related to a particular domain have limited means
to filter out irrelevant tweets and tend to get lost in the huge
amount of data they encounter. A recent study by Singer et
al. found that software developers use Twitter to stay aware of
industry trends, to learn from others, and to network with other
developers. However, Singer et al. also reported that developers
often find Twitter streams to contain too much noise which is a
barrier to the adoption of Twitter. In this paper, to help developers
cope with noise, we propose a novel approach named NIRMAL,
which automatically identifies software relevant tweets from a
collection or stream of tweets. Our approach is based on language
modeling which learns a statistical model based on a training
corpus (i.e., set of documents). We make use of a subset of posts
from StackOverflow, a programming question and answer site, as
a training corpus to learn a language model. A corpus of tweets
was then used to test the effectiveness of the trained language
model. The tweets were sorted based on the rank the model
assigned to each of the individual tweets. The top 200 tweets
were then manually analyzed to verify whether they are software
related or not, and then an accuracy score was calculated. The
results show that decent accuracy scores can be achieved by
various variants of NIRMAL, which indicates that NIRMAL can
effectively identify software related tweets from a huge corpus of
tweets.</p>
</td>
	<td>information extraction </td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/saraiva2015products/">Products, Developers, and Milestones: How Should I Build My N-Gram Language Model</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Products, Developers, and Milestones: How Should I Build My N-Gram Language Model' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Products, Developers, and Milestones: How Should I Build My N-Gram Language Model' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Products,%20Developers,%20and%20Milestones:%20How%20Should%20I%20Build%20My%20N-Gram%20Language%20Model' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>C. Saraiva, C. Bird, T. Zimmermann</td>
	<td>FSE</td>
	<td><p>Recent work has shown that although programming languages en-
able source code to be rich and complex, most code tends to be
repetitive and predictable. The use of natural language processing
(NLP) techniques applied to source code such as n-gram language
models show great promise in areas such as code completion, aiding impaired developers, and code search. In this paper, we address
three questions related to different methods of constructing lan-
guage models in an industrial context. Specifically, we ask: (1) Do
application specific, but smaller language models perform better
than language models across applications? (2) Are developer specific language models effective and do they differ depending on
what parts of the codebase a developer is working in? (3) Finally,
do language models change over time, i.e., does a language model
from early development model change later on in development?
The answers to these questions enable techniques that make use of
programming language models in development to choose the model
training corpus more effectively.</p>

<p>We evaluate these questions by building 28 language models across
developers, time periods, and applications within Microsoft Office
and present the results in this paper. We find that developer and
application specific language models perform better than models
from the entire codebase, but that temporality has little to no effect
on language model performance.</p>
</td>
	<td>language model </td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/glassman2015overcode/">OverCode: visualizing variation in student solutions to programming problems at scale</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=OverCode: visualizing variation in student solutions to programming problems at scale' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=OverCode: visualizing variation in student solutions to programming problems at scale' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=OverCode:%20visualizing%20variation%20in%20student%20solutions%20to%20programming%20problems%20at%20scale' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>E.L. Glassman, J. Scott, R. Singh, P. Guo, and R.C. Miller</td>
	<td></td>
	<td><p>In MOOCs, a single programming exercise may produce thousands of solutions from learners. Understanding solution variation is important for providing appropriate feedback to students at scale. The wide variation among these solutions can be a source of pedagogically valuable examples and can be used to refine the autograder for the exercise by exposing corner cases. We present OverCode, a system for visualizing and exploring thousands of programming solutions. OverCode uses both static and dynamic analysis to cluster similar solutions, and lets teachers further filter and cluster solutions based on different criteria. We evaluated OverCode against a nonclustering baseline in a within-subjects study with 24 teaching assistants and found that the OverCode interface allows teachers to more quickly develop a high-level view of students’ understanding and misconceptions, and to provide feedback that is relevant to more students’ solutions.</p>
</td>
	<td>repair </td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/raychev2015predicting/">Predicting Program Properties from “Big Code”</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Predicting Program Properties from “Big Code”' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Predicting Program Properties from “Big Code”' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Predicting%20Program%20Properties%20from%20%E2%80%9CBig%20Code%E2%80%9D' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>V. Raychev, M. Vechev, A. Krause</td>
	<td>POPL</td>
	<td><p>We present a new approach for predicting program properties from
massive codebases (aka “Big Code”). Our approach first learns a
probabilistic model from existing data and then uses this model to
predict properties of new, unseen programs.</p>

<p>The key idea of our work is to transform the input program into
a representation which allows us to phrase the problem of inferring program properties as structured prediction in machine learning. This formulation enables us to leverage powerful probabilistic
graphical models such as conditional random fields (CRFs) in order
to perform joint prediction of program properties.</p>

<p>As an example of our approach, we built a scalable prediction
engine called JSNICE 1 for solving two kinds of problems in the
context of JavaScript: predicting (syntactic) names of identifiers
and predicting (semantic) type annotations of variables. Experimentally, JSNICE predicts correct names for 63% of name identifiers and its type annotation predictions are correct in 81% of the
cases. In the first week since its release, JSN ICE was used by more
than 30,000 developers and in only few months has become a popular tool in the JavaScript developer community.</p>

<p>By formulating the problem of inferring program properties as
structured prediction and showing how to perform both learning
and inference in this context, our work opens up new possibilities
for attacking a wide range of difficult problems in the context of
“Big Code” including invariant generation, de-compilation, synthesis and others.</p>
</td>
	<td>program analysis naming types deobfuscation </td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/white2015toward/">Toward Deep Learning Software Repositories</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Toward Deep Learning Software Repositories' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Toward Deep Learning Software Repositories' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Toward%20Deep%20Learning%20Software%20Repositories' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>M. White, C. Vendome, M. Linares-Vásquez, D. Poshyvanyk</td>
	<td>MSR</td>
	<td><p>Deep learning subsumes algorithms that automatically learn compositional representations. The ability of these
models to generalize well has ushered in tremendous advances
in many fields such as natural language processing (NLP).
Recent research in the software engineering (SE) community
has demonstrated the usefulness of applying NLP techniques to
software corpora. Hence, we motivate deep learning for software
language modeling, highlighting fundamental differences between
state-of-the-practice software language models and connectionist
models. Our deep learning models are applicable to source
code files (since they only require lexically analyzed source
code written in any programming language) and other types
of artifacts. We show how a particular deep learning model
can remember its state to effectively model sequential data,
e.g., streaming software tokens, and the state is shown to be
much more expressive than discrete tokens in a prefix. Then we
instantiate deep learning models and show that deep learning
induces high-quality models compared to n-grams and cache-based n-grams on a corpus of Java projects. We experiment
with two of the models’ hyperparameters, which govern their
capacity and the amount of context they use to inform predictions,
before building several committees of software language models
to aid generalization. Then we apply the deep learning models to
code suggestion and demonstrate their effectiveness at a real SE
task compared to state-of-the-practice models. Finally, we propose
avenues for future work, where deep learning can be brought to
bear to support model-based testing, improve software lexicons,
and conceptualize software artifacts. Thus, our work serves as
the first step toward deep learning software repositories.</p>
</td>
	<td>representation </td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/corley2015exploring/">Exploring the Use of Deep Learning for Feature Location</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Exploring the Use of Deep Learning for Feature Location' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Exploring the Use of Deep Learning for Feature Location' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Exploring%20the%20Use%20of%20Deep%20Learning%20for%20Feature%20Location' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>C.S. Corley, K. Damevski, N.A. Kraft</td>
	<td></td>
	<td><p>Deep learning models are a class of neural networks. Relative to n-gram models, deep learning models can capture more complex statistical patterns based on smaller training corpora. In this paper we explore the use of a particular deep learning model, document vectors (DVs), for feature location. DVs seem well suited to use with source code, because they both capture the influence of context on each term in a corpus and map terms into a continuous semantic space that encodes semantic relationships such as synonymy. We present preliminary results that show that a feature location technique (FLT) based on DVs can outperform an analogous FLT based on latent Dirichlet allocation (LDA) and then suggest several directions for future work on the use of deep learning models to improve developer effectiveness in feature location.</p>
</td>
	<td>feature location representation </td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/gvero2015synthesizing/">Synthesizing Java expressions from free-form queries</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Synthesizing Java expressions from free-form queries' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Synthesizing Java expressions from free-form queries' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Synthesizing%20Java%20expressions%20from%20free-form%20queries' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>T. Gvero, V. Kuncak</td>
	<td>OOPSLA</td>
	<td><p>We present a new code assistance tool for integrated development environments. Our system accepts as input free-form queries containing a mixture of English and Java, and produces Java code expressions that take the query into account and respect syntax, types, and scoping rules of Java, as well as statistical usage patterns. In contrast to solutions based on code search, the results returned by our tool need not directly correspond to any previously seen code fragment. As part of our system we have constructed a probabilistic context free grammar for Java constructs and library invocations, as well as an algorithm that uses a customized natural language processing tool chain to extract information from free-form text queries. We present the results on a number of examples showing that our technique (1) often produces the expected code fragments, (2) tolerates much of the flexibility of natural language, and (3) can repair incorrect Java expressions that use, for example, the wrong syntax or missing arguments.</p>
</td>
	<td>synthesis generation bimodal </td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/hellendoorn2015will/">Will they like this? Evaluating Code Contributions With Language Models</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Will they like this? Evaluating Code Contributions With Language Models' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Will they like this? Evaluating Code Contributions With Language Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Will%20they%20like%20this?%20Evaluating%20Code%20Contributions%20With%20Language%20Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>V.J. Hellendoorn, P. Devanbu, A. Bacchelli</td>
	<td>MSR</td>
	<td><p>Popular open-source software projects receive and
review contributions from a diverse array of developers, many
of whom have little to no prior involvement with the project. A
recent survey reported that reviewers consider conformance to
the project’s code style to be one of the top priorities when evaluating code contributions on Github. We propose to quantitatively
evaluate the existence and effects of this phenomenon. To this aim
we use language models, which were shown to accurately capture
stylistic aspects of code. We find that rejected changesets do
contain code significantly less similar to the project than accepted
ones; furthermore, the less similar changesets are more likely
to be subject to thorough review. Armed with these results we
further investigate whether new contributors learn to conform to
the project style and find that experience is positively correlated
with conformance to the project’s code style.</p>
</td>
	<td>code review language model </td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/proksch2015intelligent/">Intelligent Code Completion with Bayesian Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Intelligent Code Completion with Bayesian Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Intelligent Code Completion with Bayesian Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Intelligent%20Code%20Completion%20with%20Bayesian%20Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>S. Proksch, J. Lerch, M. Mezini</td>
	<td>TSE</td>
	<td><p>Code completion is an integral part of modern Integrated Development Environments (IDEs). Developers
often use it to explore Application Programming Interfaces (APIs). It is also useful to reduce the required
amount of typing and to help avoid typos. Traditional code completion systems propose all type-correct
methods to the developer. Such a list is often very long with many irrelevant items. More intelligent code
completion systems have been proposed in prior work to reduce the list of proposed methods to relevant
items.</p>

<p>This work extends one of these existing approaches, the Best Matching Neighbor (BMN) algorithm. We
introduce Bayesian networks as an alternative underlying model, use additional context information for
more precise recommendations, and apply clustering techniques to improve model sizes. We compare our
new approach, Pattern-based Bayesian Networks (PBN), to the existing BMN algorithm. We extend previously used evaluation methodologies and, in addition to prediction quality, we also evaluate model size and
inference speed.</p>

<p>Our results show that the additional context information we collect improves prediction quality, especially
for queries that do not contain method calls. We also show that PBN can obtain comparable prediction
quality to BMN, while model size and inference speed scale better with large input sizes.</p>
</td>
	<td>autocomplete </td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/mangal2015user/">A User-Guided Approach to Program Analysis</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A User-Guided Approach to Program Analysis' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A User-Guided Approach to Program Analysis' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=A%20User-Guided%20Approach%20to%20Program%20Analysis' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>R. Mangal, X. Zhang, A. V. Nori, M. Naik</td>
	<td>FSE</td>
	<td><p>Program analysis tools often produce undesirable output
due to various approximations. We present an approach
and a system Eugene that allows user feedback to guide
such approximations towards producing the desired output.
We formulate the problem of user-guided program analysis in terms of solving a combination of hard rules and soft
rules: hard rules capture soundness while soft rules capture
degrees of approximations and preferences of users. Our
technique solves the rules using an off-the-shelf solver in a
manner that is sound (satisfies all hard rules), optimal (maximally satisfies soft rules), and scales to real-world analy-
ses and programs. We evaluate Eugene on two different
analyses with labeled output on a suite of seven Java pro-
grams of size 131–198 KLOC. We also report upon a user
study involving nine users who employ Eugene to guide an
information-flow analysis on three Java micro-benchmarks.
In our experiments, Eugene significantly reduces misclassified reports upon providing limited amounts of feedback.</p>
</td>
	<td>program analysis </td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/karpathy2015visualizing/">Visualizing and Understanding Recurrent Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Visualizing and Understanding Recurrent Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Visualizing and Understanding Recurrent Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Visualizing%20and%20Understanding%20Recurrent%20Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>A. Karpathy, J. Johnson, L. Fei-Fei</td>
	<td>arXiv preprint arXiv:1506.02078</td>
	<td><p>Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as a result of successful
applications in a wide range of machine learning problems that involve sequential
data. However, while LSTMs provide exceptional results in practice, the source
of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge
this gap by providing an analysis of their representations, predictions and error
types. In particular, our experiments reveal the existence of interpretable cells that
keep track of long-range dependencies such as line lengths, quotes and brackets.
Moreover, our comparative analysis with finite horizon n-gram models traces the
source of the LSTM improvements to long-range structural dependencies. Finally,
we provide analysis of the remaining errors and suggests areas for further study.</p>

</td>
	<td>language model generation </td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/piech2015learning/">Learning Program Embeddings to Propagate Feedback on Student Code</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning Program Embeddings to Propagate Feedback on Student Code' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning Program Embeddings to Propagate Feedback on Student Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20Program%20Embeddings%20to%20Propagate%20Feedback%20on%20Student%20Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>C. Piech, J. Huang, A. Nguyen, M. Phulsuksombati, M, Sahami, L. Guibas</td>
	<td>ICML</td>
	<td><p>Providing feedback, both assessing final work
and giving hints to stuck students, is difficult
for open-ended assignments in massive online
classes which can range from thousands to millions of students. We introduce a neural network
method to encode programs as a linear mapping
from an embedded precondition space to an embedded postcondition space and propose an algorithm for feedback at scale using these linear maps as features. We apply our algorithm
to assessments from the Code.org Hour of Code
and Stanford University’s CS1 course, where we
propagate human comments on student assignments to orders of magnitude more submissions.</p>
</td>
	<td>representation repair education </td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/nguyen2015graph/">Graph-based Statistical Language Model for Code</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Graph-based Statistical Language Model for Code' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Graph-based Statistical Language Model for Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Graph-based%20Statistical%20Language%20Model%20for%20Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>A.T. Nguyen, T.N. Nguyen</td>
	<td>ICSE</td>
	<td><p>n-gram statistical language model has been successfully applied to capture programming patterns to support code
completion and suggestion. However, the approaches using n-gram face challenges in capturing the patterns at higher levels
of abstraction due to the mismatch between the sequence nature
in n-grams and the structure nature of syntax and semantics
in source code. This paper presents GraLan, a graph-based
statistical language model and its application in code suggestion. GraLan can learn from a source code corpus and compute
the appearance probabilities of any graphs given the observed
(sub)graphs. We use GraLan to develop an API suggestion
engine and an AST-based language model, ASTLan. ASTLan
supports the suggestion of the next valid syntactic template
and the detection of common syntactic templates. Our empirical
evaluation on a large corpus of open-source projects has shown
that our engine is more accurate in API code suggestion than
the state-of-the-art approaches, and in 75% of the cases, it can
correctly suggest the API with only five candidates. ASTLan also
has high accuracy in suggesting the next syntactic template and
is able to detect many useful and common syntactic templates.</p>
</td>
	<td>representation language model autocomplete </td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/ray2015naturalness/">On the “Naturalness” of Buggy Code</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=On the “Naturalness” of Buggy Code' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=On the “Naturalness” of Buggy Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=On%20the%20%E2%80%9CNaturalness%E2%80%9D%20of%20Buggy%20Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>B. Ray, V. Hellendoorn, S. Godhane, Z. Tu, A. Bacchelli, P. Devanbu</td>
	<td>ICSE</td>
	<td><p>Real software, the kind working programmers produce by the kLOC
to solve real-world problems, tends to be “natural”, like speech or
natural language; it tends to be highly repetitive and predictable.
Researchers have captured this naturalness of software through statistical models and used them to good effect in suggestion engines,
porting tools, coding standards checkers, and idiom miners. This
suggests that code that appears improbable, or surprising, to a good
statistical language model is “unnatural” in some sense, and thus
possibly suspicious. In this paper, we investigate this hypothesis. We consider a large corpus of bug fix commits (ca. 8,296),
from 10 different Java projects, and we focus on its language statistics, evaluating the naturalness of buggy code and the corresponding fixes. We find that code with bugs tends to be more entropic
(i.e. unnatural), becoming less so as bugs are fixed. Focusing on
highly entropic lines is similar in cost-effectiveness to some well-known static bug finders (PMD, FindBugs) and ordering warnings
from these bug finders using an entropy measure improves the cost-effectiveness of inspecting code implicated in warnings. This suggests that entropy may be a valid language-independent and simple
way to complement the effectiveness of PMD or FindBugs, and
that search-based bug-fixing methods may benefit from using entropy both for fault-localization and searching for fixes.</p>

</td>
	<td>defect </td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/allamanis2015suggesting/">Suggesting Accurate Method and Class Names</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Suggesting Accurate Method and Class Names' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Suggesting Accurate Method and Class Names' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Suggesting%20Accurate%20Method%20and%20Class%20Names' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>M. Allamanis, E. T. Barr, C. Bird, C. Sutton</td>
	<td>FSE</td>
	<td><p>Descriptive names are a vital part of readable, and hence maintainable, code. Recent progress on automatically suggesting names for local variables tantalizes with the prospect of replicating that success with method and class names.  However, suggesting names for methods and classes is much more difficult. This is because good method and class names need to be functionally descriptive, but suggesting such names requires that the model goes beyond local context. We introduce a neural probabilistic language model for source code that is specifically designed for the method naming problem. Our model learns which names are semantically similar by assigning them to locations, called embeddings, in a high-dimensional continuous space, in such a way that names with similar embeddings tend to be used in similar contexts. These embeddings seem to contain semantic information about tokens, even though they are learned only from statistical co-occurrences of tokens.  Furthermore, we introduce a variant of our model
that is, to our knowledge, the first that can propose neologisms, names that have not appeared in the training corpus. We obtain state of the art results on the method, class, and even the simpler variable naming tasks. More broadly, the continuous embeddings that are learned by our model have the potential for wide application within software engineering.</p>

</td>
	<td>naming </td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/allamanis2015bimodal/">A Bimodal Modelling of Source Code and Natural Language</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Bimodal Modelling of Source Code and Natural Language' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Bimodal Modelling of Source Code and Natural Language' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=A%20Bimodal%20Modelling%20of%20Source%20Code%20and%20Natural%20Language' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>M. Allamanis, D. Tarlow, A. D. Gordon, Y. Wei</td>
	<td>ICML</td>
	<td><p>We consider the problem of building probabilistic models that jointly 
model short natural language utterances and source code snippets. The
aim is to bring together recent work on statistical modelling of source
code and work on bimodal models of images and natural language. The
resulting models are useful for a variety of tasks that involve natural
language and source code. We demonstrate their performance on two
retrieval tasks: retrieving source code snippets given a natural language
query, and retrieving natural language descriptions given a source code
query (i.e., source code captioning). Experiments show there to be
promise in this direction, and that modelling the structure of source
code improves performance.</p>
</td>
	<td>search grammar AST bimodal </td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/cerulo2015irish/">Irish: A Hidden Markov Model to detect coded information islands in free text</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Irish: A Hidden Markov Model to detect coded information islands in free text' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Irish: A Hidden Markov Model to detect coded information islands in free text' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Irish:%20A%20Hidden%20Markov%20Model%20to%20detect%20coded%20information%20islands%20in%20free%20text' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>L. Cerulo, M. Di Penta, A. Bacchelli, M, Ceccarelli, G. Canfora</td>
	<td>Science of Computer Programming</td>
	<td><p>Developers’ communication, as contained in emails, issue trackers, and forums, is a precious source of information to support the development process. For example, it can
be used to capture knowledge about development practice or about a software project itself. Thus, extracting the content of developers’ communication can be useful to support
several software engineering tasks, such as program comprehension, source code analysis, and software analytics. However, automating the extraction process is challenging, due to the unstructured nature of free text, which mixes different coding languages (e.g., source code, stack dumps, and log traces) with natural language parts.</p>

<p>We conduct an extensive evaluation of Irish (InfoRmation ISlands Hmm), an approach we proposed to extract islands of coded information from free text at token granularity, with respect to the state of art approaches based on island parsing or island parsing combined with machine learners. The evaluation considers a wide set of natural language documents (e.g., textbooks, forum discussions, and development emails) taken from different contexts and encompassing different coding languages. Results indicate an F-measure of Irish between 74% and 99%; this is in line with existing approaches which, differently from Irish, require specific expertise for the definition of regular expressions or grammars.</p>

</td>
	<td>information extraction </td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/aggarwal2015using/">Using Machine Translation for Converting Python 2 to Python 3 Code</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Using Machine Translation for Converting Python 2 to Python 3 Code' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Using Machine Translation for Converting Python 2 to Python 3 Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Using%20Machine%20Translation%20for%20Converting%20Python%202%20to%20Python%203%20Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>K. Aggarwal, M. Salameh, and A. Hindle</td>
	<td></td>
	<td><p>In this paper, we have tried to use Statistical machine translation in order to convert Python 2 code to Python 3 code. We use data from two projects and achieve a high BLEU score. We also investigate the cross-project training and testing to analyze the errors so as to ascertain differences with previous case. We have described a pilot study on modeling programming languages as natural language to build translation models on the lines of natural languages. This can be further worked on to translate between versions of a programming language or cross-programming-languages code translation.</p>
</td>
	<td>migration </td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/movshovitz2015kb/">KB-LDA: Jointly Learning a Knowledge Base of Hierarchy, Relations, and Facts</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=KB-LDA: Jointly Learning a Knowledge Base of Hierarchy, Relations, and Facts' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=KB-LDA: Jointly Learning a Knowledge Base of Hierarchy, Relations, and Facts' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=KB-LDA:%20Jointly%20Learning%20a%20Knowledge%20Base%20of%20Hierarchy,%20Relations,%20and%20Facts' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>D. Movshovitz-Attias, W. W. Cohen</td>
	<td>ACL</td>
	<td><p>Many existing knowledge bases (KBs), including Freebase, Yago, and NELL, rely
on a fixed ontology, given as an input
to the system, which defines the data to
be cataloged in the KB, i.e., a hierarchy of categories and relations between
them. The system then extracts facts that
match the predefined ontology. We propose an unsupervised model that jointly
learns a latent ontological structure of an
input corpus, and identifies facts from the
corpus that match the learned structure.
Our approach combines mixed membership stochastic block models and topic
models to infer a structure by jointly modeling text, a latent concept hierarchy, and
latent semantic relationships among the
entities mentioned in the text. As a case
study, we apply the model to a corpus
of Web documents from the software domain, and evaluate the accuracy of the various components of the learned ontology.</p>
</td>
	<td>pattern mining </td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/franks2015cacheca/">CACHECA: A Cache Language Model Based Code Suggestion Tool</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=CACHECA: A Cache Language Model Based Code Suggestion Tool' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=CACHECA: A Cache Language Model Based Code Suggestion Tool' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=CACHECA:%20A%20Cache%20Language%20Model%20Based%20Code%20Suggestion%20Tool' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>C. Franks, Z. Tu, P. Devanbu, V. Hellendoorn</td>
	<td>ICSE</td>
	<td><p>Nearly every Integrated Development Environment includes a form of code completion. The suggested completions (“suggestions”) are typically based on information available at compile time, such as type signatures and variables in scope. A statistical approach, based on estimated models of code patterns in large code corpora, has been demonstrated to be effective at predicting tokens given a context. In this demo, we present CACHECA, an Eclipse plugin that combines the native suggestions with a statistical suggestion regime. We demonstrate that a combination of the two approaches more than doubles Eclipse’s suggestion accuracy. A video demonstration is available at <a href="https://www.youtube.com/watch?v=3INk0N3JNtc">https://www.youtube.com/watch?v=3INk0N3JNtc</a>.</p>
</td>
	<td>language model </td>
</tr>



<tr>
	<td>2014</td>
	<td><a href="/publications/tu2014localness/">On the Localness of Software</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=On the Localness of Software' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=On the Localness of Software' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=On%20the%20Localness%20of%20Software' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>Z. Tu, Z. Su, P. Devanbu</td>
	<td>FSE</td>
	<td><p>The n-gram language model, which has its roots in statistical natural
language processing, has been shown to successfully capture the
repetitive and predictable regularities (“naturalness”) of source code,
and help with tasks such as code suggestion, porting, and designing
assistive coding devices. However, we show in this paper that this
natural-language-based model fails to exploit a special property of
source code: localness. We find that human-written programs are
localized: they have useful local regularities that can be captured
and exploited. We introduce a novel cache language model that
consists of both an n-gram and an added “cache” component to
exploit localness. We show empirically that the additional cache
component greatly improves the n-gram approach by capturing
the localness of software, as measured by both cross-entropy and
suggestion accuracy. Our model’s suggestion accuracy is actually
comparable to a state-of-the-art, semantically augmented language
model; but it is simpler and easier to implement. Our cache language
model requires nothing beyond lexicalization, and thus is applicable
to all programming languages.</p>
</td>
	<td>language model </td>
</tr>

<tr>
	<td>2014</td>
	<td><a href="/publications/zaremba2014learning/">Learning to Execute</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning to Execute' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning to Execute' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20to%20Execute' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>W. Zaremba, I. Sutskever</td>
	<td>ArXiV 1410.4615</td>
	<td><p>Recurrent Neural Networks (RNNs) with Long Short-Term Memory units (LSTM) are widely used because they are expressive and are easy to train. Our interest lies in empirically evaluating the expressiveness and the learnability of LSTMs in the sequence-to-sequence regime by training them to evaluate short computer programs, a domain that has traditionally been seen as too complex for neural networks. We consider a simple class of programs that can be evaluated with a single left-to-right pass using constant memory. Our main result is that LSTMs can learn to map the character-level representations of such programs to their correct outputs. Notably, it was necessary to use curriculum learning, and while conventional curriculum learning proved ineffective, we developed a new variant of curriculum learning that improved our networks’ performance in all experimental conditions. The improved curriculum had a dramatic impact on an addition problem, making it possible to train an LSTM to add two 9-digit numbers with 99% accuracy.</p>
</td>
	<td>representation </td>
</tr>

<tr>
	<td>2014</td>
	<td><a href="/publications/mou2014building/">Building Program Vector Representations for Deep Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Building Program Vector Representations for Deep Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Building Program Vector Representations for Deep Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Building%20Program%20Vector%20Representations%20for%20Deep%20Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>L. Mou, G. Li, Y. Liu, H. Peng, Z. Jin, Y. Xu, L. Zhang</td>
	<td>International Conference on Knowledge Science, Engineering and Management</td>
	<td><p>Deep learning has made significant breakthroughs
in various fields of artificial intelligence. Advantages of deep
learning include the ability to capture highly complicated features, weak involvement of human engineering, etc. However,
it is still virtually impossible to use deep learning to analyze
programs since deep architectures cannot be trained effectively
with pure back propagation. In this pioneering paper, we propose
the “coding criterion” to build program vector representations,
which are the premise of deep learning for program analysis. Our
representation learning approach directly makes deep learning a
reality in this new field. We evaluate the learned vector representations both qualitatively and quantitatively. We conclude, based
on the experiments, the coding criterion is successful in building
program representations. To evaluate whether deep learning
is beneficial for program analysis, we feed the representations
to deep neural networks, and achieve higher accuracy in the
program classification task than “shallow” methods, such as
logistic regression and the support vector machine. This result
confirms the feasibility of deep learning to analyze programs. It
also gives primary evidence of its success in this new field. We
believe deep learning will become an outstanding technique for
program analysis in the near future.</p>

</td>
	<td>representation AST </td>
</tr>

<tr>
	<td>2014</td>
	<td><a href="/publications/hsiao2014using/">Using Web Corpus Statistics for Program Analysis</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Using Web Corpus Statistics for Program Analysis' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Using Web Corpus Statistics for Program Analysis' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Using%20Web%20Corpus%20Statistics%20for%20Program%20Analysis' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>C. Hsiao, M. Cafarella, S. Narayanasamy</td>
	<td>OOPSLA</td>
	<td><p>Several program analysis tools—such as plagiarism detection and bug finding—rely on knowing a piece of code’s
relative semantic importance. For example, a plagiarism detector should not bother reporting two programs that have
an identical simple loop counter test, but should report programs that share more distinctive code. Traditional program
analysis techniques (e.g., finding data and control dependencies) are useful, but do not say how surprising or common
a line of code is. Natural language processing researchers
have encountered a similar problem and addressed it using
an n-gram model of text frequency, derived from statistics
computed over text corpora.</p>

<p>We propose and compute an n-gram model for programming languages, computed over a corpus of 2.8 million
JavaScript programs we downloaded from the Web. In contrast to previous techniques, we describe a code n-gram as
a subgraph of the program dependence graph that contains
all nodes and edges reachable in n steps from the statement.
We can count n-grams in a program and count the frequency
of n-grams in the corpus, enabling us to compute tf-idf-style
measures that capture the differing importance of different
lines of code. We demonstrate the power of this approach by
implementing a plagiarism detector with accuracy that beats
previous techniques, and a bug-finding tool that discovered
over a dozen previously unknown bugs in a collection of real
deployed programs.</p>
</td>
	<td>defect </td>
</tr>

<tr>
	<td>2014</td>
	<td><a href="/publications/srikant2014system/">A system to grade computer programming skills using machine learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A system to grade computer programming skills using machine learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A system to grade computer programming skills using machine learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=A%20system%20to%20grade%20computer%20programming%20skills%20using%20machine%20learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>S. Srikant, V. Aggarwal</td>
	<td>KDD</td>
	<td><p>The automatic evaluation of computer programs is a nascent area of research with a potential for large-scale impact. Extant program assessment systems score mostly based on the number of test-cases passed, providing no insight into the competency of the programmer. In this paper, we present a system to grade computer programs automatically. In addition to grading a program on its programming practices and complexity, the key kernel of the system is a machine-learning based algorithm which determines closeness of the logic of the given program to a correct program. This algorithm uses a set of highly-informative features, derived from the abstract representations of a given program, that capture the program’s functionality. These features are then used to learn a model to grade the programs, which are built against evaluations done by experts. We show that the regression models provide much better grading than the ubiquitous test-case-pass based grading and rivals the grading accuracy of other open-response problems such as essay grading . We also show that our novel features add significant value over and above basic keyword/expression count features. In addition to this, we propose a novel way of posing computer-program grading as a one-class modeling problem and report encouraging preliminary results. We show the value of the system through a case study in a real-world industrial deployment. To the best of the authors’ knowledge, this is the first time a system using machine learning has been developed and used for grading programs. The work is timely with regard to the recent boom in Massively Online Open Courseware (MOOCs), which promises to produce a significant amount of hand-graded digitized data.</p>
</td>
	<td>education </td>
</tr>

<tr>
	<td>2014</td>
	<td><a href="/publications/maddison2014structured/">Structured Generative Models of Natural Source Code</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Structured Generative Models of Natural Source Code' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Structured Generative Models of Natural Source Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Structured%20Generative%20Models%20of%20Natural%20Source%20Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>C.J. Maddison, D. Tarlow</td>
	<td>ICML</td>
	<td><p>We study the problem of building generative
models of natural source code (NSC); that is,
source code written by humans and meant to
be understood by humans. Our primary con-
tribution is to describe new generative models
that are tailored to NSC. The models are based
on probabilistic context free grammars (PCFGs)
and neuro-probabilistic language models (Mnih
&amp; Teh, 2012), which are extended to incorporate
additional source code-specific structure. These
models can be efficiently trained on a corpus
of source code and outperform a variety of less
structured baselines in terms of predictive log
likelihoods on held-out data.</p>

</td>
	<td>language model generation grammar AST </td>
</tr>

<tr>
	<td>2014</td>
	<td><a href="/publications/nguyen2015divide/">Divide-and-Conquer Approach for Multi-phase Statistical Migration for Source Code</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Divide-and-Conquer Approach for Multi-phase Statistical Migration for Source Code' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Divide-and-Conquer Approach for Multi-phase Statistical Migration for Source Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Divide-and-Conquer%20Approach%20for%20Multi-phase%20Statistical%20Migration%20for%20Source%20Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>A.T. Nguyen, T.T. Nguyen, T.N. Nguyen</td>
	<td>ASE</td>
	<td><p>Prior research shows that directly applying phrase-based SMT on lexical tokens to migrate Java to C# produces
much semantically incorrect code. A key limitation is the use of
sequences in phrase-based SMT to model and translate source
code with well-formed structures. We propose mppSMT, a divideand-conquer technique to address that with novel training and migration algorithms using phrase-based SMT in three phases. First,
mppSMT treats a program as a sequence of syntactic units and
maps/translates such sequences in two languages to one another.
Second, with a syntax-directed fashion, it deals with the tokens
within syntactic units by encoding them with semantic symbols to
represent their data and token types. This encoding via semantic
symbols helps better migration of API usages. Third, the lexical
tokens corresponding to each sememe are mapped or migrated.
The resulting sequences of tokens are merged together to form
the final migrated code. Such divide-and-conquer and syntax-direction strategies enable phrase-based SMT to adapt well to
syntactical structures in source code, thus, improving migration
accuracy. Our empirical evaluation on several real-world systems
shows that 84.8–97.9% and 70–83% of the migrated methods are
syntactically and semantically correct, respectively. 26.3–51.2%
of total migrated methods are exactly matched to the human-written C# code in the oracle. Compared to Java2CSharp, a rule-based migration tool, it achieves higher semantic accuracy from
6.6–57.7% relatively. Importantly, it does not require manual
labeling for training data or manual definition of rules.</p>
</td>
	<td>migration </td>
</tr>

<tr>
	<td>2014</td>
	<td><a href="/publications/campbell2014syntax/">Syntax Errors Just Aren’t Natural: Improving Error Reporting with Language Models</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Syntax Errors Just Aren’t Natural: Improving Error Reporting with Language Models' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Syntax Errors Just Aren’t Natural: Improving Error Reporting with Language Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Syntax%20Errors%20Just%20Aren%E2%80%99t%20Natural:%20Improving%20Error%20Reporting%20with%20Language%20Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>J. C. Campbell, A. Hindle, J. N. Amaral</td>
	<td>MSR</td>
	<td><p>A frustrating aspect of software development is that compiler error messages often fail to locate the actual cause of a syntax error. An errant semicolon or brace can result in
many errors reported throughout the file. We seek to find the actual source of these syntax errors by relying on the consistency of software: valid source code is usually repetitive and unsurprising. We exploit this consistency by constructing a simple N-gram language model of lexed source code tokens. We implemented an automatic Java syntax-error locator using the corpus of the project itself and evaluated its performance on mutated source code from several projects. Our tool, trained on the past versions of a project, can effectively augment the syntax error locations produced by the native compiler. Thus we provide a methodology and tool that exploits the naturalness of software source code to detect syntax errors alongside the parser.</p>
</td>
	<td>repair language model </td>
</tr>

<tr>
	<td>2014</td>
	<td><a href="/publications/nguyen2014statistical/">Statistical Learning Approach for Mining API Usage Mappings for Code Migration</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Statistical Learning Approach for Mining API Usage Mappings for Code Migration' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Statistical Learning Approach for Mining API Usage Mappings for Code Migration' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Statistical%20Learning%20Approach%20for%20Mining%20API%20Usage%20Mappings%20for%20Code%20Migration' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>A.T. Nguyen, H.A. Nguyen, T.T. Nguyen, T.N. Nguyen</td>
	<td>ASE</td>
	<td><p>The same software product nowadays could appear in multiple platforms and devices. To address business needs, software companies
develop a software product in a programming language and then
migrate it to another one. To support that process, semi-automatic
migration tools have been proposed. However, they require users
to manually define the mappings between the respective APIs of
the libraries used in two languages. To reduce such manual effort,
we introduce StaMiner, a novel data-driven approach that statistically learns the mappings between APIs from the corpus of the
corresponding client code of the APIs in two languages Java and
C#. Instead of using heuristics on the textual or structural similarity
between APIs in two languages to map API methods and classes
as in existing mining approaches, StaMiner is based on a statistical
model that learns the mappings in such a corpus and provides mappings for APIs with all possible arities. Our empirical evaluation
on several projects shows that StaMiner can detect API usage mappings with higher accuracy than a state-of-the-art approach. With
the resulting API mappings mined by StaMiner, Java2CSharp, an
existing migration tool, could achieve a higher level of accuracy.</p>
</td>
	<td>migration API </td>
</tr>

<tr>
	<td>2014</td>
	<td><a href="/publications/karaivanov2014phrase/">Phrase-Based Statistical Translation of Programming Languages</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Phrase-Based Statistical Translation of Programming Languages' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Phrase-Based Statistical Translation of Programming Languages' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Phrase-Based%20Statistical%20Translation%20of%20Programming%20Languages' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>S. Karaivanov, V. Raychev, M. Vechev</td>
	<td>Onward</td>
	<td><p>Phrase-based statistical machine translation approaches have been
highly successful in translating between natural languages and are
heavily used by commercial systems (e.g. Google Translate).</p>

<p>The main objective of this work is to investigate the applicability of
these approaches for translating between programming languages.
Towards that, we investigated several variants of the phrase-based
translation approach: i) a direct application of the approach to
programming languages, ii) a novel modification of the approach
to incorporate the grammatical structure of the target programming
language (so to avoid generating target programs which do not
parse), and iii) a combination of ii) with custom rules added to
improve the quality of the translation.</p>

<p>To experiment with the above systems, we investigated machine
translation from C# to Java. For the training, which takes about
60 hours, we used a parallel corpus of 20, 499 C#-to-Java method
translations. We then evaluated each of the three systems above by
translating 1,000 C# methods. Our experimental results indicate
that with the most advanced system, about 60% of the translated
methods compile (the top ranked) and out of a random sample of 50
correctly compiled methods, 68% (34 methods) were semantically
equivalent to the reference solution.</p>
</td>
	<td>migration generation </td>
</tr>

<tr>
	<td>2014</td>
	<td><a href="/publications/allamanis2014mining/">Mining Idioms from Source Code</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Mining Idioms from Source Code' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Mining Idioms from Source Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Mining%20Idioms%20from%20Source%20Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>M. Allamanis, C. Sutton</td>
	<td>FSE</td>
	<td><p>We present the first method for automatically mining code idioms from a corpus of previously written, idiomatic software projects. We take the view that a code idiom is a syntactic fragment that recurs across projects and has a single semantic purpose. Idioms may have metavariables, such as the body of a for loop. Modern IDEs commonly provide facilities for manually defining idioms and inserting them on demand, but this does not help programmers to write idiomatic code in languages or using libraries with which they are unfamiliar. We present Haggis, a system for mining code idioms that builds on recent advanced techniques from statistical natural language processing, namely, nonparametric Bayesian probabilistic tree substitution grammars. We apply Haggis to several of the most popular open source projects from GitHub. We present a wide range of evidence that the resulting idioms are semantically meaningful, demonstrating that they do indeed recur across software projects and that they occur more frequently in illustrative code examples collected from a Q&amp;A site. Manual examination of the most common idioms indicate that they describe important program concepts, including object creation, exception handling, and resource management.</p>
</td>
	<td>pattern mining grammar AST </td>
</tr>

<tr>
	<td>2014</td>
	<td><a href="/publications/allamanis2014learning/">Learning Natural Coding Conventions</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning Natural Coding Conventions' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning Natural Coding Conventions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20Natural%20Coding%20Conventions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>M. Allamanis, E. T. Barr, C. Bird, C. Sutton</td>
	<td>FSE</td>
	<td><p>Every programmer has a characteristic style, ranging from preferences
about identifier naming to preferences about object relationships and
design patterns. Coding conventions define a consistent syntactic style,
fostering readability and hence maintainability. When collaborating,
programmers strive to obey a project’s coding conventions. However,
one third of reviews of changes contain feedback about coding conventions,
indicating that programmers do not always follow them and that project
members care deeply about adherence. Unfortunately, programmers are
often unaware of coding conventions because inferring them requires a
global view, one that aggregates the many local decisions programmers
make and identifies emergent consensus on style. We present Naturalize,
a framework that learns the style of a codebase, and suggests revisions
to improve stylistic consistency. Naturalize builds on recent work in
applying statistical natural language processing to source code. We
apply Naturalize to suggest natural identifier names and formatting
conventions. We present four tools focused on ensuring natural code
during development and release management, including code review.
Naturalize achieves 94% accuracy in its top suggestions for identifier
names. We used Naturalize to generate 18 patches for 5 open source
projects: 14 were accepted.</p>
</td>
	<td>naming language model style </td>
</tr>

<tr>
	<td>2014</td>
	<td><a href="/publications/raychev2014code/">Code Completion with Statistical Language Models</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Code Completion with Statistical Language Models' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Code Completion with Statistical Language Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Code%20Completion%20with%20Statistical%20Language%20Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>V. Raychev, M. Vechev, E. Yahav</td>
	<td>PLDI</td>
	<td><p>We address the problem of synthesizing code completions for programs using APIs. Given a program with holes, we synthesize completions for holes with the most likely sequences of method calls.</p>

<p>Our main idea is to reduce the problem of code completion to
a natural-language processing problem of predicting probabilities
of sentences. We design a simple and scalable static analysis that
extracts sequences of method calls from a large codebase, and
index these into a statistical language model. We then employ
the language model to find the highest ranked sentences, and use
them to synthesize a code completion. Our approach is able to
synthesize sequences of calls across multiple objects together with
their arguments.</p>

<p>Experiments show that our approach is fast and effective. Virtually all computed completions typecheck, and the desired completion appears in the top 3 results in 90% of the cases.</p>
</td>
	<td>language model autocomplete generation </td>
</tr>

<tr>
	<td>2014</td>
	<td><a href="/publications/gulwani2014nlyze/">NLyze: Interactive Programming by Natural Language for SpreadSheet Data Analysis and Manipulation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=NLyze: Interactive Programming by Natural Language for SpreadSheet Data Analysis and Manipulation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=NLyze: Interactive Programming by Natural Language for SpreadSheet Data Analysis and Manipulation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=NLyze:%20Interactive%20Programming%20by%20Natural%20Language%20for%20SpreadSheet%20Data%20Analysis%20and%20Manipulation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>S. Gulwani, M. Marron</td>
	<td>SIGMOD</td>
	<td><p>Millions of computer end users need to perform tasks over tabular spreadsheet data, yet lack the programming knowledge to do such tasks automatically. This paper describes
the design and implementation of a robust natural language
based interface to spreadsheet programming. Our methodology involves designing a typed domain-specific language
(DSL) that supports an expressive algebra of map, filter, reduce, join, and formatting capabilities at a level of abstraction appropriate for non-expert users. The key algorithmic
component of our methodology is a translation algorithm
for converting a natural language specification in the context of a given spreadsheet to a ranked set of likely programs
in the DSL. The translation algorithm leverages the spreadsheet spatial and temporal context to assign interpretations
to specifications with implicit references, and is thus robust
to a variety of ways in which end users can express the same
task. The translation algorithm builds over ideas from keyword programming and semantic parsing to achieve both
high precision and high recall. We implemented the system
as an Excel add-in called NLyze that supports a rich user
interaction model including annotating the user’s natural
language specification and explaining the synthesized DSL
programs by paraphrasing them into structured English. We
collected a total of 3570 English descriptions for 40 spreadsheet tasks and our system was able to generate the intended
interpretation as the top candidate for 94% (97% for the top
3) of those instances.</p>

</td>
	<td>generation bimodal synthesis </td>
</tr>



<tr>
	<td>2013</td>
	<td><a href="/publications/cerulo2013hidden/">A Hidden Markov Model to Detect Coded Information Islands in Free Text</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Hidden Markov Model to Detect Coded Information Islands in Free Text' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Hidden Markov Model to Detect Coded Information Islands in Free Text' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=A%20Hidden%20Markov%20Model%20to%20Detect%20Coded%20Information%20Islands%20in%20Free%20Text' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>L. Cerulo, M. Ceccarelli, M. Di Penta, G. Canfora</td>
	<td>SCAM</td>
	<td><p>Emails and issue reports capture useful knowledge about development practices, bug fixing, and change activities. Extracting such a content is challenging, due to the mix-up of
source code and natural language, unstructured text.</p>

<p>In this paper we introduce an approach, based on Hidden Markov Models (HMMs), to extract coded information islands, such as source code, stack traces, and patches, from free text at a token level of granularity. We train a HMM for each category of information contained in the text, and adopt the Viterbi algorithm to recognize whether the sequence of tokens — e.g., words, language keywords, numbers, parentheses, punctuation marks, etc. — observed in a text switches among those HMMs. Although our implementation focuses on extracting source code from emails, the approach could be easily extended to include in principle any text-interleaved language.</p>

<p>We evaluated our approach with respect to the state of art on a set of development emails and bug reports drawn from the software repositories of well known open source systems. Results indicate an accuracy between 82% and 99%, which is in line with existing approaches which, differently from ours, require the manual definition of regular expressions or parsers.</p>

</td>
	<td>information extraction </td>
</tr>

<tr>
	<td>2013</td>
	<td><a href="/publications/nguyen2013lexical/">Lexical Statistical Machine Translation for Language Migration</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Lexical Statistical Machine Translation for Language Migration' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Lexical Statistical Machine Translation for Language Migration' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Lexical%20Statistical%20Machine%20Translation%20for%20Language%20Migration' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>A. T. Nguyen, T. T. Nguyen, T. N. Nguyen</td>
	<td>FSE</td>
	<td><p>Prior research has shown that source code also exhibits naturalness, i.e. it is written by humans and is likely to be
repetitive. The researchers also showed that the n-gram language model is useful in predicting the next token in a source
file given a large corpus of existing source code. In this paper, we investigate how well statistical machine translation
(SMT) models for natural languages could help in migrating source code from one programming language to another.
We treat source code as a sequence of lexical tokens and
apply a phrase-based SMT model on the lexemes of those
tokens. Our empirical evaluation on migrating two Java
projects into C# showed that lexical, phrase-based SMT
could achieve high lexical translation accuracy ( BLEU from
81.3-82.6%). Users would have to manually edit only 11.9-15.8% of the total number of tokens in the resulting code to
correct it. However, a high percentage of total translation
methods (49.5-58.6%) is syntactically incorrect. Therefore,
our result calls for a more program-oriented SMT model that
is capable of better integrating the syntactic and semantic
information of a program to support language migration.</p>
</td>
	<td>migration API </td>
</tr>

<tr>
	<td>2013</td>
	<td><a href="/publications/nguyen2013study/">A Study of Repetitiveness of Code Changes in Software Evolution</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Study of Repetitiveness of Code Changes in Software Evolution' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Study of Repetitiveness of Code Changes in Software Evolution' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=A%20Study%20of%20Repetitiveness%20of%20Code%20Changes%20in%20Software%20Evolution' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>H.A. Nguyen, A.T. Nguyen, T.T. Nguyen, T.N. Nguyen, H. Rajan</td>
	<td>ASE</td>
	<td><p>In this paper, we present a large-scale study of
repetitiveness of code changes in software evolution. We collected
a large data set of 2,841 Java projects, with 1.7 billion source lines
of code (SLOC) at the latest revisions, 1.8 million code change
revisions (0.4 million fixes), 6.2 million changed files, and 2.5
billion changed SLOCs. A change is considered repeated within
or cross-project if it matches another change having occurred
in the history of the project or another project, respectively. We
report the following important findings. First, repetitiveness of
changes could be as high as 70–100% at small sizes and decreases
exponentially as size increases. Second, repetitiveness is higher
and more stable in the cross-project setting than in the project-within one. Third, fixing changes repeat similarly to general
changes. Importantly, learning code changes and recommending
them in software evolution is beneficial with accuracy for top-1
recommendation of over 30% and top-3 of nearly 35%. Repeated
fixing changes could also be useful for automatic program repair.</p>

</td>
	<td>edit </td>
</tr>

<tr>
	<td>2013</td>
	<td><a href="/publications/allamanis2013mining/">Mining Source Code Repositories at Massive Scale Using Language Modeling </a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Mining Source Code Repositories at Massive Scale Using Language Modeling ' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Mining Source Code Repositories at Massive Scale Using Language Modeling ' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Mining%20Source%20Code%20Repositories%20at%20Massive%20Scale%20Using%20Language%20Modeling%20' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>M. Allamanis, C. Sutton</td>
	<td>MSR</td>
	<td><p>The tens of thousands of high-quality open source software projects on the Internet raise the exciting possibility of studying software development by finding patterns across truly large source code repositories. This could enable new tools for developing code, encouraging reuse, and navigating large projects. In this paper, we build the first giga-token probabilistic language model of source code, based on 352 million lines of Java. This is 100 times the scale of the pioneering work by Hindle et al. The giga-token model is significantly better at the code suggestion task than previous models. More broadly, our approach provides a new “lens” for analyzing software projects, enabling new complexity metrics based on statistical analysis of large corpora. We call these metrics data-driven complexity metrics. We propose new metrics that measure the complexity of a code module and the topical centrality of a module to a software project. In particular, it is possible to distinguish reusable utility classes from classes that are part of a program’s core logic based solely on general information theoretic criteria.</p>
</td>
	<td>language model </td>
</tr>

<tr>
	<td>2013</td>
	<td><a href="/publications/kushman2013using/">Using Semantic Unification to Generate Regular Expressions from Natural Language</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Using Semantic Unification to Generate Regular Expressions from Natural Language' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Using Semantic Unification to Generate Regular Expressions from Natural Language' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Using%20Semantic%20Unification%20to%20Generate%20Regular%20Expressions%20from%20Natural%20Language' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>N. Kushman, R. Barzilay</td>
	<td>NAACL</td>
	<td><p>We consider the problem of translating natural language text queries into regular expressions which represent their meaning. The mismatch in the level of abstraction between the natural language representation and the regular expression representation make this a novel and challenging problem. However, a given regular expression can be written in many semantically equivalent forms, and we exploit this flexibility to facilitate translation by finding a form which more directly corresponds to the natural language. We evaluate our technique on a set of natural language queries and their associated regular expressions which we gathered from Amazon Mechanical Turk. Our model substantially outperforms a state-of-the-art semantic parsing baseline, yielding a 29% absolute improvement in accuracy.</p>
</td>
	<td>bimodal generation </td>
</tr>

<tr>
	<td>2013</td>
	<td><a href="/publications/movshovitz2013natural/">Natural Language Models for Predicting Programming Comments</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Natural Language Models for Predicting Programming Comments' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Natural Language Models for Predicting Programming Comments' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Natural%20Language%20Models%20for%20Predicting%20Programming%20Comments' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>D. Movshovitz-Attias, W.W. Cohen</td>
	<td>ACL</td>
	<td><p>Statistical language models have successfully been used to describe and analyze
natural language documents. Recent work
applying language models to programming languages is focused on the task
of predicting code, while mainly ignoring
the prediction of programmer comments.
In this work, we predict comments from
JAVA source files of open source projects,
using topic models and n-grams, and we
analyze the performance of the models
given varying amounts of background data
on the project being predicted. We evaluate models on their comment-completion
capability in a setting similar to code completion tools built into standard code
editors, and show that using a comment
completion tool can save up to 47% of the
comment typing.</p>

</td>
	<td>bimodal documentation summarization </td>
</tr>

<tr>
	<td>2013</td>
	<td><a href="/publications/omar2013structured/">Structured Statistical Syntax Tree Prediction</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Structured Statistical Syntax Tree Prediction' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Structured Statistical Syntax Tree Prediction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Structured%20Statistical%20Syntax%20Tree%20Prediction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>C. Omar</td>
	<td>SPLASH</td>
	<td><p>Statistical models of source code can be used to improve
code completion systems, assistive interfaces, and code
compression engines. We are developing a statistical model
where programs are represented as syntax trees, rather than
simply a stream of tokens. Our model, initially for the Java
language, combines corpus data with information about syntax, types and the program context. We tested this model
using open source code corpuses and find that our model
is significantly more accurate than the current state of the
art, providing initial evidence for our claim that combining
structural and statistical information is a fruitful strategy.</p>
</td>
	<td>language model AST </td>
</tr>

<tr>
	<td>2013</td>
	<td><a href="/publications/nguyen2013statistical/">A Statistical Semantic Language Model for Source Code</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Statistical Semantic Language Model for Source Code' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Statistical Semantic Language Model for Source Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=A%20Statistical%20Semantic%20Language%20Model%20for%20Source%20Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>T.T. Nguyen, A.T. Nguyen, H.A. Nguyen, T.N. Nguyen</td>
	<td>FSE</td>
	<td><p>Recent research has successfully applied the statistical n-gram language model to show that source code exhibits a
good level of repetition. The n-gram model is shown to have
good predictability in supporting code suggestion and completion. However, the state-of-the-art n-gram approach to
capture source code regularities/patterns is based only on
the lexical information in a local context of the code units.
To improve predictability, we introduce SLAMC, a novel statistical semantic language model for source code. It incorporates semantic information into code tokens and models the
regularities/patterns of such semantic annotations, called sememes, rather than their lexemes. It combines the local context in semantic n-grams with the global technical concerns/functionality into an n-gram topic model, together with pairwise associations of program elements. Based on SLAMC,
we developed a new code suggestion method, which is empirically evaluated on several projects to have relatively 18–68%
higher accuracy than the state-of-the-art approach.</p>

</td>
	<td>language model </td>
</tr>

<tr>
	<td>2013</td>
	<td><a href="/publications/menon2013machine/">A Machine Learning Framework for Programming by Example</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Machine Learning Framework for Programming by Example' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Machine Learning Framework for Programming by Example' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=A%20Machine%20Learning%20Framework%20for%20Programming%20by%20Example' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>A. K. Menon, O. Tamuz, S. Gulwani, B. Lampson, A.T. Kalai</td>
	<td>ICML</td>
	<td><p>Learning programs is a timely and interesting challenge. In Programming by Example
(PBE), a system attempts to infer a program
from input and output examples alone, by
searching for a composition of some set of
base functions. We show how machine learning can be used to speed up this seemingly
hopeless search problem, by learning weights
that relate textual features describing the
provided input-output examples to plausible
sub-components of a program. This generic
learning framework lets us address problems
beyond the scope of earlier PBE systems.
Experiments on a prototype implementation
show that learning improves search and ranking on a variety of text processing tasks found
on help forums.</p>
</td>
	<td>generation </td>
</tr>



<tr>
	<td>2012</td>
	<td><a href="/publications/hindle2012naturalness/">On the Naturalness of Software</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=On the Naturalness of Software' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=On the Naturalness of Software' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=On%20the%20Naturalness%20of%20Software' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>A. Hindle, E. T. Barr, Z. Su, M. Gabel, P. Devanbu</td>
	<td>ICSE</td>
	<td><p>Natural languages like English are rich, complex,
and powerful. The highly creative and graceful use of languages
like English and Tamil, by masters like Shakespeare and
Avvaiyar, can certainly delight and inspire. But in practice,
given cognitive constraints and the exigencies of daily life, most
human utterances are far simpler and much more repetitive
and predictable. In fact, these utterances can be very usefully
modeled using modern statistical methods. This fact has led
to the phenomenal success of statistical approaches to speech
recognition, natural language translation, question-answering,
and text mining and comprehension.</p>

<p>We begin with the conjecture that most software is also
natural, in the sense that it is created by humans at work,
with all the attendant constraints and limitations—and thus,
like natural language, it is also likely to be repetitive and
predictable. We then proceed to ask whether a) code can
be usefully modeled by statistical language models and b)
such models can be leveraged to support software engineers.
Using the widely adopted n-gram model, we provide empirical
evidence supportive of a positive answer to both these questions.
We show that code is also very repetitive, and in fact even more
so than natural languages. As an example use of the model,
we have developed a simple code completion engine for Java
that, despite its simplicity, already improves Eclipse’s built-in
completion capability. We conclude the paper by laying out a
vision for future research in this area.</p>

</td>
	<td>language model autocomplete </td>
</tr>



<tr>
	<td>2009</td>
	<td><a href="/publications/bruch2009learning/">Learning from Examples to Improve Code Completion Systems</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning from Examples to Improve Code Completion Systems' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning from Examples to Improve Code Completion Systems' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=Learning%20from%20Examples%20to%20Improve%20Code%20Completion%20Systems' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>M. Bruch, M. Monperrus, and M. Mezini</td>
	<td>ESEC/FSE</td>
	<td><p>The suggestions made by current IDE’s code completion features are based exclusively on static type system of the programming language. As a result, often proposals are made which are irrelevant for a particular working context. Also, these suggestions are ordered alphabetically rather than by their relevance in a particular context. In this paper, we present intelligent code completion systems that learn from existing code repositories. We have implemented three such systems, each using the information contained in
repositories in a different way. We perform a large-scale quantitative evaluation of these systems, integrate the best performing one into Eclipse, and evaluate the latter also by a user study. Our experiments give evidence that intelligent code completion systems which learn from examples significantly outperform mainstream code completion systems in terms of the relevance of their suggestions and thus have the potential to enhance developers’ productivity.</p>
</td>
	<td>autocomplete </td>
</tr>



<tr>
	<td>2007</td>
	<td><a href="/publications/kremenek2007factor/">A Factor Graph Model for Software Bug Finding</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Factor Graph Model for Software Bug Finding' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Factor Graph Model for Software Bug Finding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
			<a href='http://academic.microsoft.com/#/search?iq=A%20Factor%20Graph%20Model%20for%20Software%20Bug%20Finding' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
		</span>
	</td>
	<td>T. Kremenek, A.Y. Ng, D. Engler</td>
	<td>IJCAI</td>
	<td><p>Automatic tools for finding software errors require
knowledge of the rules a program must obey, or
“specifications,” before they can identify bugs. We
present a method that combines factor graphs and
static program analysis to automatically infer specifications directly from programs. We illustrate the
approach on inferring functions in C programs that
allocate and release resources, and evaluate the approach on three codebases: SDL, OpenSSH, and
the OS kernel for Mac OS X (XNU). The inferred
specifications are highly accurate and with them we
have discovered numerous bugs.</p>

</td>
	<td>program analysis </td>
</tr>


</tbody></table>

<script>
var datatable;

function searchTable() {
    var hash = decodeURIComponent(window.location.hash.substr(1));
    datatable.search(hash).draw();
}


$(document).ready( function () {
    datatable = $('#allPapers').DataTable({
		paging: false,
		"order": [[ 0, 'desc' ], [ 1, 'asc' ]],
		columnDefs: [
			{
				targets: [3, 4, 5],
				visible: false,
				searchable: true
			}]
		});
    searchTable();
});

$(window).on('hashchange', function() {
  searchTable();
});
</script>


    </div>

  </body>
</html>
