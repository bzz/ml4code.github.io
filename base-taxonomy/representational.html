<!DOCTYPE html>
<html lang="en-us">

  <head>
  <!-- Global Site Tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-107339008-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments)};
    gtag('js', new Date());
    gtag('config', 'UA-107339008-1');
  </script>

  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="keywords" content="machine learning, source code, big code, naturalness, software engineering, programming languages">

  <title>
    
      Representational Models &middot; Machine Learning for Big Code and Naturalness
    
  </title>

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="shortcut icon" href="/public/favicon.svg">
  <link rel="search" href="/public/opensearchdescription.xml" 
      type="application/opensearchdescription+xml" 
      title="ML4Code" />

  <script src="https://code.jquery.com/jquery-3.2.1.min.js"
  integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
  crossorigin="anonymous"></script>
  
  <link rel="stylesheet" type="text/css" href="//cdn.datatables.net/1.10.16/css/jquery.dataTables.min.css">
  <script type="text/javascript" charset="utf8" src="//cdn.datatables.net/1.10.16/js/jquery.dataTables.min.js"></script>
</head>


  <body class="theme-base-0d layout-reverse">

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Machine Learning for Big Code and Naturalness
        </a>
      </h1>
      <p class="lead">Research on machine learning for source code.</p>      
    </div>

  <nav class="sidebar-nav">
   <div class="sidebar-item"><p style="font-size: 12px">Search related work <input type='text' id='searchTarget' size="16"/> <button onClick="search();">Go</button></p></div>
   <a class="sidebar-nav-item" href="/papers.html">List of Papers</a>
   <a class="sidebar-nav-item" href="/tags.html">Papers by Tag</a>
   <a class="sidebar-nav-item" href="/tsne-viz.html">2D Map of Papers</a>

   <a class="sidebar-nav-item" href="/base-taxonomy/">Core Taxonomy</a>


  <a class="sidebar-nav-item" href="/resources.html">Resources, Courses &#38; Events</a>
  <a class="sidebar-nav-item" href="/contributing.html">Contributing</a>
  <a class="sidebar-nav-item" href="/contributors.html">Contributors</a>
  </nav>

  <div class="sidebar-item">
    <p style="font-size: 12px">Contact <a href="https://miltos.allamanis.com">Miltos Allamanis</a> about this survey or website.
    <span style="font-size: 9px">
      Made with <a href="https://jekyllrb.com">Jekyll</a> and <a href="https://github.com/poole/hyde">Hyde</a>.
    </span></p>
  </div>
</div></div>

<script>
$("#searchTarget").keydown(function (e) {	
  if (e.keyCode == 13) {
    search();
  }
});

function search() {
  try {
    ga('send', 'event', 'search', 'search', $("#searchTarget").val());
  } finally {
    window.location = "/papers.html#" + $("#searchTarget").val();
  }
}
</script>


    <div class="content container">
      <h2>Representational Models</h2>

<b>Representational models</b> take an abstract representation of 
code as input.  Example representations include token contexts or data flow.
The resulting model yields a conditional probability distribution over code
element properties, like the types of variables, and can predict them.


<table id="reprModelTable">
<thead><th>Name</th><th>Input Code Representation</th><th>Target</th><th>Intermediate Representation</th><th>Application</th><th>Abstract</th></thead><tbody>

    
    
      <tr>
          <td data-order="allamanis2015bimodal"><a href="/publications/allamanis2015bimodal">M. Allamanis, D. Tarlow, A. D. Gordon, Y. Wei, 2015.</a> A Bimodal Modelling of Source Code and Natural Language</td>
          <td>Natural language</td>
          <td>Language Model</td>
          <td>Distributed</td>
          <td>Code Search/Synthesis</td>
          <td><p>We consider the problem of building probabilistic models that jointly 
model short natural language utterances and source code snippets. The
aim is to bring together recent work on statistical modelling of source
code and work on bimodal models of images and natural language. The
resulting models are useful for a variety of tasks that involve natural
language and source code. We demonstrate their performance on two
retrieval tasks: retrieving source code snippets given a natural language
query, and retrieving natural language descriptions given a source code
query (i.e., source code captioning). Experiments show there to be
promise in this direction, and that modelling the structure of source
code improves performance.</p>
</td>
      </tr>

    
    
      <tr>
          <td data-order="allamanis2015suggesting"><a href="/publications/allamanis2015suggesting">M. Allamanis, E. T. Barr, C. Bird, C. Sutton, 2015.</a> Suggesting Accurate Method and Class Names</td>
          <td>Token Context</td>
          <td>Identifier Name</td>
          <td>Distributed</td>
          <td>Naming</td>
          <td><p>Descriptive names are a vital part of readable, and hence maintainable, code. Recent progress on automatically suggesting names for local variables tantalizes with the prospect of replicating that success with method and class names.  However, suggesting names for methods and classes is much more difficult. This is because good method and class names need to be functionally descriptive, but suggesting such names requires that the model goes beyond local context. We introduce a neural probabilistic language model for source code that is specifically designed for the method naming problem. Our model learns which names are semantically similar by assigning them to locations, called embeddings, in a high-dimensional continuous space, in such a way that names with similar embeddings tend to be used in similar contexts. These embeddings seem to contain semantic information about tokens, even though they are learned only from statistical co-occurrences of tokens.  Furthermore, we introduce a variant of our model
that is, to our knowledge, the first that can propose neologisms, names that have not appeared in the training corpus. We obtain state of the art results on the method, class, and even the simpler variable naming tasks. More broadly, the continuous embeddings that are learned by our model have the potential for wide application within software engineering.</p>

</td>
      </tr>

    
    
      <tr>
          <td data-order="allamanis2016convolutional"><a href="/publications/allamanis2016convolutional">M. Allamanis, H. Peng, C. Sutton, 2016.</a> A Convolutional Attention Network for Extreme Summarization of Source Code</td>
          <td>Tokens</td>
          <td>Method Name</td>
          <td>Distributed</td>
          <td>Naming</td>
          <td><p>Attention mechanisms in neural networks have proved useful for problems in which
the input and output do not have fixed dimension. Often there exist features that
are locally translation invariant and would be valuable for directing the model’s attention,
but previous attentional architectures are not constructed to learn such features specifically.
We introduce an attentional neural network that employs convolution on the input tokens to detect
local time-invariant and long-range topical attention features in a context-dependent way. We
apply this architecture to the problem of extreme summarization of source code snippets into short,
descriptive function name-like summaries. Using those features, the model sequentially generates a
summary by marginalizing over two attention mechanisms: one that predicts the next summary token based 
n the attention weights of the input tokens and another that is able to copy a code token as-is directly
into the summary. We demonstrate our convolutional attention neural network’s performance on 10 popular Java
projects showing that it achieves better performance compared to previous attentional mechanisms.</p>
</td>
      </tr>

    
    
      <tr>
          <td data-order="allamanis2017smartpaste"><a href="/publications/allamanis2017smartpaste">M. Allamanis, M. Brockscmidt, 2017.</a> SmartPaste: Learning to Adapt Source Code</td>
          <td>Data Flow</td>
          <td>Variable Allocation</td>
          <td>Distributed</td>
          <td>Contextualization</td>
          <td><p>Deep Neural Networks have been shown to succeed at a range of natural
language tasks such as machine translation and text summarization.
While tasks on source code (ie, formal languages) have been considered
recently, most work in this area does not attempt to capitalize on the
unique opportunities offered by its known syntax and structure. In this
work, we introduce SmartPaste, a first task that requires to use such
information. The task is a variant of the program repair problem that
requires to adapt a given (pasted) snippet of code to surrounding,
existing source code. As first solutions, we design a set of deep
neural models that learn to represent the context of each variable
location and variable usage in a data flow-sensitive way. Our
evaluation suggests that our models can learn to solve the SmartPaste
task in many cases, achieving 58.6% accuracy, while learning meaningful
representation of variable usages.</p>
</td>
      </tr>

    
    
      <tr>
          <td data-order="alon2018general"><a href="/publications/alon2018general">U. Alon, M. Zilberstein, O. Levy, E. Yahav, 2018.</a> A General Path-Based Representation for Predicting Program Properties</td>
          <td>AST Paths</td>
          <td>General-Purpose</td>
          <td>Localized+Distributed</td>
          <td>General-Purpose</td>
          <td><p>Predicting program properties such as names or expression types has a wide range of applications. It can ease the task of programming and increase programmer productivity. A major challenge when learning from programs is how to represent programs in a way that facilitates effective learning. 
We present a general path-based representation for learning from programs. Our representation is purely syntactic and extracted automatically. The main idea is to represent a program using paths in its abstract syntax tree (AST). This allows a learning model to leverage the structured nature of code rather than treating it as a flat sequence of tokens. 
We show that this representation is general and can: (i) cover different prediction tasks, (ii) drive different learning algorithms (for both generative and discriminative models), and (iii) work across different programming languages. 
We evaluate our approach on the tasks of predicting variable names, method names, and full types. We use our representation to drive both CRF-based and word2vec-based learning, for programs of four languages: JavaScript, Java, Python and C#. Our evaluation shows that our approach obtains better results than task-specific handcrafted representations across different tasks and programming languages.</p>
</td>
      </tr>

    
    
      <tr>
          <td data-order="bichsel2016statistical"><a href="/publications/bichsel2016statistical">B. Bichsel, V. Raychev, P. Tsankov, M. Vechev, 2016.</a> Statistical Deobfuscation of Android Applications</td>
          <td>Dependency Net</td>
          <td>Identifier Name</td>
          <td>CRF (GM)</td>
          <td>Deobfuscation</td>
          <td><p>This work presents a new approach for deobfuscating Android APKs based on probabilistic learning of large code bases (termed “Big Code”). The key idea is to learn a probabilistic model over thousands of non-obfuscated Android applications and to use this probabilistic model to deobfuscate new, unseen Android APKs. The concrete focus of the paper is on reversing layout obfuscation, a popular transformation which renames key program elements such as classes, packages, and methods, thus making it difficult to understand what the program does. Concretely, the paper: (i) phrases the layout deobfuscation problem of Android APKs as structured prediction in a probabilistic graphical model, (ii) instantiates this model with a rich set of features and constraints that capture the Android setting, ensuring both semantic equivalence and high prediction accuracy, and (iii) shows how to leverage powerful inference and learning algorithms to achieve overall precision and scalability of the probabilistic predictions.</p>

<p>We implemented our approach in a tool called DeGuard and used it to: (i) reverse the layout obfuscation performed by the popular ProGuard system on benign, open-source applications, (ii) predict third-party libraries imported by benign APKs (also obfuscated by ProGuard), and (iii) rename obfuscated program elements of Android malware. The experimental results indicate that DeGuard is practically effective: it recovers 79.1% of the program element names obfuscated with ProGuard, it predicts third-party libraries with accuracy of 91.3%, and it reveals string decoders and classes that handle sensitive data in Android malware.</p>

</td>
      </tr>

    
    
      <tr>
          <td data-order="bruch2009learning"><a href="/publications/bruch2009learning">M. Bruch, M. Monperrus, and M. Mezini, 2009.</a> Learning from Examples to Improve Code Completion Systems</td>
          <td>Partial Object Use</td>
          <td>Invoked Method</td>
          <td>Localized</td>
          <td>Code Completion</td>
          <td><p>The suggestions made by current IDE’s code completion features are based exclusively on static type system of the programming language. As a result, often proposals are made which are irrelevant for a particular working context. Also, these suggestions are ordered alphabetically rather than by their relevance in a particular context. In this paper, we present intelligent code completion systems that learn from existing code repositories. We have implemented three such systems, each using the information contained in
repositories in a different way. We perform a large-scale quantitative evaluation of these systems, integrate the best performing one into Eclipse, and evaluate the latter also by a user study. Our experiments give evidence that intelligent code completion systems which learn from examples significantly outperform mainstream code completion systems in terms of the relevance of their suggestions and thus have the potential to enhance developers’ productivity.</p>
</td>
      </tr>

    
    
      <tr>
          <td data-order="chae2016automatically"><a href="/publications/chae2016automatically">K. Chae, H. Oh, K. Heo, H. Yang, 2016.</a> Automatically generating features for learning program analysis heuristics</td>
          <td>Data Flow Graph</td>
          <td>Static Analysis</td>
          <td>Localized</td>
          <td>Program Analysis</td>
          <td><p>We present a technique for automatically generating features for data-driven program analyses. Recently data-driven approaches for building a program analysis have been proposed, which mine existing codebases and automatically learn heuristics for finding a cost-effective abstraction for a given analysis task. Such approaches reduce the burden of the analysis designers, but they do not remove it completely; they still leave the highly nontrivial task of designing so called features to the hands of the designers. Our technique automates this feature design process. The idea is to use programs as features after reducing and abstracting them. Our technique goes through selected program-query pairs in codebases, and it reduces and abstracts the program in each pair to a few lines of code, while ensuring that the analysis behaves similarly for the original and the new programs with respect to the query. Each reduced program serves as a boolean feature for program-query pairs. This feature evaluates to true for a given program-query pair when (as a program) it is included in the program part of the pair. We have implemented our approach for three real-world program analyses. Our experimental evaluation shows that these analyses with automatically-generated features perform comparably to those with manually crafted features.</p>
</td>
      </tr>

    
    
      <tr>
          <td data-order="corley2015exploring"><a href="/publications/corley2015exploring">C.S. Corley, K. Damevski, N.A. Kraft, 2015.</a> Exploring the Use of Deep Learning for Feature Location</td>
          <td>Tokens</td>
          <td>Feature Location</td>
          <td>Distributed</td>
          <td>Feature Location</td>
          <td><p>Deep learning models are a class of neural networks. Relative to n-gram models, deep learning models can capture more complex statistical patterns based on smaller training corpora. In this paper we explore the use of a particular deep learning model, document vectors (DVs), for feature location. DVs seem well suited to use with source code, because they both capture the influence of context on each term in a corpus and map terms into a continuous semantic space that encodes semantic relationships such as synonymy. We present preliminary results that show that a feature location technique (FLT) based on DVs can outperform an analogous FLT based on latent Dirichlet allocation (LDA) and then suggest several directions for future work on the use of deep learning models to improve developer effectiveness in feature location.</p>
</td>
      </tr>

    
    
      <tr>
          <td data-order="cummins2017end"><a href="/publications/cummins2017end">C. Cummins, P. Petoumenos, Z. Wang, H. Leather, 2017.</a> End-to-end Deep Learning of Optimization Heuristics</td>
          <td>Tokens</td>
          <td>Optimization Flags</td>
          <td>Distributed</td>
          <td>Optimization Heuristics</td>
          <td><p>Accurate automatic optimization heuristics are necessary for dealing with the complexity and diversity of modern hardware and software. Machine learning is a proven technique for learning such heuristics, but its success is bound by the quality of the features used. These features must be hand crafted by developers through a combination of expert domain knowledge and trial and error. This makes the quality of the final model directly dependent on the skill and available time of the system architect.</p>

<p>Our work introduces a better way for building heuristics. We develop a deep neural network that learns heuristics over raw code, entirely without using code features. The neural network simultaneously constructs appropriate representations of the code and learns how best to optimize, removing the need for manual feature creation. Further, we show that our neural nets can transfer learning from one optimization problem to another, improving the accuracy of new models, without the help of human experts.</p>

<p>We compare the effectiveness of our automatically generated heuristics against ones with features hand-picked by experts. We examine two challenging tasks: predicting optimal mapping for heterogeneous parallelism and GPU thread coarsening factors. In 89% of the cases, the quality of our fully automatic heuristics matches or surpasses that of state-of-the-art predictive models using hand-crafted features, providing on average 14% and 12% more performance with no human effort expended on designing features.</p>
</td>
      </tr>

    
    
      <tr>
          <td data-order="dam2016deep"><a href="/publications/dam2016deep">H. K. Dam, T. Tran, T. Pham, 2016.</a> A deep language model for software code</td>
          <td>Token Context</td>
          <td>LM (Tokens)</td>
          <td>Distributed</td>
          <td>---</td>
          <td><p>Existing language models such as n-grams for software code often fail to capture a long context where dependent code elements scatter far apart. In this paper, we propose a novel approach to build a language model for software code to address this particular issue. Our language model, partly inspired by human memory, is built upon the powerful deep learning-based Long Short Term Memory architecture that is capable of learning long-term dependencies which occur frequently in software code. Results from our intrinsic evaluation on a corpus of Java projects have demonstrated the effectiveness of our language model. This work contributes to realizing our vision for DeepSoft, an end-to-end, generic deep learning-based framework for modeling software and its development process.</p>
</td>
      </tr>

    
    
      <tr>
          <td data-order="gu2016deep"><a href="/publications/gu2016deep">X. Gu, H. Zhang, D. Zhang, S. Kim, 2016.</a> Deep API Learning</td>
          <td>Natural Language</td>
          <td>API Calls</td>
          <td>Distributed</td>
          <td>API Search</td>
          <td><p>Developers often wonder how to implement a certain functionality (e.g., how to parse XML files) using APIs. Obtaining an API usage sequence based on an API-related natural language query is very helpful in this regard. Given a query, existing approaches utilize information retrieval models to search for matching API sequences. These approaches treat queries and APIs as bag-of-words (i.e., keyword matching or word-to-word alignment) and lack a deep understanding of the semantics of the query.</p>

<p>We propose DeepAPI, a deep learning based approach to generate API usage sequences for a given natural language query. Instead of a bags-of-words assumption, it learns the
sequence of words in a query and the sequence of associated APIs. DeepAPI adapts a neural language model named RNN Encoder-Decoder. It encodes a word sequence (user query) into a fixed-length context vector, and generates an API sequence based on the context vector. We also augment the RNN Encoder-Decoder by considering the importance of individual APIs. We empirically evaluate our approach with more than 7 million annotated code snippets collected from GitHub. The results show that our approach generates largely accurate API sequences and outperforms the related approaches.</p>

</td>
      </tr>

    
    
      <tr>
          <td data-order="guo2017semantically"><a href="/publications/guo2017semantically">J. Guo, J. Cheng, J. Cleland-Huang, 2017.</a> Semantically enhanced software traceability using deep learning techniques</td>
          <td>Tokens</td>
          <td>Traceability link</td>
          <td>Distributed</td>
          <td>Traceability</td>
          <td><p>In most safety-critical domains the need for traceability is prescribed by certifying bodies. Trace links are generally created among requirements, design, source code, test cases and other artifacts; however, creating such links manually is time consuming and error prone. Automated solutions use information retrieval and machine learning techniques to generate trace links; however, current techniques fail to understand semantics of the software artifacts or to integrate domain knowledge into the tracing process and therefore tend to deliver imprecise and inaccurate results. In this paper, we present a solution that uses deep learning to incorporate requirements artifact semantics and domain knowledge into the tracing solution. We propose a tracing network architecture that utilizes Word Embedding and Recurrent Neural Network (RNN) models to generate trace links. Word embedding learns word vectors that represent knowledge of the domain corpus and RNN uses these word vectors to learn the sentence semantics of requirements artifacts. We trained 360 different configurations of the tracing network using existing trace links in the Positive Train Control domain and identified the Bidirectional Gated Recurrent Unit (BI-GRU) as the best model for the tracing task. BI-GRU significantly out-performed state-of-the-art tracing methods including the Vector Space Model and Latent Semantic Indexing.</p>
</td>
      </tr>

    
    
      <tr>
          <td data-order="gupta2017deepfix"><a href="/publications/gupta2017deepfix">R. Gupta, S. Pal, A. Kanade, S. Shevade, 2017.</a> DeepFix: Fixing Common C Language Errors by Deep Learning</td>
          <td>Tokens</td>
          <td>Code Fix</td>
          <td>Distributed</td>
          <td>Code Fixing</td>
          <td><p>The problem of automatically fixing programming errors is a
very active research topic in software engineering. This is a
challenging problem as fixing even a single error may require
analysis of the entire program. In practice, a number of errors
arise due to programmer’s inexperience with the programming language or lack of attention to detail. We call these
common programming errors. These are analogous to grammatical errors in natural languages. Compilers detect such errors, but their error messages are usually inaccurate. In this
work, we present an end-to-end solution, called DeepFix, that
can fix multiple such errors in a program without relying on
any external tool to locate or fix them. At the heart of DeepFix
is a multi-layered sequence-to-sequence neural network with
attention which is trained to predict erroneous program locations along with the required correct statements. On a set of
6971 erroneous C programs written by students for 93 programming tasks, DeepFix could fix 1881 (27%) programs
completely and 1338 (19%) programs partially.</p>
</td>
      </tr>

    
    
      <tr>
          <td data-order="hu2017codesum"><a href="/publications/hu2017codesum">X. Hu, Y. Wei, G. Li, Z. Jin, 2017.</a> CodeSum: Translate Program Language to Natural Language</td>
          <td>Linearized AST</td>
          <td>Natural Language</td>
          <td>Distributed</td>
          <td>Summarization</td>
          <td><p>During software maintenance, programmers spend a lot of time on code comprehension. Reading comments is an effective way for programmers to reduce the reading and navigating time when comprehending source code. Therefore, as a critical task in software engineering, code summarization aims to generate brief natural language descriptions for source code. In this paper, we propose a new code summarization model named CodeSum. CodeSum exploits the attention-based sequence-to-sequence (Seq2Seq) neural network with Structure-based Traversal (SBT) of Abstract Syntax Trees (AST). The AST sequences generated by SBT can better present the structure of ASTs and keep unambiguous. We conduct experiments on three large-scale corpora in different program languages, i.e., Java, C#, and SQL, in which Java corpus is our new proposed industry code extracted from Github. Experimental results show that our method CodeSum outperforms the state-of-the-art significantly.</p>
</td>
      </tr>

    
    
      <tr>
          <td data-order="iyer2016summarizing"><a href="/publications/iyer2016summarizing">S. Iyer, I. Konstas, A. Cheung, L. Zettlemoyer, 2016.</a> Summarizing Source Code using a Neural Attention Model</td>
          <td>Tokens</td>
          <td>Natural Language</td>
          <td>Distributed</td>
          <td>Summarization</td>
          <td><p>High quality source code is often paired
with high level summaries of the computation it performs, for example in code
documentation or in descriptions posted
in online forums. Such summaries are
extremely useful for applications such as
code search but are expensive to manually
author, hence only done for a small fraction of all code that is produced. In this
paper, we present the first completely data-driven approach for generating high level
summaries of source code. Our model,
CODE-NN , uses Long Short Term Memory (LSTM) networks with attention to
produce sentences that describe C# code
snippets and SQL queries. CODE-NN
is trained on a new corpus that is automatically collected from StackOverflow,
which we release. Experiments demonstrate strong performance on two tasks:
(1) code summarization, where we establish the first end-to-end learning results
and outperform strong baselines, and (2)
code retrieval, where our learned model
improves the state of the art on a recently
introduced C# benchmark by a large margin.</p>
</td>
      </tr>

    
    
      <tr>
          <td data-order="jiang2017automatically"><a href="/publications/jiang2017automatically">S. Jiang, A. Armaly, C. McMillan, 2017.</a> Automatically Generating Commit Messages from Diffs using Neural Machine Translation</td>
          <td>Tokens (Diff)</td>
          <td>Natural Language</td>
          <td>Distributed</td>
          <td>Commit Message</td>
          <td><p>Commit messages are a valuable resource in comprehension of software evolution, since they provide a record of changes such as feature additions and bug repairs. Unfortunately, programmers often neglect to write good commit messages. Different techniques have been proposed to help programmers by automatically writing these messages. These techniques are effective at describing what changed, but are often verbose and lack context for understanding the rationale behind a change. In contrast, humans write messages that are short and summarize the high level rationale. In this paper, we adapt Neural Machine Translation (NMT) to automatically “translate” diffs into commit messages. We trained an NMT algorithm using a corpus of diffs and human-written commit messages from the top 1k Github projects. We designed a filter to help ensure that we only trained the algorithm on higher-quality commit messages. Our evaluation uncovered a pattern in which the messages we generate tend to be either very high or very low quality. Therefore, we created a quality-assurance filter to detect cases in which we are unable to produce good messages, and return a warning instead.</p>
</td>
      </tr>

    
    
      <tr>
          <td data-order="koc2017learning"><a href="/publications/koc2017learning">U. Koc, P. Saadatpanah, J. S. Foster, A. A. Porter, 2017.</a> Learning a Classifier for False Positive Error Reports Emitted by Static Code Analysis Tools</td>
          <td>Bytecode</td>
          <td>False Positives</td>
          <td>Distributed</td>
          <td>Program Analysis</td>
          <td><p>The large scale and high complexity of modern software systems
make perfectly precise static code analysis (SCA) infeasible. Therefore SCA tools often over-approximate, so not to miss any real
problems. This, however, comes at the expense of raising false
alarms, which, in practice, reduces the usability of these tools.</p>

<p>To partially address this problem, we propose a novel learning
process whose goal is to discover program structures that cause
a given SCA tool to emit false error reports, and then to use this
information to predict whether a new error report is likely to be a
false positive as well. To do this, we first preprocess code to isolate
the locations that are related to the error report. Then, we apply
machine learning techniques to the preprocessed code to discover
correlations and to learn a classifier.</p>

<p>We evaluated this approach in an initial case study of a widely-used SCA tool for Java. Our results showed that for our dataset
we could accurately classify a large majority of false positive error
reports. Moreover, we identified some common coding patterns that
led to false positive errors. We believe that SCA developers may be
able to redesign their methods to address these patterns and reduce
false positive error reports.</p>
</td>
      </tr>

    
    
      <tr>
          <td data-order="kremenek2007factor"><a href="/publications/kremenek2007factor">T. Kremenek, A.Y. Ng, D. Engler, 2007.</a> A Factor Graph Model for Software Bug Finding</td>
          <td>Partial PDG</td>
          <td>Ownership</td>
          <td>Factor (GM)</td>
          <td>Pointer Ownership</td>
          <td><p>Automatic tools for finding software errors require
knowledge of the rules a program must obey, or
“specifications,” before they can identify bugs. We
present a method that combines factor graphs and
static program analysis to automatically infer specifications directly from programs. We illustrate the
approach on inferring functions in C programs that
allocate and release resources, and evaluate the approach on three codebases: SDL, OpenSSH, and
the OS kernel for Mac OS X (XNU). The inferred
specifications are highly accurate and with them we
have discovered numerous bugs.</p>

</td>
      </tr>

    
    
      <tr>
          <td data-order="levy2017learning"><a href="/publications/levy2017learning">D. Levy, L. Wolf, 2017.</a> Learning to Align the Source Code to the Compiled Object Code</td>
          <td>Statements</td>
          <td>Alignment</td>
          <td>Distributed</td>
          <td>Decompiling</td>
          <td><p>We propose a new neural network architecture
and use it for the task of statement-by-statement
alignment of source code and its compiled object code. Our architecture learns the alignment
between the two sequences – one being the translation of the other – by mapping each statement
to a context-dependent representation vector and
aligning such vectors using a grid of the two sequence domains. Our experiments include short
C functions, both artificial and human-written,
and show that our neural network architecture
is able to predict the alignment with high accuracy, outperforming known baselines. We also
demonstrate that our model is general and can
learn to solve graph problems such as the Traveling Salesman Problem.</p>
</td>
      </tr>

    
    
      <tr>
          <td data-order="li2015gated"><a href="/publications/li2015gated">, .</a> </td>
          <td>Memory Heap</td>
          <td>Separation Logic</td>
          <td>Distributed</td>
          <td>Verification</td>
          <td></td>
      </tr>

    
    
      <tr>
          <td data-order="loyola2017neural"><a href="/publications/loyola2017neural">P. Loyola, E. Marrese-Taylor, Y. Matsuo, 2017.</a> A Neural Architecture for Generating Natural Language Descriptions from Source Code Changes</td>
          <td>Tokens (Diff)</td>
          <td>Natural Language</td>
          <td>Distributed</td>
          <td>Explain code changes</td>
          <td><p>We propose a model to automatically describe changes introduced in the source code of a program using natural language. Our method receives as input a set of code commits, which contains both the modifications and message introduced by an user. These two modalities are used to train an encoder-decoder architecture. We evaluated our approach on twelve real world open source projects from four different programming languages. Quantitative and qualitative results showed that the proposed approach can generate feasible and semantically sound descriptions not only in standard in-project settings, but also in a cross-project setting.</p>
</td>
      </tr>

    
    
      <tr>
          <td data-order="maddison2014structured"><a href="/publications/maddison2014structured">C.J. Maddison, D. Tarlow, 2014.</a> Structured Generative Models of Natural Source Code</td>
          <td>LM AST Context</td>
          <td>Language Model</td>
          <td>Distributed</td>
          <td>---</td>
          <td><p>We study the problem of building generative
models of natural source code (NSC); that is,
source code written by humans and meant to
be understood by humans. Our primary con-
tribution is to describe new generative models
that are tailored to NSC. The models are based
on probabilistic context free grammars (PCFGs)
and neuro-probabilistic language models (Mnih
&amp; Teh, 2012), which are extended to incorporate
additional source code-specific structure. These
models can be efficiently trained on a corpus
of source code and outperform a variety of less
structured baselines in terms of predictive log
likelihoods on held-out data.</p>

</td>
      </tr>

    
    
      <tr>
          <td data-order="mangal2015user"><a href="/publications/mangal2015user">R. Mangal, X. Zhang, A. V. Nori, M. Naik, 2015.</a> A User-Guided Approach to Program Analysis</td>
          <td>Logic + Feedback</td>
          <td>Prob. Analysis</td>
          <td>MaxSAT</td>
          <td>Program Analysis</td>
          <td><p>Program analysis tools often produce undesirable output
due to various approximations. We present an approach
and a system Eugene that allows user feedback to guide
such approximations towards producing the desired output.
We formulate the problem of user-guided program analysis in terms of solving a combination of hard rules and soft
rules: hard rules capture soundness while soft rules capture
degrees of approximations and preferences of users. Our
technique solves the rules using an off-the-shelf solver in a
manner that is sound (satisfies all hard rules), optimal (maximally satisfies soft rules), and scales to real-world analy-
ses and programs. We evaluate Eugene on two different
analyses with labeled output on a suite of seven Java pro-
grams of size 131–198 KLOC. We also report upon a user
study involving nine users who employ Eugene to guide an
information-flow analysis on three Java micro-benchmarks.
In our experiments, Eugene significantly reduces misclassified reports upon providing limited amounts of feedback.</p>
</td>
      </tr>

    
    
      <tr>
          <td data-order="mou2016convolutional"><a href="/publications/mou2016convolutional">L. Mou, G. Li, L. Zhang, T. Wang, Z. Jin, 2016.</a> Convolutional Neural Networks over Tree Structures for Programming Language Processing</td>
          <td>Syntax</td>
          <td>Classification</td>
          <td>Distributed</td>
          <td>Task Classification</td>
          <td><p>Programming language processing (similar to natural language processing) is a hot research topic in the field of software engineering; it has also aroused growing interest in the
artificial intelligence community. However, different from a
natural language sentence, a program contains rich, explicit,
and complicated structural information. Hence, traditional
NLP models may be inappropriate for programs. In this paper, we propose a novel tree-based convolutional neural network (TBCNN) for programming language processing, in
which a convolution kernel is designed over programs’ abstract syntax trees to capture structural information. TBCNN
is a generic architecture for programming language processing; our experiments show its effectiveness in two different program analysis tasks: classifying programs according
to functionality, and detecting code snippets of certain patterns. TBCNN outperforms baseline methods, including several neural models for NLP.</p>
</td>
      </tr>

    
    
      <tr>
          <td data-order="movshovitz2013natural"><a href="/publications/movshovitz2013natural">D. Movshovitz-Attias, W.W. Cohen, 2013.</a> Natural Language Models for Predicting Programming Comments</td>
          <td>Tokens</td>
          <td>Code Comments</td>
          <td>Directed GM</td>
          <td>Comment Prediction</td>
          <td><p>Statistical language models have successfully been used to describe and analyze
natural language documents. Recent work
applying language models to programming languages is focused on the task
of predicting code, while mainly ignoring
the prediction of programmer comments.
In this work, we predict comments from
JAVA source files of open source projects,
using topic models and n-grams, and we
analyze the performance of the models
given varying amounts of background data
on the project being predicted. We evaluate models on their comment-completion
capability in a setting similar to code completion tools built into standard code
editors, and show that using a comment
completion tool can save up to 47% of the
comment typing.</p>

</td>
      </tr>

    
    
      <tr>
          <td data-order="nguyen2016mapping"><a href="/publications/nguyen2016mapping">T.D. Nguyen, A.T. Nguyen, T.N. Nguyen, 2016.</a> Mapping API Elements for Code Migration with Vector Representations</td>
          <td>API Calls</td>
          <td>API Calls</td>
          <td>Distributed</td>
          <td>Migration</td>
          <td>
</td>
      </tr>

    
    
      <tr>
          <td data-order="oh2015learning"><a href="/publications/oh2015learning">H. Oh, H. Yang, K, Yi, 2015.</a> Learning a Strategy for Adapting a Program Analysis via Bayesian Optimisation</td>
          <td>Features</td>
          <td>Analysis Params</td>
          <td>Static Analysis</td>
          <td>Program Analysis</td>
          <td><p>Building a cost-effective static analyser for real-world programs is still regarded an art. One key contributor to this
grim reputation is the difficulty in balancing the cost and the
precision of an analyser. An ideal analyser should be adap-
tive to a given analysis task, and avoid using techniques that
unnecessarily improve precision and increase analysis cost.
However, achieving this ideal is highly nontrivial, and it requires a large amount of engineering efforts.</p>

<p>In this paper we present a new approach for building
an adaptive static analyser. In our approach, the analyser
includes a sophisticated parameterised strategy that decides, for each part of a given program, whether to apply
a precision-improving technique to that part or not. We
present a method for learning a good parameter for such
a strategy from an existing codebase via Bayesian optimisation. The learnt strategy is then used for new, unseen programs. Using our approach, we developed partially flow-
and context-sensitive variants of a realistic C static analyser.
The experimental results demonstrate that using Bayesian
optimisation is crucial for learning from an existing codebase. Also, they show that among all program queries that
require flow- or context-sensitivity, our partially flow- and
context-sensitive analysis answers the 75% of them, while
increasing the analysis cost only by 3.3x of the baseline
flow- and context-insensitive analysis, rather than 40x or
more of the fully sensitive version.</p>
</td>
      </tr>

    
    
      <tr>
          <td data-order="omar2013structured"><a href="/publications/omar2013structured">C. Omar, 2013.</a> Structured Statistical Syntax Tree Prediction</td>
          <td>Syntactic Context</td>
          <td>Expressions</td>
          <td>Directed GM</td>
          <td>Code Completion</td>
          <td><p>Statistical models of source code can be used to improve
code completion systems, assistive interfaces, and code
compression engines. We are developing a statistical model
where programs are represented as syntax trees, rather than
simply a stream of tokens. Our model, initially for the Java
language, combines corpus data with information about syntax, types and the program context. We tested this model
using open source code corpuses and find that our model
is significantly more accurate than the current state of the
art, providing initial evidence for our claim that combining
structural and statistical information is a fruitful strategy.</p>
</td>
      </tr>

    
    
      <tr>
          <td data-order="piech2015learning"><a href="/publications/piech2015learning">C. Piech, J. Huang, A. Nguyen, M. Phulsuksombati, M, Sahami, L. Guibas, 2015.</a> Learning Program Embeddings to Propagate Feedback on Student Code</td>
          <td>Syntax + State</td>
          <td>Student Feedback</td>
          <td>Distributed</td>
          <td>Student Feedback</td>
          <td><p>Providing feedback, both assessing final work
and giving hints to stuck students, is difficult
for open-ended assignments in massive online
classes which can range from thousands to millions of students. We introduce a neural network
method to encode programs as a linear mapping
from an embedded precondition space to an embedded postcondition space and propose an algorithm for feedback at scale using these linear maps as features. We apply our algorithm
to assessments from the Code.org Hour of Code
and Stanford University’s CS1 course, where we
propagate human comments on student assignments to orders of magnitude more submissions.</p>
</td>
      </tr>

    
    
      <tr>
          <td data-order="proksch2015intelligent"><a href="/publications/proksch2015intelligent">S. Proksch, J. Lerch, M. Mezini, 2015.</a> Intelligent Code Completion with Bayesian Networks</td>
          <td>Inc. Object Usage</td>
          <td>Object Usage</td>
          <td>Directed GM</td>
          <td>Code Completion</td>
          <td><p>Code completion is an integral part of modern Integrated Development Environments (IDEs). Developers
often use it to explore Application Programming Interfaces (APIs). It is also useful to reduce the required
amount of typing and to help avoid typos. Traditional code completion systems propose all type-correct
methods to the developer. Such a list is often very long with many irrelevant items. More intelligent code
completion systems have been proposed in prior work to reduce the list of proposed methods to relevant
items.</p>

<p>This work extends one of these existing approaches, the Best Matching Neighbor (BMN) algorithm. We
introduce Bayesian networks as an alternative underlying model, use additional context information for
more precise recommendations, and apply clustering techniques to improve model sizes. We compare our
new approach, Pattern-based Bayesian Networks (PBN), to the existing BMN algorithm. We extend previously used evaluation methodologies and, in addition to prediction quality, we also evaluate model size and
inference speed.</p>

<p>Our results show that the additional context information we collect improves prediction quality, especially
for queries that do not contain method calls. We also show that PBN can obtain comparable prediction
quality to BMN, while model size and inference speed scale better with large input sizes.</p>
</td>
      </tr>

    
    
      <tr>
          <td data-order="rabinovich2017abstract"><a href="/publications/rabinovich2017abstract">M. Rabinovich, M. Stern, D. Klein, 2017.</a> Abstract Syntax Networks for Code Generation and Semantic Parsing</td>
          <td>LM AST context</td>
          <td>LM (Syntax)</td>
          <td>Distributed</td>
          <td>Code Synthesis</td>
          <td><p>Tasks like code generation and semantic parsing require mapping unstructured (or partially structured) inputs to well-formed, executable outputs. We introduce abstract syntax networks, a modeling framework for these problems. The outputs are represented as abstract syntax trees (ASTs) and constructed by a decoder with a dynamically-determined modular structure paralleling the structure of the output tree. On the benchmark Hearthstone dataset for code generation, our model obtains 79.2 BLEU and 22.7% exact match accuracy, compared to previous state-of-the-art values of 67.1 and 6.1%. Furthermore, we perform competitively on the Atis, Jobs, and Geo semantic parsing datasets with no task-specific engineering.</p>
</td>
      </tr>

    
    
      <tr>
          <td data-order="raychev2015predicting"><a href="/publications/raychev2015predicting">V. Raychev, M. Vechev, A. Krause, 2015.</a> Predicting Program Properties from “Big Code”</td>
          <td>Dependency Net</td>
          <td>Types + Names</td>
          <td>CRF (GM)</td>
          <td>Types + Names</td>
          <td><p>We present a new approach for predicting program properties from
massive codebases (aka “Big Code”). Our approach first learns a
probabilistic model from existing data and then uses this model to
predict properties of new, unseen programs.</p>

<p>The key idea of our work is to transform the input program into
a representation which allows us to phrase the problem of inferring program properties as structured prediction in machine learning. This formulation enables us to leverage powerful probabilistic
graphical models such as conditional random fields (CRFs) in order
to perform joint prediction of program properties.</p>

<p>As an example of our approach, we built a scalable prediction
engine called JSNICE 1 for solving two kinds of problems in the
context of JavaScript: predicting (syntactic) names of identifiers
and predicting (semantic) type annotations of variables. Experimentally, JSNICE predicts correct names for 63% of name identifiers and its type annotation predictions are correct in 81% of the
cases. In the first week since its release, JSN ICE was used by more
than 30,000 developers and in only few months has become a popular tool in the JavaScript developer community.</p>

<p>By formulating the problem of inferring program properties as
structured prediction and showing how to perform both learning
and inference in this context, our work opens up new possibilities
for attacking a wide range of difficult problems in the context of
“Big Code” including invariant generation, de-compilation, synthesis and others.</p>
</td>
      </tr>

    
    
      <tr>
          <td data-order="wang2016bugram"><a href="/publications/wang2016bugram">S. Wang, D. Chollak, D. Movshovitz-Attias, L. Tan, 2016.</a> Bugram: bug detection with n-gram language models</td>
          <td>Tokens</td>
          <td>Defects</td>
          <td>LM (\ngram)</td>
          <td>Bug Detection</td>
          <td><p>To improve software reliability, many rule-based techniques have been proposed to infer programming rules and detect violations of these rules as bugs. These rule-based approaches often rely on the highly frequent appearances of certain patterns in a project to infer rules. It is known that if a pattern does not appear frequently enough, rules are not learned, thus missing many bugs.</p>

<p>In this paper, we propose a new approach—Bugram—that leverages n-gram language models instead of rules to detect bugs. Bugram models program tokens sequentially, using the n-gram language model. Token sequences from the program are then assessed according to their probability in the learned model, and low probability sequences are marked as potential bugs. The assumption is that low probability token sequences in a program are unusual, which may indicate bugs, bad practices, or unusual/special uses of code of which developers may want to be aware.</p>

<p>We evaluate Bugram in two ways. First, we apply Bugram on the latest versions of 16 open source Java projects. Results show that Bugram detects 59 bugs, 42 of which are manually verified as correct, 25 of which are true bugs and 17 are code snippets that should be refactored. Among the 25 true bugs, 23 cannot be detected by PR-Miner. We have reported these bugs to developers, 7 of which have already been confirmed by developers (4 of them have already been fixed), while the rest await confirmation. Second, we further compare Bugram with three additional graph- and rule-based bug detection tools, i.e., JADET, Tikanga, and GrouMiner. We apply Bugram on 14 Java projects evaluated in these three studies. Bugram detects 21 true bugs, at least 10 of which cannot be detected by these three tools. Our results suggest that Bugram is complementary to existing rule-based bug detection approaches.</p>

</td>
      </tr>

    
    
      <tr>
          <td data-order="white2015toward"><a href="/publications/white2015toward">M. White, C. Vendome, M. Linares-Vásquez, D. Poshyvanyk, 2015.</a> Toward Deep Learning Software Repositories</td>
          <td>Tokens</td>
          <td>Language Model</td>
          <td>Distributed</td>
          <td>---</td>
          <td><p>Deep learning subsumes algorithms that automatically learn compositional representations. The ability of these
models to generalize well has ushered in tremendous advances
in many fields such as natural language processing (NLP).
Recent research in the software engineering (SE) community
has demonstrated the usefulness of applying NLP techniques to
software corpora. Hence, we motivate deep learning for software
language modeling, highlighting fundamental differences between
state-of-the-practice software language models and connectionist
models. Our deep learning models are applicable to source
code files (since they only require lexically analyzed source
code written in any programming language) and other types
of artifacts. We show how a particular deep learning model
can remember its state to effectively model sequential data,
e.g., streaming software tokens, and the state is shown to be
much more expressive than discrete tokens in a prefix. Then we
instantiate deep learning models and show that deep learning
induces high-quality models compared to n-grams and cache-based n-grams on a corpus of Java projects. We experiment
with two of the models’ hyperparameters, which govern their
capacity and the amount of context they use to inform predictions,
before building several committees of software language models
to aid generalization. Then we apply the deep learning models to
code suggestion and demonstrate their effectiveness at a real SE
task compared to state-of-the-practice models. Finally, we propose
avenues for future work, where deep learning can be brought to
bear to support model-based testing, improve software lexicons,
and conceptualize software artifacts. Thus, our work serves as
the first step toward deep learning software repositories.</p>
</td>
      </tr>

    
    
      <tr>
          <td data-order="white2016deep"><a href="/publications/white2016deep">M. White, M. Tufano, C. Vendome, D. Poshyvanyk, 2016.</a> Deep Learning Code Fragments for Code Clone Detection</td>
          <td>Token + AST</td>
          <td></td>
          <td>Distributed</td>
          <td>Clone Detection</td>
          <td><p>Code clone detection is an important problem for software
maintenance and evolution. Many approaches consider either structure or identifiers, but none of the existing detection techniques model both sources of information. These
techniques also depend on generic, handcrafted features to
represent code fragments. We introduce learning-based detection techniques where everything for representing terms
and fragments in source code is mined from the repository.
Our code analysis supports a framework, which relies on
deep learning, for automatically linking patterns mined at
the lexical level with patterns mined at the syntactic level.
We evaluated our novel learning-based approach for code
clone detection with respect to feasibility from the point
of view of software maintainers. We sampled and manually
evaluated 398 file- and 480 method-level pairs across eight
real-world Java systems; 93% of the file- and method-level
samples were evaluated to be true positives. Among the true
positives, we found pairs mapping to all four clone types. We
compared our approach to a traditional structure-oriented
technique and found that our learning-based approach detected clones that were either undetected or suboptimally
reported by the prominent tool Deckard. Our results affirm
that our learning-based approach is suitable for clone detection and a tenable technique for researchers.</p>
</td>
      </tr>

    
    
      <tr>
          <td data-order="zaremba2014learning"><a href="/publications/zaremba2014learning">W. Zaremba, I. Sutskever, 2014.</a> Learning to Execute</td>
          <td>Characters</td>
          <td>Execution Trace</td>
          <td>Distributed</td>
          <td>---</td>
          <td><p>Recurrent Neural Networks (RNNs) with Long Short-Term Memory units (LSTM) are widely used because they are expressive and are easy to train. Our interest lies in empirically evaluating the expressiveness and the learnability of LSTMs in the sequence-to-sequence regime by training them to evaluate short computer programs, a domain that has traditionally been seen as too complex for neural networks. We consider a simple class of programs that can be evaluated with a single left-to-right pass using constant memory. Our main result is that LSTMs can learn to map the character-level representations of such programs to their correct outputs. Notably, it was necessary to use curriculum learning, and while conventional curriculum learning proved ineffective, we developed a new variant of curriculum learning that improved our networks’ performance in all experimental conditions. The improved curriculum had a dramatic impact on an addition problem, making it possible to train an LSTM to add two 9-digit numbers with 99% accuracy.</p>
</td>
      </tr>

</tbody></table>

<script>
$(document).ready( function () {
    $('#reprModelTable').DataTable({
		paging: false,
		columnDefs: [
			{
				targets: [5],
				visible: false,
				searchable: true
			}]
		});
} );
</script>

    </div>

  </body>
</html>
